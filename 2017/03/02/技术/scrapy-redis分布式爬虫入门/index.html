

<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>scrapy-redis 分布式爬虫 | 帅羊羊的博客</title>
  
  <meta name="author" content="Shuai yy">
  
  <meta name="description" content="在docker中运行基于scrapy-redis的爬虫">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="scrapy-redis 分布式爬虫"/>
  <meta property="og:site_name" content="帅羊羊的博客"/>

  
    <meta property="og:image" content=""/>
  
  <link href="/favicon.ico" rel="icon" type="image/x-ico">
  <link rel="alternate" href="http://shuaiyy.cn/atom.xml" title="帅羊羊的博客" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <script src="//libs.baidu.com/jquery/1.8.0/jquery.min.js"></script>
  
  
</head>



<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">帅羊羊的博客</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/"><i class="fa fa-home"></i>首页</a></li>
	  
    
      <li><a href="/archives"><i class="fa fa-archive"></i>归档</a></li>
	  
    
      <li><a href="/resume"><i class="fa fa-user"></i>关于</a></li>
	  
    
	<li> <a href="/atom.xml"><i class="fa fa-rss"></i>RSS</a> </li>
<li> <a title="把这个链接拖到你的Chrome收藏夹工具栏中" href='javascript:(function() {
	function c() {
		var e = document.createElement("link");
		e.setAttribute("type", "text/css");
		e.setAttribute("rel", "stylesheet");
		e.setAttribute("href", f);
		e.setAttribute("class", l);
		document.body.appendChild(e)
	}
 
	function h() {
		var e = document.getElementsByClassName(l);
		for (var t = 0; t < e.length; t++) {
			document.body.removeChild(e[t])
		}
	}
 
	function p() {
		var e = document.createElement("div");
		e.setAttribute("class", a);
		document.body.appendChild(e);
		setTimeout(function() {
			document.body.removeChild(e)
		}, 100)
	}
 
	function d(e) {
		return {
			height : e.offsetHeight,
			width : e.offsetWidth
		}
	}
 
	function v(i) {
		var s = d(i);
		return s.height > e && s.height < n && s.width > t && s.width < r
	}
 
	function m(e) {
		var t = e;
		var n = 0;
		while (!!t) {
			n += t.offsetTop;
			t = t.offsetParent
		}
		return n
	}
 
	function g() {
		var e = document.documentElement;
		if (!!window.innerWidth) {
			return window.innerHeight
		} else if (e && !isNaN(e.clientHeight)) {
			return e.clientHeight
		}
		return 0
	}
 
	function y() {
		if (window.pageYOffset) {
			return window.pageYOffset
		}
		return Math.max(document.documentElement.scrollTop, document.body.scrollTop)
	}
 
	function E(e) {
		var t = m(e);
		return t >= w && t <= b + w
	}
 
	function S() {
		var e = document.createElement("audio");
		e.setAttribute("class", l);
		e.src = i;
		e.loop = false;
		e.addEventListener("canplay", function() {
			setTimeout(function() {
				x(k)
			}, 500);
			setTimeout(function() {
				N();
				p();
				for (var e = 0; e < O.length; e++) {
					T(O[e])
				}
			}, 15500)
		}, true);
		e.addEventListener("ended", function() {
			N();
			h()
		}, true);
		e.innerHTML = " <p>If you are reading this, it is because your browser does not support the audio element. We recommend that you get a new browser.</p> <p>";
		document.body.appendChild(e);
		e.play()
	}
 
	function x(e) {
		e.className += " " + s + " " + o
	}
 
	function T(e) {
		e.className += " " + s + " " + u[Math.floor(Math.random() * u.length)]
	}
 
	function N() {
		var e = document.getElementsByClassName(s);
		var t = new RegExp("\\b" + s + "\\b");
		for (var n = 0; n < e.length; ) {
			e[n].className = e[n].className.replace(t, "")
		}
	}
 
	var e = 30;
	var t = 30;
	var n = 350;
	var r = 350;
	var i = "//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake.mp3";
	var s = "mw-harlem_shake_me";
	var o = "im_first";
	var u = ["im_drunk", "im_baked", "im_trippin", "im_blown"];
	var a = "mw-strobe_light";
	var f = "//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake-style.css";
	var l = "mw_added_css";
	var b = g();
	var w = y();
	var C = document.getElementsByTagName("*");
	var k = null;
	for (var L = 0; L < C.length; L++) {
		var A = C[L];
		if (v(A)) {
			if (E(A)) {
				k = A;
				break
			}
		}
	}
	if (A === null) {
		console.warn("Could not find a node of the right size. Please try a different page.");
		return
	}
	c();
	S();
	var O = [];
	for (var L = 0; L < C.length; L++) {
		var A = C[L];
		if (v(A)) {
			O.push(A)
		}
	}
})()    '>High一下</a> </li>

  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
    <div class="gallery">
  <div class="photoset">
    
      <img src="">
    
  </div>
  <div class="control">
    <div class="prev"></div>
    <div class="next"></div>
  </div>
</div>
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-01T16:00:00.000Z"><a href="/2017/03/02/技术/scrapy-redis分布式爬虫入门/">2017-03-02</a></time>
      
      
  
    <h1 class="title">scrapy-redis 分布式爬虫</h1>
  

    </header>
    <div class="entry">
         
        
    <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy-redis-分布式爬虫"><span class="toc-text">scrapy-redis 分布式爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#简介"><span class="toc-text">简介</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#安装和配置"><span class="toc-text">安装和配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#作用和特点"><span class="toc-text">作用和特点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#初步使用"><span class="toc-text">初步使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#settings参数"><span class="toc-text">settings参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#scrapy-redis使用方法"><span class="toc-text">scrapy-redis使用方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分布式爬取"><span class="toc-text">分布式爬取</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#直接运行多个爬虫"><span class="toc-text">直接运行多个爬虫</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#利用docker部署爬虫"><span class="toc-text">利用docker部署爬虫</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#导出redis中的items"><span class="toc-text">导出redis中的items</span></a></li></ol></li></ol></li></ol></li></ol>
    </div>

      
        <h1 id="scrapy-redis-分布式爬虫"><a href="#scrapy-redis-分布式爬虫" class="headerlink" title="scrapy-redis 分布式爬虫"></a>scrapy-redis 分布式爬虫</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><h4 id="安装和配置"><a href="#安装和配置" class="headerlink" title="安装和配置"></a>安装和配置</h4><ul>
<li>安装redis数据库 </li>
<li><code>pip install scrapy redis-py scrapy-redis</code></li>
</ul>
<h4 id="作用和特点"><a href="#作用和特点" class="headerlink" title="作用和特点"></a>作用和特点</h4><blockquote>
<p>scrapy-redis是为Scrapy提供redis支持以实现分布式爬虫的组件</p>
</blockquote>
<ul>
<li>多个爬虫共享一个redis队列（分配request）</li>
<li>分布式的post处理。将爬到的items也放入redis队列，因而可以实现items的分布式处理。</li>
</ul>
<p>scrapy-redis仅仅为scrapy提供了部分基于redis的组件，可以查看源码。</p>
<ul>
<li>pipeline</li>
<li>scheluder</li>
<li>redis队列替换原有的scrapy队列</li>
<li>过滤器 Duplication</li>
</ul>
<h3 id="初步使用"><a href="#初步使用" class="headerlink" title="初步使用"></a>初步使用</h3><h4 id="settings参数"><a href="#settings参数" class="headerlink" title="settings参数"></a><strong>settings参数</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Enables scheduling storing requests queue in redis.</span></div><div class="line">SCHEDULER = <span class="string">"scrapy_redis.scheduler.Scheduler"</span></div><div class="line"></div><div class="line"><span class="comment"># Ensure all spiders share same duplicates filter through redis.</span></div><div class="line">DUPEFILTER_CLASS = <span class="string">"scrapy_redis.dupefilter.RFPDupeFilter"</span></div><div class="line"></div><div class="line"><span class="comment"># Don't cleanup redis queues, allows to pause/resume crawls.</span></div><div class="line">SCHEDULER_PERSIST = <span class="keyword">True</span></div><div class="line"></div><div class="line"><span class="comment"># Schedule requests using a priority queue. (default)</span></div><div class="line">SCHEDULER_QUEUE_CLASS = <span class="string">'scrapy_redis.queue.PriorityQueue'</span></div><div class="line"><span class="comment"># Alternative queues.</span></div><div class="line"><span class="comment">#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'</span></div><div class="line"><span class="comment">#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'</span></div><div class="line"></div><div class="line"><span class="comment"># Max idle time to prevent the spider from being closed when distributed crawling.</span></div><div class="line"><span class="comment"># This only works if queue class is SpiderQueue or SpiderStack,</span></div><div class="line"><span class="comment"># and may also block the same time when your spider start at the first time (because the queue is empty).</span></div><div class="line"><span class="comment">#SCHEDULER_IDLE_BEFORE_CLOSE = 10</span></div><div class="line"></div><div class="line"><span class="comment"># Store scraped item in redis for post-processing.</span></div><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">    <span class="string">'scrapy_redis.pipelines.RedisPipeline'</span>: <span class="number">300</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Specify the host and port to use when connecting to Redis (optional).</span></div><div class="line"><span class="comment">#REDIS_HOST = 'localhost'</span></div><div class="line"><span class="comment">#REDIS_PORT = 6379</span></div><div class="line"></div><div class="line"><span class="comment"># Specify the full Redis URL for connecting (optional).</span></div><div class="line"><span class="comment"># If set, this takes precedence over the REDIS_HOST and REDIS_PORT settings.</span></div><div class="line">REDIS_URL = <span class="string">'redis://user:pass@hostname:9001'</span></div><div class="line"></div><div class="line"><span class="comment"># Use other encoding than utf-8 for redis.默认utf-8</span></div><div class="line">REDIS_ENCODING = <span class="string">'latin1'</span></div></pre></td></tr></table></figure>
<h4 id="scrapy-redis使用方法"><a href="#scrapy-redis使用方法" class="headerlink" title="scrapy-redis使用方法"></a>scrapy-redis使用方法</h4><ul>
<li><p>首先用scrapy实现一个爬虫，然后在替换其中的组件为scrapy-redis</p>
<p>setting里修改：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># setting.py</span></div><div class="line">BOT_NAME = <span class="string">'moko1'</span></div><div class="line"></div><div class="line">SPIDER_MODULES = [<span class="string">'moko1.spiders'</span>]</div><div class="line">NEWSPIDER_MODULE = <span class="string">'moko1.spiders'</span></div><div class="line"></div><div class="line"><span class="comment"># 使用scrapy-redis的去重和调度器组件</span></div><div class="line">DUPEFILTER_CLASS = <span class="string">"scrapy_redis.dupefilter.RFPDupeFilter"</span></div><div class="line">SCHEDULER = <span class="string">"scrapy_redis.scheduler.Scheduler"</span></div><div class="line">SCHEDULER_PERSIST = <span class="keyword">True</span></div><div class="line"></div><div class="line">ITEM_PIPELINES = &#123;</div><div class="line"><span class="comment"># 会将items放入redis队列中</span></div><div class="line"><span class="string">'scrapy_redis.pipelines.RedisPipeline'</span>: <span class="number">400</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>spiders里修改：</p>
<p><strong>spider类</strong>从scrapy_redis.spiders导入，有RedisSpider和RedisCrawlSpider，对应scrapy原来的Spider和CrawlSpider。</p>
<p><strong>start_urls</strong>改为从redis中某个key获取，因此redis_key = ‘moko_spider:start_urls’，然后向该key push数据。</p>
<p>直接给出start_urls也是可行的，但是不太符合redis队列及分布式的逻辑，而且不能手动动态添加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</div><div class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> Rule</div><div class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span>  Moko1Item</div><div class="line"><span class="keyword">from</span> scrapy_redis.spiders <span class="keyword">import</span> RedisCrawlSpider</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MokoSpiderSpider</span><span class="params">(RedisCrawlSpider)</span>:</span> <span class="comment"># 修改此处</span></div><div class="line">    name = <span class="string">'moko_spider'</span></div><div class="line">    allowed_domains = [<span class="string">'moko.cc'</span>]</div><div class="line">    start_urls = [<span class="string">'http://www.moko.cc/moko/post/1.html'</span>]  <span class="comment"># 修改此处</span></div><div class="line">    <span class="comment"># redis_key = 'moko_spider:start_urls'</span></div><div class="line">  </div><div class="line">    rules = (</div><div class="line">        Rule(LinkExtractor(allow=<span class="string">r'http://www\.moko\.cc/post/\d+\.html'</span>), callback=<span class="string">'parse_item'</span>, follow=<span class="keyword">True</span>),</div><div class="line">    )</div><div class="line">  </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></div><div class="line">        item = Moko1Item()</div><div class="line">        item[<span class="string">'name'</span>] = response.css(<span class="string">"#workNickName::text"</span>).extract()[<span class="number">0</span>]</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
</li>
<li><p>确保redis数据库运行，清空数据库<code>flushdb</code>, “moko_spider:dupefilter”保存了我上次执行时已经爬取过的url信息。再次执行会被过滤掉，scrapy的去重机制。</p>
</li>
<li><p>然后启动scrapy爬虫，然后向redis_key = ‘moko_spider:start_urls’中push数据，在redis-cli客户端中执行<code>lpush moko_spider:start_urls https://moko.cc/1.html</code></p>
</li>
<li><p>如果启用了scrapy_redis.pipelines.RedisPipeline，items会存储在moko_spider:items中。可以将items不断的pop出来，并进行其他处理，如存储等。(感觉这种活应该交给一个pipeline干)</p>
</li>
</ul>
<h3 id="分布式爬取"><a href="#分布式爬取" class="headerlink" title="分布式爬取"></a>分布式爬取</h3><h4 id="直接运行多个爬虫"><a href="#直接运行多个爬虫" class="headerlink" title="直接运行多个爬虫"></a>直接运行多个爬虫</h4><blockquote>
<p>上面的单个爬虫默认从localhost的Redis数据库中存取request和爬到的items，</p>
<p>而实现分布式爬虫只需要为爬虫指定Redis数据库的网络位置，所有的爬虫都去redis队列里存取。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># settings.py</span></div><div class="line">REDIS_URL = <span class="string">'redis://user:mima@localhost:6379'</span> </div><div class="line"><span class="comment"># 或者不带密码的</span></div><div class="line">REDIS_HOST = <span class="string">'localhost'</span></div><div class="line">REDIS_PORT = <span class="number">6379</span></div></pre></td></tr></table></figure>
<ul>
<li><p>配置redis允许远程访问</p>
<p>修改配置文件/etc/redis.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># bind 127.0.0.1</div><div class="line">requirepass mima  # 设置密码</div></pre></td></tr></table></figure>
<p>​</p>
</li>
<li><p>关于<strong>主从模式</strong></p>
<p>分布式架构一般分为主从模式和P2P模式。有的人认为scrapy-redis中安装有redis数据库的节点就是master，错的！</p>
<p>scrapy-redis中的每只爬虫都是平级的，没有主从之分。每只爬虫都是主动请求任务，执行任务，爬到的数据也可以提交给redis。redis的request队列为空时，爬虫处于饥饿状态。</p>
<p>scrapy-redis仅仅是把scrapy原来得本地队列放入redis数据库中，从通信和数据传输的角度来看，redis像是一个master，而实际上redis对爬虫没做任何控制和操作，只是被动的为它们提供数据。</p>
</li>
</ul>
<h4 id="利用docker部署爬虫"><a href="#利用docker部署爬虫" class="headerlink" title="利用docker部署爬虫"></a>利用docker部署爬虫</h4><ul>
<li><p>创建一个docker镜像并配置scrapy环境，这样下次就能恢复环境直接使用了</p>
<ul>
<li>首先在daocloud申请一台胶囊主机，然后ssh登录上去，配置scrapy环境</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> 使用ubuntu的docker镜像</span></div><div class="line">docker run -it  daocloud.io/ubuntu:14.04 /bin/bash</div><div class="line">apt-get update </div><div class="line">apt-get install lrzsz</div><div class="line">apt-get install python2.7 python-pip python-dev</div><div class="line">apt-get install libxml2-dev libxslt-dev  python-lxml # 安装lxml</div><div class="line">apt-get install build-essential libssl-dev  libffi-dev</div><div class="line">pip install  six --upgrade</div><div class="line">python -m pip install pyparsing appdirs</div><div class="line">pip install  cryptography</div><div class="line">pip install pymongo redis twisted scrapy scrapy-redis</div><div class="line">exit # 退出docker，记住id，docker id root@ea0d832b19bb</div></pre></td></tr></table></figure>
<ul>
<li>打包上传镜像</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> 打包镜像，docker容器的id ea0d832b19bb</span></div><div class="line">~$ docker commit -m "ubuntu with scrapy_redis" -a "author——info" ea0d832b19bb scrapy-redis</div><div class="line">sha256:53a605ccc92ab29bba70f9f026c002a9c4afa43fb9b14a9819d5859f51b0d586</div><div class="line">~$ docker images # 查看镜像</div><div class="line"><span class="meta">#</span><span class="bash"> 为镜像打上tag</span></div><div class="line">docker tag scrapy-redis syy2358/scrapy-redis:latest</div><div class="line"><span class="meta">#</span><span class="bash"> 上传至dockerhub托管</span></div><div class="line">docker login # 先注册并登录dockerhub，创建一个托管仓库scrapy-redis</div><div class="line">docker push syy2358/scrapy-redis:lastest # 将镜像上传至dockerhub</div><div class="line"><span class="meta">#</span><span class="bash"> 胶囊主机只有2小时，hub上传速度又慢，只好打包镜像文件下载到我的电脑上。</span></div><div class="line">docker save ea0d832b19bb &gt; /home/ubuntu/scrapy-redis.tar</div><div class="line"><span class="meta">#</span><span class="bash"> 可以在有docker的电脑上恢复该容器，</span></div><div class="line">docker load &lt; scrapy-redis.tar</div><div class="line"><span class="meta">#</span><span class="bash"> 或者直接拉dockerhub/daocloud上的镜像用就行了</span></div><div class="line">docker pull syy2358/scrapy-redis</div><div class="line">docker run -it xx.xx</div><div class="line"><span class="meta">#</span><span class="bash"> 导出 <span class="built_in">export</span> 和save的区别- 导入 import</span></div><div class="line"><span class="meta">#</span><span class="bash"> save保存了容器的运行状态，支持回滚，但是数据较大。</span></div></pre></td></tr></table></figure>
<ul>
<li><p>​<strong>自己配置环境各种报错，主要是下载链接超时，用daocloud就很顺利  </strong></p>
<blockquote>
<p>首先自己编译docker镜像容易遇到各种错误，而且dockerhub的镜像push、pull的速度巨慢，估计是被墙了，所以决定改用daocloud在线编译发布镜像，编译和pull的速度都很快。</p>
</blockquote>
<p>镜像制作过程：</p>
<ol>
<li>在自己 GitHub 创建新的 repository 。</li>
<li>将爬虫的代码，包含<code>Dockerfile</code> push 到自己刚创建的 repository。</li>
<li>到 <code>https://dashboard.daocloud.io/build-flows/new</code> ，项目名称 <code>scrapy</code>，选择自己刚在 GitHub 创建的 repository同步代码，开始创建，选择<code>分支：master</code>，手动执行。如果失败，可以先看下流程定义里的构建阶段，修改任务，选择云端Dockerfile。</li>
<li>到 <code>https://dashboard.daocloud.io/packages</code> 选择 <code>scrapy</code>，设置 -&gt; 镜像访问控制 -&gt; 公开,设置tag为latest。</li>
<li><code>https://dashboard.daocloud.io/packages/</code>选择scrapy后，版本 -&gt; latest 。然后可以部署到已经接入的docker或者云测试环境(右上角打开控制台，能进入web版的终端，在里面执行scrapy crawl spider即可)。</li>
<li>或者在自己的docker环境下，使用<code>docker run -it daocloud.io/blue_whale/scrapy</code>,然后就能看到爬虫在运行了</li>
</ol>
</li>
<li><p>镜像地址</p>
<p><code>daocloud.io/blue_whale/scrapy</code> : daocloud上的，速度很快</p>
<p><code>syy2358/scrapy-redis</code>: dockerhub上的，巨慢</p>
<p>​</p>
</li>
</ul>
</li>
<li><p>运行爬虫</p>
<ol>
<li><p>上传源码，并从Dockerfile build镜像，然后运行爬虫</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 项目结构</span></div><div class="line">.</div><div class="line">├── docker-compose.yml</div><div class="line">├── Dockerfile</div><div class="line">├── moko1</div><div class="line">│   ├── __init__.py</div><div class="line">│   ├── items.py</div><div class="line">│   ├── pipelines.py</div><div class="line">│   ├── settings.py</div><div class="line">│   └── spiders</div><div class="line">│       ├── __init__.py</div><div class="line">│       ├── moko_spider.py</div><div class="line">├── requirements.txt</div><div class="line">└── scrapy.cfg</div><div class="line"></div><div class="line">---------------------------</div><div class="line"></div><div class="line"><span class="comment"># docker-compose.yml</span></div><div class="line">version: '2'</div><div class="line">services:</div><div class="line">  spider:</div><div class="line">    build: .</div><div class="line">    volumes:</div><div class="line">     - .:/code</div><div class="line"></div><div class="line">------------------------</div><div class="line"></div><div class="line"><span class="comment"># Dockerfile</span></div><div class="line">FROM syy2358/scrapy-redis</div><div class="line">ENV PATH /usr/local/bin:$PATH</div><div class="line">ADD . /code</div><div class="line">WORKDIR /code</div><div class="line">RUN pip install -r requirements.txt</div><div class="line"><span class="comment"># COPY spiders.py /usr/local/lib/python3.5/site-packages/scrapy_redis</span></div><div class="line">CMD scrapy crawl moko_spider</div></pre></td></tr></table></figure>
<p>我的redis服务器是在腾讯云上的，没有使用docker。</p>
<p>如果redis在docker中运行的话，需要在<code>docker-compose.yml</code>中定义redis的container，将spider和redis link起来，同时redis需要映射端口6379，这样不同的container之间才能相互通信。</p>
<p>使用docker-compose创建container</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">pip install docker-compose</div><div class="line"> rz -E  # 上传爬虫源码</div><div class="line"> docker-compose up #从 docker-compose.yml 中创建 `container`</div><div class="line"> docker-compose scale spider=4 #将 spider 这一个服务扩展到4个container</div><div class="line"><span class="meta"> #</span><span class="bash"> 会有4个scrapy爬虫运行，处于饥饿状态，因为刚开始start_urls为空，直到我们pushurl去feed爬虫，爬虫才会开始抓取工作。</span></div></pre></td></tr></table></figure>
<p>​</p>
</li>
</ol>
</li>
</ul>
<ol>
<li><p>方法二，使用已经build好的docker镜像(爬虫代码也已经copy进去了)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">docker run -it daocloud.io/blue_whale/scrapy</div><div class="line"><span class="meta">#</span><span class="bash"><span class="comment">## 或者</span></span></div><div class="line">docker run -it syy2358/scrapy-redis</div><div class="line"><span class="meta">#</span><span class="bash"><span class="comment">## Ctrl+P+Q 将当前container放入后台，回到docker界面</span></span></div><div class="line">docker ps -a ## 查看正在运行的container</div><div class="line">docker attach  id  # 连接入正在执行的container</div></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li><p>退出attach的docker container</p>
<p>用1执行爬虫时真惨，attach上container退不出去，scrapy不停地输出log内容，按啥键都不好使，只好退出ssh重连T.T，重连后发现原来的container仍在运行中。</p>
<p>正常attach上一个container，可以Ctrl+P+Q退出再后台执行，或exit终止运行并退出container。</p>
<p>使用<a href="http://lib.csdn.net/base/docker" target="_blank" rel="external">Docker</a> attach命名进入docker容器后：</p>
<p>【场景一】如果要正常退出不关闭容器，请按Ctrl+P+Q进行退出容器。</p>
<p>【场景二】如果使用exit退出，那么在退出容器后会关闭容器，如下图所示。</p>
</li>
<li><p>总结</p>
<p>只需要配置一次scrapy的docker运行环境，上传代码，然后将container打包成镜像，就可以在任何有docker的地方pull下镜像运行。</p>
<p>docker挺有意思的，项目部署非常方便，不过我这个新手对docker只是一知半解。</p>
</li>
</ul>
<h4 id="导出redis中的items"><a href="#导出redis中的items" class="headerlink" title="导出redis中的items"></a>导出redis中的items</h4><ul>
<li><p>linux下使用redis-dump <code>redis-dump -u 127.0.0.1:6379 &gt; db_full.json</code></p>
</li>
<li><p>将数据导入mongodb中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> redis</div><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    r = redis.Redis(host=<span class="string">'192.168.1.112'</span>,port=<span class="number">6379</span>,db=<span class="number">0</span>)</div><div class="line">    client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</div><div class="line">    db = client[<span class="string">'dmoz'</span>]</div><div class="line">    sheet = db[<span class="string">'sheet'</span>]</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># 将队列里的数据逐条pop出来，并插入mongodb中</span></div><div class="line">        <span class="comment"># process queue as FIFO, change `blpop` to `brpop` to process as LIFO</span></div><div class="line">        source, data = r.blpop([<span class="string">"dmoz:items"</span>])</div><div class="line">        item = json.loads(data)</div><div class="line">        sheet.insert(item)</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            <span class="keyword">print</span> <span class="string">u"Processing: %(name)s &lt;%(link)s&gt;"</span> % item</div><div class="line">        <span class="keyword">except</span> KeyError:</div><div class="line">            <span class="keyword">print</span> <span class="string">u"Error procesing: %r"</span> % item</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure>
<p>​</p>
</li>
</ul>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/技术/">技术</a>, <a href="/categories/技术/scrapy/">scrapy</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/爬虫/">爬虫</a>, <a href="/tags/Scrapy/">Scrapy</a>, <a href="/tags/Redis/">Redis</a>
  </div>

<!-- 打赏按钮开始 -->

    <br>
<br>
<div id="donate"></div>

<script src="https://unpkg.com/vdonate"></script>
<script type="text/javascript">
new Donate({
  title: '如果我的博客帮助了您，请随意打赏。感谢支持!', // 可选参数，打赏标题
  btnText: '打赏作者', // 可选参数，打赏按钮文字
  el: document.getElementById('donate'), // 可选参数，打赏按钮的容器
  wechatImage: 'http://o8i01ajlj.bkt.clouddn.com/blog/171004/9h7fIfcILJ.png',
  alipayImage: 'http://o8i01ajlj.bkt.clouddn.com/blog/171004/KLLJdiLg9c.jpg'
});
</script>

<script>
window.onload = function(){
 var oTop = document.getElementById("donate");
 var screenw = document.documentElement.clientWidth || document.body.clientWidth;
 var screenh = document.documentElement.clientHeight || document.body.clientHeight;
 oTop.style.left = screenw - oTop.offsetWidth +"px";
 oTop.style.top = screenh - oTop.offsetHeight + "px";
 window.onscroll = function(){
  var scrolltop = document.documentElement.scrollTop || document.body.scrollTop;
  oTop.style.top = screenh - oTop.offsetHeight + scrolltop +"px";
 }
 oTop.onclick = function(){
  document.documentElement.scrollTop = document.body.scrollTop =0;
 }
} 
</script>




<!-- 打赏结束 -->
<!-- Baidu Button BEGIN -->

    
<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a><a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"32"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>


<!--
<div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>
-->

<!-- Baidu Button END -->      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>



 <nav id="pagination" >
    
    <a href="/2017/03/11/阅读/Book-List-2017-Q1/" class="alignleft prev" >上一页</a>
    
    
    <a href="/2017/03/01/技术/redis学习笔记/" class="alignright next" >下一页</a>
    
    <div class="clearfix"></div>
</nav>

<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ0NS82MDEz"></div>

<script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
</script>


</div></div>
    <aside id="sidebar" class="alignright">
  
<div>
<a href="javascript:;" class="popup-trigger">      
      <i class="menu-item-icon fa fa-search fa-fw"></i>        
   搜索
</a>
</div>


<div class="site-search">
<div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>
</div>

<script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }
     
  var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        success: function( xmlResponse ) {
            // get the contents from search data
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var str='<ul class=\"search-result-list\">';
                var str1=str                
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>';
                if (this.value.trim().length <= 0) {
                    return;
                }
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
					else { isMatch=false; } //更新此处
					
					//更新此处
					
                    // show search results
                    if (isMatch) {
                        str += "<li><a href='"+ data_url +"' class='search-result-title' target='_blank'>"+ "> " + data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out characters
                            var start = first_occur - 20;
                            var end = first_occur + 30;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 10;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substr(start, end); 
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<em class=\"search-keyword\">"+keyword+"</em>");
                            })
							//console.log(match_content)
                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                    }
                })
                //修改
				if (str1==str){
					str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';
					}
				
				$resultContent.innerHTML = str;
            })
        }
    })
}
    // search function;
 </script>
<script>

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
	  proceedsearch();
	  //添加的
	  document.getElementById("local-search-result").innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>';
     
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>
  
<script type="text/javascript">      
     var search_path = "search.xml";
     if (search_path.length == 0) {
     	search_path = "search.xml";
     }
     var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
</script>





  
<div class="widget tag">
  <h3 class="title" id="categories">分类</h3>
     <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/技术/">技术</a><span class="category-list-count">43</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/技术/django/">django</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/git/">git</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/latex/">latex</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/linux/">linux</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/markdown/">markdown</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/mongodb/">mongodb</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/numpy/">numpy</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/pandas/">pandas</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/python/">python</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/python-爬虫/">python 爬虫</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/redis/">redis</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/scrapy/">scrapy</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/其他/">其他</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/生活/">生活</a><span class="category-list-count">11</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/生活/游戏/">游戏</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/生活/读书/">读书</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/科研/">科研</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/资源/">资源</a><span class="category-list-count">3</span></li></ul> 
</div>
 

  <div class="widget tag">
<h3 class="title">简介</h3>
<ul class="entry">
<li>博主：帅羊羊</li>
<li>现状：武大CS在读研究生</li>
<li>Theme: <a href="https://github.com/shuaiyy/lightum">Lightum</a>
<!-- <li>想交友的朋友请<a href="http://zipperary.com/about">联系我</a>！</li> -->
<li>QQ 号：2Ol896963</li>
<li>博客: 记录个人生活学习的点滴</li>
<!-- <font color="red">Hexo 交流群：287306637</font> -->
</ul>
</div>



  <iframe width="100%" height="550" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=2&ptype=1&speed=0&skin=1&isTitle=1&noborder=1&isWeibo=1&isFans=0&uid=2946363961&verifier=371067b2&dpc=1"></iframe>

  <div class="widget tag">
<h3 class="title">打赏</h3>
<ul class="entry">
<li>支付宝扫一扫，捐助支持，谢谢！</li>
<li><a href="http://o8i01ajlj.bkt.clouddn.com/blog/171004/LikDAfKDg0.png" title="" class="fancybox" rel="gallery3"><img width="100%" src="http://o8i01ajlj.bkt.clouddn.com/blog/171004/LikDAfKDg0.png"></a></li>
</ul>
</div>

  <div class="widget tag">
  <h3 class="title">日历云</h3>
  <div id="calendar"></div>
</div>

  
  <div class="widget tag">
    <h3 class="title">归档</h3>
	<ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">2017年09月</a><span class="archive-list-count">25</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">2017年05月</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">2017年04月</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">2017年03月</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">2017年02月</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">2016年12月</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">2016年11月</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">2016年10月</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">2016年09月</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">2016年08月</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">2016年07月</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">2016年05月</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">2016年01月</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">2015年01月</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">2014年12月</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/09/">2014年09月</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/08/">2014年08月</a><span class="archive-list-count">1</span></li></ul>
  </div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><section>
Theme of <a href="https://github.com/shuaiyy/lightum">Lightum</a>, Improved from <a href="https://github.com/hexojs/hexo-theme-light">Light</a>, by <a href="/">shuaiyy</a> 
</section>
<div class="clearfix"></div>
</footer>
  <script src="//libs.baidu.com/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('zh-CN',{single:true, root:'calendar/'});
    
    });
  </script>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>


<a href="https://github.com/shuaiyy" target="_blank"><img style="position: absolute; top: 0; left: 0; border: 0;" src="/imgs/forkme_left_green_007200.png" alt="Fork me on GitHub"></a>
