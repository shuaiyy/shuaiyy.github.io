<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2016年玩过的游戏]]></title>
    <url>%2F2017%2F09%2F18%2Fstarve%2F</url>
    <content type="text"></content>
      <categories>
        <category>生活</category>
        <category>游戏</category>
      </categories>
      <tags>
        <tag>Enjoy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一只喵喵喵]]></title>
    <url>%2F2017%2F09%2F17%2F%E6%95%99%E7%A8%8B%E5%88%86%E4%BA%AB%2F2011-09-21-Cat%2F</url>
    <content type="text"><![CDATA[占位]]></content>
      <categories>
        <category>资源</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[买买买]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%94%9F%E6%B4%BB-%E7%94%B5%E5%BD%B1%E9%9F%B3%E4%B9%90%E6%97%85%E8%A1%8C%E6%91%84%E5%BD%B1%2FXXX-2016-07-15-Buy-Buy-Buy%2F</url>
    <content type="text"><![CDATA[记录我的各种“剁手”行为 书籍 12web scraping with python 16.0 电子打印版FreeSWITCH 权威指南 14.0 PDF 电子数码 游戏 12Don&apos;t starve together 24.0 steam gamePortal 2 12.0 steam game 吃喝嫖赌]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2016年9月明信片]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%94%9F%E6%B4%BB-%E7%94%B5%E5%BD%B1%E9%9F%B3%E4%B9%90%E6%97%85%E8%A1%8C%E6%91%84%E5%BD%B1%2FXX-2016-10-14-postcards-in-Sep%2F</url>
    <content type="text"></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[约好的再见]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%94%9F%E6%B4%BB-%E7%94%B5%E5%BD%B1%E9%9F%B3%E4%B9%90%E6%97%85%E8%A1%8C%E6%91%84%E5%BD%B1%2FXX-2016-06-30-Graduation-and-future%2F</url>
    <content type="text"><![CDATA[因为在本校读研，所以一直没有离开的感觉，直到熟悉的人都离开。山水一程，三生有幸，相逢是缘，写于2016.06.30 我毕业啦今天早上和最后一个室友拥抱告别，又一次体会到毕业分别的伤感，由于离校较晚，所有的好友都走在我的前面。6月9号拍摄毕业照，19号院级毕业晚会，20号学院毕业典礼&amp;&amp;学位授予仪式，22号学校的毕业典礼，具体的细节记不清了，只记得有激动有感动也有百般无聊，23号瞬间走光了一半的人，30号是最后离校截止日期。分享一首校友原创的毕业歌曲，超级好听。 武大学子原创毕业歌曲MV《几年》 我的毕业照是好友little D用手机帮我拍的，像素不高但我会珍藏一生。下图背景，逸夫楼（人文馆）。 我的朋友们宿舍的4个人中，cmj留在武汉工作了，他还是个户外旅行达人，估计会是4个人中第一个结婚的，人生赢家。lxc去了帝都联想，被评为future leader，未来肯定一帆风顺，祝他早日成为leader。ljk拿到了南加州的offer，我，lt，yhy还是留在武大读研。吹逼小伙伴lhp去了深圳华为，ysh，教主在魔都。一切从分别开始，遇见更好的自己。 大学大学还想着，我们初次见面，樱花飘落，转眼几年又几天。当年选择武大，感觉是自己一生中做出的最正确的决定。我无法想象如果在一个严肃理性的大学环境中，原本就内向封闭的我会变成啥样，虽然性格是很难改变的，但是武大的轻松自由的氛围一直在潜移默化的影响着我。武大的校训是“自强弘毅、求是拓新”，但武大最想教给我的却是独立和自由。独立生活，独立思考，保持独立的人格和自由的思想，这些我只学到了不足十分之一，我还是那个喜欢逃避，做事不积极主动，不能从对自己负责的角度处理事情，偶尔也会成为自己讨厌的人。如果是以前的我或许都不会意识到哪里出了问题，现在我正在逐渐改变，变得更加成熟。大学里对我影响最大的人还是室友，他们都很优秀，我从他们身上学会了很多。 未来3年研究生要在武大继续读3年，决定与准备读研的过程虽然一路坎坷，最后还是收获了这个。 未来的三年一定要加倍努力，心之所往，不负樱花。在此处立个flag，3年后更新。遇见未来更好的自己]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>随感</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[明信片互寄]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%94%9F%E6%B4%BB-%E7%94%B5%E5%BD%B1%E9%9F%B3%E4%B9%90%E6%97%85%E8%A1%8C%E6%91%84%E5%BD%B1%2FXX-2016-09-20-postcard-201609%2F</url>
    <content type="text"><![CDATA[1、贵州省安顺市开发区学院路25号安顺学院资源与环境工程学院 September，561000…2、河北省石家庄市 桥东区 东三教街 华城绿洲一期6-2-1001 crystal 收 0500213、黑龙江省齐齐哈尔市泰来县第二中学九年级四班 SG 162400 满片实寄不摘抄，内容就写写武大吧。麻烦带上坐标和id（很重要），写好我们互相上图蟹蟹辣4、安徽省六安市舒城县千人桥镇千人桥中学高二七朱进玲收 2313005、湖北省大冶市第一中学高二13班 LXJ收～435000～不挑片～ 未寄出 6、广西壮族自治区河池市都安县高中442班 林东屹收 530700 要求贼多！ 7、陕西省西安市长安区韦郭路558号，西北政法大学 经济学院 肖亚楠 710100 8、江苏省徐州市泉山区翟山街道中国矿业大学文昌校区工程管理1麦子收，221000可以互寄两张吗~想要一张武大建筑的，还有一张樱花的~ 9、广东省广州市白云区钟落潭仲恺农业工程学院国贸143，Crystal.510550楼主，我要上面那种风格的可以吗～建筑的又看得到樱花～ 10、广东省惠州市实验中学高二11班阿绾收516000 建筑 未寄出 11、广东省清远连州市福荣花园1302邮编513400banana收 12、重庆市北碚区天生路2号西南大学李园6舍初玖（收）400715 13、四川成都龙泉驿区十陵成都大学经管院 舒锦茗610106 在我等小透明大学看来武大简直666(成都大学就真是成都大学，不是川大QwQ) 14、江苏省南京市鼓楼区汉中路282号南京中医药大学收发室，吴骏烨收，21002 想要摄影的有樱花的那种可以咩⊙▽⊙， 15、新疆乌鲁木齐天山区五星南路兵团二中高二十一吴雨晨830002，楼主挑片嘛 16、北京市朝阳区望京中环南路6号 北京中医药大学望京校区 三公寓512 蔺明煊 100102 17、上海市奉贤区海思路100号上海师范大学61寝室楼204寝 赤芙收 201418 两张可以嘛，我可以两张师大也可以师大手绘上海手绘各一，贼喜欢武大，希望一张樱花一张随意~ 18、我的地址，江苏省南京市江宁区龙眠大道639号药大b8陈西维211100 19、广东省广州市培正路2号培正中学yyy收 510080 20、上海市闵行区华宁路111号上海电机学院 游瑾扬 200240 落樱纷纷，如诗如画东湖之滨，珞珈山上山水一程，三生有幸江城多山，珞珈独秀。山上有黉，武汉大学！学大汉武立国自强 弘毅 求是 拓新 最美你的容颜 国立武汉大学]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>明信片</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2014年阅读书单]]></title>
    <url>%2F2017%2F09%2F17%2F%E9%98%85%E8%AF%BB%2FBook-List-2014%2F</url>
    <content type="text"></content>
      <categories>
        <category>生活</category>
        <category>读书</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[魅蓝3s移动版刷机+修改串号]]></title>
    <url>%2F2017%2F09%2F17%2F%E6%95%99%E7%A8%8B%E5%88%86%E4%BA%AB%2F2016-10-24-meizu-mobile-meilan3s-imei-change%2F</url>
    <content type="text"><![CDATA[闲鱼上买了一个全新的魅蓝3s移动版，虽然比在官网买便宜了130块，但是移动定制版，开机logo，系统软件，功能限制，网络限制，特恶心了，果断刷机。 刷机教程刷机教程魅族论坛有。 移动定制版的系统recovery会验证固件，因此直接刷全网通版的固件会提示固件损坏，无法写入固件。 使用flashfire刷入全网通的update.zip， 首先获取root权限，安装superSU，并重启。 然后flashfire写入，等待系统自动重启。这时的系统并不是clean的，会有各种问题，相当于只是从移动版升级到全网通，绕过系统的rom检测。 开机后，使用系统自带的recovery再刷一次全网通的固件，刷时选择清除数据。这是最后一步，然后就是全网通版的魅蓝3s啦。如果还是有相机图库无法使用的情况，恢复出厂设置，或再刷一次全网通固件，记得选清除数据。 修改串号 在拔号按##3646633##–&gt; 进入connectivity 选 cds information –&gt; radioinformation 选 phone1(SIM1) 在command (有A+的位置)列按入”AT +EGMR = 1,7,”你的IMEI””(在输入sim2时改为”AT +EGMR = 1,10, “你的IMEI”) 按 “SEND ATCOMMAND” 完成后重启手机 注意：步骤4中AT和+加个空格（AT +EGMR=1,7,””像这样，+号前加个空格） 这个不用root 我试过可以改魅族note2 以下是网上找到的参考 系统必须降价到Flyme 4.5，因为Flyme5.1工程模式(拨号界面输入*#*#3646633#*#*)没有cds information 选项，无法进行下一步操作； 选 cds information --&gt; radioinformation之后，选择phone1(SIM1) or phone2(SIM2)时候，最好把2个都搞一下，因为我才开始选择phone1(SIM1)，但是输入*#06#后，还是原来的串号，但是工程模式下却是我要修改的串号，但是再改phone2(SIM2)时候，两个就一样了； 在输入AT + EGMR = 1,7,”你的IMEI”这一串代码时候，请注意，把原来的AT+删掉，直接输入这一串代码-----AT + EGMR = 1,7,&quot;你的IMEI&quot;（AT和+之间有空格，一定要有）； 改完之后英文显示ok，要重启，然后再看是不是已经改成功了 用工具侠或者移动叔叔 应该也是要在Flyme4.5下进行，昨天我是在Flyme5.1下弄的，始终不行 使用工具侠查看iemi，更改成功。]]></content>
      <categories>
        <category>资源</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[我的装机必备软件]]></title>
    <url>%2F2017%2F09%2F17%2F%E6%95%99%E7%A8%8B%E5%88%86%E4%BA%AB%2F2016-07-20-software-tools-on-mylaptop%2F</url>
    <content type="text"><![CDATA[windows美化 RocketDock 实现mac上的Dock栏 TrueLancherBar 目录式的任务栏快捷操作入口 Q-Dir 1分4的资源管理器 Fliqlo 极客感十足的时钟屏保 fences2.01 桌面整理，提高效率 win10安装破解后，如出现桌面图标无法移动，需要安装破解补丁 破解补丁 实用工具 QQ Internet 国际版QQ，功能少，无广告，消息多时不卡顿 魔影工厂 音视频格式转换工具 镜像工具 DAEMON 虚拟光驱 V-Disk 虚拟光驱 UltraIso 光驱加载刻录 win32diskImager 镜像刻录 编辑器 MarkdownPad2 notepad++ 替换系统的记事本，打开大文件不卡死，语法高亮，可辅助编程 sublime text 3 同上，作为Python编辑器很方便 ultra edit 对二进制支持友好 虚拟机与镜像 VMware VirtualBox 客户机系统镜像下载，解压直接使用 360隔离沙箱 ：用来运行从网下二进制小程序， docker 翻墙工具 lantern ：自己编译生成的exe是免安装，无流量限制的 xx-net ：基于GoAgent实现的，第一次部署比较麻烦，流量每天1G*n（GAE apps） ss：配合ss服务器食用 安卓软件 微博国际版 日事清 Newton 邮件客户端 JuiceSSh 双开助手 BusyBox + 终端模拟器]]></content>
      <categories>
        <category>资源</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[明信片互寄]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%94%9F%E6%B4%BB-%E7%94%B5%E5%BD%B1%E9%9F%B3%E4%B9%90%E6%97%85%E8%A1%8C%E6%91%84%E5%BD%B1%2FXX-2016-06-12-postcards-sending%2F</url>
    <content type="text"><![CDATA[想要和我互寄明信片的留言地址哦~~ 好久没寄明信片了，手上还有很多明信片。记得大一时经常和网友互寄明信片，每次去check信箱时都会特别开心，我的明信片都是室友老曹代写的哈哈，他字写的好看^.^ 下学期打算开始新的明信片互寄，现在信箱地址还不确定，先写下这篇文章占位，后期再更新~~ 相识是一种缘分。哈哈。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>明信片</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机考研个人经历]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%94%9F%E6%B4%BB-%E7%94%B5%E5%BD%B1%E9%9F%B3%E4%B9%90%E6%97%85%E8%A1%8C%E6%91%84%E5%BD%B1%2F2016-07-22-Something-with-NEEP-of-WHU%2F</url>
    <content type="text"><![CDATA[本文写于2016年07月21日，因为有很多人问我考研经验和各种问题。回答每个宝宝的十万个为什么真的很心累，而且有些人也没下定决心要考研武大。 我是16年考研，本科武大计院，报考本院学硕计科一级学科，二级学科信息安全，具体方向信息系统安全，最后进了通信与信息系统。初试359，73,76,88,122。初试分数线335。复试跪了，调剂进了通信专业。 我很菜的，但这种考试都是从小考到大，只要我按计划复习，刷刷题，结果都不会差，所以我觉得这种机械应试能力一般人还是干不过我的。其实也很苦逼，一个人搬出宿舍租房子，每天自习室待到10点，手机换成黑白屏的诺基亚，A4纸用了大概1K张。 以下内容全是瞎扯的，个人观点不要当真，仅供娱乐，概不负责。 十万个为什么 Q of All 高中学习，不会有人教，考哪里有人圈重点，定期测验检验你的学习效果，周围人都在学习的氛围很好，还没考上好大学的话，可能是智商不够，运气差，或者当时心智不成熟。 but，现在你决定要考研，天时地利全都没有，高校保研的人数大概是考研名额的2~3倍。150万的考生3分之1有学上，想上个好点的，大概10分之1的可能性吧。 Q1、哪个专业好考？ 国软&lt;计院 计院内：通信&lt;软工&lt;计科 专硕&lt;学硕 ，非985,211就别考学硕了，根本没戏，因为名额太少，但是有夏令营资格的就很easy。 Q2、夏令营啥情况？ 优秀营员可以免复试录取，初试过国家线即可。大概考300分就够了。 怎么参加别问我，大概时间是6、7月份，关注计院网站通知吧。形式上午笔试，下午面试。 Q3、普通本科学校能考上985吗？ 能啊，专硕还是比较容易的。你要是考不过初试线，那就算了吧。 复试会被歧视吗，会，如果你的学校跟武大有仇，比如某学校来的孩子搞坏了上百万的实验室设备，以后都没招过这个学校的学生。 会有劣势吗，会。但专硕复试中自少一半的人不是名校来的，最重要的是要有实力，学历，颜值，专业能力，交往都是实力的一部分。 Q4、武大计算机考研相关资料 计算机学院的专业课933，包括计组、计网、计操，数据结构。英数政公共课没啥说的。 资料书，参考书 专业课用业内好评的教材就可以，我觉得武大的教材并不好，只用了天勤的4本+天勤8套模拟（错误真多）+王道的8套，其他品牌的辅导书都是垃圾，买了一本新东方的看了第一章就把书给撕了。其实天勤的只数据结构出色，其他的建议用王道考研。非计算机专业的还是有必要好好看看教材的。 英语，张剑150篇，考试虫作文，张剑模拟6套，真题。单词书用的新东方的，真厚，很多不是重点单词，浪费我时间，推荐何凯文的。我不喜欢看视频教程，虽然下了很多，这里就推荐一下何凯文的单词演讲视频吧，可以记下大概200个高频词汇，对单词也有更深的认识，背单词书或做阅读时一眼就能看出哪些不是重点单词。 政治，肖1000，肖4，风中劲草，蒋政治全书，肖时政，真题。 数学，复习全书一套含（模拟题，真题），单独真题。 我把我用过的资料整理了一下，挂在闲鱼上了。 Q5、你报的哪个方向？ 前文已讲。学硕有通信、软工、计科，下设二级学科，然后是对应研究方向。专硕不太清楚，应该是没分二级学科这么具体，而是录取后在选择的？ Q6、你认为你的导师怎么样？ 你想干嘛！？ 武大的老师无论学术人品有啥好怀疑的？有的实验室做的项目或工作会比较坑，和导师没关系。提前联系导师吗？外校的还是确定能进复试在说吧。。。 Q7、参考书 见下文 Q8、我有学校内部消息吗？ 抱歉，啥也没有！ Q9、考研期间你觉得最重要的是啥？如何复习的？ 我没想那么多。复习见下文。 ​ 初试经验 坚持就是胜利 坚持到最后也不一定会有好下场的！ 方法 如果你比别人更努力更辛苦,却还是担心考不上，你就要想想工地上的工人流的汗水最多，是不是应该拿最高的工资呢！ 状态 我就没有找到过最好学习的状态，所以该怎么学就怎么学喽~ 辅导班和资料书 反正我觉得没必要，浪费我时间和money。参考书，买书要慎重，买了就要看，发现不好用，赶紧扔了。。。 数学 复习的时间最长，考的最烂。 考试考蒙了，做错了2道简单的大题，本来以为数学只能考60分。 复习全书看了2遍，第一遍做了例题和课后题，第二遍只做了例题。我后悔做章节的题了，浪费大量时间，卵用没有，又偏又难考试根本不会用这种为了出题而出题的角度去出题。 真题做了2遍，模拟题一遍。全是按考试来模拟的，用A4纸写答案，后来装订了一本。真题第一遍得分100左右，然后做模拟，得分70~90。最后第二遍真题得分130~140。感觉数学1也就那样，考120分没问题啊。结果出题难度堪比模拟，很多人出来大呼坑爹，我当时也是心态爆炸。 考试时的草纸让我震惊啊，比A5纸还小一点，只有一张，还是彩虹系列，粉的，黄的，蓝的，绿的，4场考试4种颜色。当然草纸不够用可以用试卷或再问老师要，大多数人都没要第二张草纸，所以大家都很牛逼啊。 考的不顺利，别慌。遇到太难的就放弃。下午还有专业课呢，虽然数学炸了，专业课扳回一局也是可以的。 英语 单词 选一本薄的词汇书啃，看到单词知道2个意思就OK了。只看每页的最下边有助记词汇，不认识的画一个圈。我这样快速背了5遍，有大概300个5星词汇，整理笔记之。 做阅读时记单词，从张剑阅读里整理了大约200个词汇，真的是高频词汇反复出现。 做模拟题真题时整理的词汇。 前期整理了大概900多单词，9月份就把单词书扔了，改背笔记。10月中旬又觉得好多单词没必要背，从中选了大概400个，一直背到考试。 阅读 前期就当熟悉套路了，主要记单词。一般5错3，一次一个小时做4篇。有时候觉得答案解析都不能说服我，哈哈。后期单词跟上了，阅读也适应了，大概5错1。 作文 预期考个70%的分数的，就整理模板吧。 我用的考试虫，但我觉得他的模板也不是很好，感觉就是能得50%的分。我一共整理了10篇小作文模板和7篇大作文模板，反正原模板只要有我觉得在考场写起来不顺手的一律改成自己喜欢的或干脆不用那句话。如果用模板能在考场一气呵成，十分开心的迅速写完作文，目的就达到了，只要不跑题，字体还可以，语法不出错，我不信得分会低。但是你要是写不完或是草草结尾，作文分数就祈祷上帝吧。做模拟或真题时我就是这种状态，10分钟小作文，20分钟内完成大作文，写完读起来自己都觉得，就我这小水平给我一天时间来改还是这样。 完型、新题型、翻译 这些是有技巧的，我是直接放弃专门训练。用模拟和真题练练感觉就OK了。完型2错1，另外2个拿50~70%的分数就有很大的性价比了。翻译一般2分，分成4个0.5，直译不行就歪译，歪译也不行，就1句话拆2或2合1，改变语序位置之类，遇到是在不认识的单词就蒙呗。新题型说到底还是看阅读能力。 政治 复习全书要看死宝宝了，看一章，做一章肖1000题。然后就知道知识点了，整理错题及笔记后扔掉2本书。 风中劲草虽然全是知识点，但我一点也看不进去。复习还是主要靠笔记。 然后认真整理了马原的知识点，好像是辩证法、认识论之类的，反正答题起来一套一套的，然后做了2010年以前的真题，按考试模拟，用的A4纸写答案。就是在你觉得知道很多却没记住或写不出来的状态下去找感觉，然后就有一种蜜汁自信，哈哈，感觉怎么考都能考60分。 肖八套模拟尽量早作，肖四套和近5年真题是重点，后期练手时做。肖4是要背的，没啥好说的，肖八如果有精彩的让你非背不可，就背一下吧。 时政和世界经济，关注和党、和中国相关的，这是重点，谁会关心像法国恐怖事件之类的事呢，我们只关心中国是热爱和平反对恐怖主义的。 政治答题模式，做真题时对比答案就会发现。马原主观题官方答案很有条理。我答题都是123456小标号，逻辑清晰，每一条踩中他就得给我分，踩偏的也不会扣分。所以我主观题部分估计得分在40+，客观选择题错了不少，毕竟政治复习用的时间最少，又憎恨看书背书。 关于对我党的个人看法：这直接决定你政治能不能学好。很多人对党有偏见，党员腐败问题严重也是事实，但是党内绝大多数人都是社会精英啊，总不会让一群蠢材去领导13亿人民吧。你入不入党无所谓，难道考研不是为了成为社会精英，变得更优秀嘛。以后大部分党员都是大学生（知识分子），而且社会精英人才大部分也是大学生，所以政治教育是为了培养政治素养和共同信仰，一块做中国梦喽。从感性的角度讲就当是体量党的良苦用心吧，现实生活中搬砖才是正解。 专业课 因为学硕和专硕考的不太一样，公共课你们也有自己的学习方法，专业课因为我是本科计算机，跨考的我就不知道怎么复习了。 专业课是自主命题，计算机学院的科目代码933计算机综合，包含计组计网计操数据结构。考研大纲官方说用的是全国卷408的大纲。呵呵，也就是说没有大纲。出题难度比408简单，范围小，有的地方偏。 专业课书籍，计科的可以不看书，看王道复习就可以了，实在不理解了，在去翻课本吧。非计科的学生，推荐计网：谢希仁的；数据结构：清华严蔚敏或其他C/c++的口碑好的，不建议用java版；操作系统：国内大学教材谁的好用谁的；计组：清华版的。别用专业类的书或国外的书，国内教材都是偏原理很肤浅的适合考试，而且各家抄来抄去，你懂得。所以别太纠结这个教材哪家强的问题。 我复习时直接刷完了4本天勤的书，做完习题就做8套模拟和真题了，后来发现408的模拟跟武大的933有很多出入，好坑啊，浪费很多时间。所以先做武大真题，在做8套模拟。 数据结构：偏爱树，线性表太简单，图太难，不会出编程题，只有二叉树。其他的算法也要会用类c写出来，我能告诉你我是整理到笔记上，然后背下来的嘛！图的几个复杂算法太难背了，至少要知道原理及功能，万一考到了，写上结构体，几个函数定义，加上注释也能拿分的。程序题我是爱写注释的，让你看手写的代码你也会发疯的好吧。 计网：网络拓扑年年都考啊，子网划分之类的，一道大题啊。然后就是其他的协议之类的 计组：这个是真难，还好没考到微处理器，指令微操作之类的，不然准跪。考了程序数据和内存映射的关系，这个我也是瞎做的。第二个大题是cpu和内存，cache之间的关系，这个简单。408好像爱考进制转换之类，编码海明码求解之类的要算半天的那种题，超级费劲，933从来补考，我在那上面浪费很多时间搞明白了，后来才发现真是日了狗，408去掉这些难度要降一成，而且408不考微操作，933考了好几次。cpu、内存、cache映射这个是重点。 操作系统 互斥和同步，涉及写伪代码算法，是大题。 其他的概念性的题居多，考选择，不难。 复试 我复试跪了，面试表现不太好。复试学硕是19选10，但有6个夏令营已经确定了，于是变成13选4，有3个武大的，另外一个是普通大学，但初试分很高。所以竞争残酷，而自身实力不济，不想多说。 笔试（占复试成绩40%） 数据库原理（计科，软工） 找本习题书啃下来就OK了 关系数据库，范式啥的是重点 数字信号处理（通信） 具体不清楚 英语听力 6级难度，但是形式完全不一样，全选择题，有题目和选项，听力内容全是场景应用型的，需要在听懂的基础上稍作分析，给的备选答案也不是在原文中就能找到的，比如两个人对话又聊上课又聊考试又聊兼职，最后问你对话发生在哪里。听懂对话真的很容易，6级只是难在我听不懂听力说的啥。 都是100分的卷面，听力占笔试10%，专业课占90% 面试（60%） 英语口语（占面试10%） 自我介绍 讲个人项目经历 问答（90%） 问项目的东西较多 问你的研究方向 机考（专硕） 编程题，c或java实现，能运行的那种。 好像有选择题？ 都很简单，hello world级别，是考你基础，不是选拔，毕竟还有跨考的，就算是计科的，大多数编程也很菜，考研期间也很少写代码，熟练度也会降低之类的。 编程IDE学校实验室电脑装有DEV-c，vc++6.0，Java eclispe，vs10，这不是重点 肯定准备复试的时候要码几个简单的小程序练练手 看看往年机考的原题 总评总评=初试成绩的60%+复试成绩的40% 整体学习节奏 学硕我就觉得数学最难了，用的时间也最长。专硕考试好像各科都简单，具体情况自行分析。 暑假开始前背了3遍单词书，做了张剑阅读150，做完一遍数1全书 7月第一周回家休息了，然后回校复习。做了第二遍数1，英语不记得看的啥了。 8月开始政治全书加肖1000，数学继续啃书，英语还是在背单词做阅读。 9月重点专业课，10月中旬才看完。看完第二遍数1。做完肖一千。英语阅读和单词。 10月做了政治真题2010年前的英语作文模板背诵，各科基础复习都finish了。 单词一直在看，笔记也一直在做。 我做笔记一般整理解题思路和错题，重要背诵知识点，单词。做笔记的目的就是要看的，11月份我基本上复习书都扔一边了，考试前一周随身带的只有笔记和各科试卷题。 11月搬出宿舍租房子去了，开始8点到自习室，10点回去，全天刷题模拟考的模式，一直到考研。淘宝买500张A4纸才不到20块，把答题纸最后订一块绝对是复习查错的最佳材料。 每天计划 早上 9点开始，30分钟单词，数学。 下午 2点开始，英语阅读1h，整理半小时，然后数学 晚上 数学或专业课 10月份以后是8点到自习室，上午数学or政治，下午英语，晚上专业课。 整体计划 暑假前快速看完一遍数学和词汇 暑假重点是英语和数学，搞定基础复习 9月10月重点专业课和政治，10月底结束各单科复习 11月刷题模拟 总结 为啥要考研啊 考研真难真累]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2016年玩过的游戏]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%94%9F%E6%B4%BB-%E7%94%B5%E5%BD%B1%E9%9F%B3%E4%B9%90%E6%97%85%E8%A1%8C%E6%91%84%E5%BD%B1%2F2016-06-01-Game-of-2016%2F</url>
    <content type="text"></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>Enjoy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对于一致平均问题的理解]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2014-08-22-Average-Consensus%2F</url>
    <content type="text"><![CDATA[最近几个月在研究分布式低秩矩阵补全的问题，参考文章《低秩矩阵补全》。 低秩矩阵补全问题的各类算法中，很关键的一个子问题叫做一致平均问题(Average Consensus)，而上一篇文章并没有对这个问题进行说明。那么，什么叫做“一致平均”呢？ ###问题阐释： 考虑一个有个节点的网络，每个节点上存储一个关于自己的数值信息，叫做初值。一致平均问题就是使所有节点在算法停止的时候收敛到个初值的平均。 在分布式算法中，通常会存在这样一个变量，它作为公有信息在网络中传递，每个节点储存自己的。在总算法的每一次迭代中，每个节点接收自己邻居节点传递过来的公有信息，然后与自己的私有信息共同计算出新的公有信息，并传给自己的邻居。我们需要保证每个节点上的公有信息，使分布的算法以某种方式交流合作，以便获得最优解。这就是分布式算法中的一致平均问题。 ###问题分类：根据节点上的数值信息是否随时间变化，又把一致平均问题分为： 静态一致平均 (Static average consensus) 动态一致平均 (Dynamic average consensus) 顾名思义，静态一致平均指节点上的初值不会发生变化，只需要保证最后每个节点都收敛到个初值的平均值即可；而动态一致平均则是，节点上的数值不断发生变化，即在时刻的值并不一定与0时刻的值（初值）相同，我们使用不断变化的公有信息（因为公有信息不断在被更新），仍需要保证最后每个节点都收敛到个初值的平均。相比于静态一致平均，动态一致平均问题更为棘手。 ###求解方法分类：在文章《低秩矩阵补全》的最后，提供一种求解方法的分类概念，将方法分为： 精确一致平均 (Exact average consensus) 不精确一致平均 (Inexact average consensus) 这又是什么意思呢？ 在解决分布式低秩矩阵补全问题的时候，我们将算法分解两个子问题不断求解，一是交替极小化(Alternating minimization)得到每个节点的新的,；二是在网络中对个节点的求一致平均。对于第二个子问题，在每一次算法总的迭代中，都去求解精确的一致平均显然能够解决问题，但是因为一致平均也需要一定次数的迭代才能被解除，如果在总算法的每一次迭代中都去求精确的一致平均，则相当耗费计算资源，增加了算法的时间复杂度。 很自然地，我们会想到，既然一致平均只是矩阵补全的一个子问题，我们是不是可以通过某种松弛，来降低算法的时间复杂度并节省计算资源，同时仍旧保证总算法的收敛呢。这样，就提出了不精确的一致平均。 不精确一致平均，就是在每一次求解一致平均子问题时，只迭代一次或者若干次，而不是迭代所有次（可以很大），使每个节点近似的达到它们初值的均值。 ###精确一致平均与不精确一致平均优缺点比较: 精确(Exact)求解：理论上容易证明，但计算代价通常比inexact方法高 不精确(Inexact)求解：理论分析上不好证明，除此之外具有exact不具有的所有优点，比如算法时间复杂度低，节省计算资源等等。]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SNMP调研]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2016-07-04-Survey-of-SNMP-protocol%2F</url>
    <content type="text"><![CDATA[SNMP的发展应用调研报告第1章 SNMP基础概述1.1、SNMP简介简单网络管理协议（SNMP），由一组网络管理的标准组成，包含一个应用层协议（application layer protocol）、数据库模型（database schema）和一组资源对象。该协议能够支持网络管理系统，用以监测连接到网络上的设备是否有任何引起管理上关注的情况。SNMP是internet协议簇的一部分,目标是管理互联网Internet上众多厂家生产的软硬件平台。 SNMP产生背景：网络的迅速发展和普及使得以下几个问题日益突出： 网络规模逐渐增大，网络设备数量成级数增加，网络管理员很难及时监控所有设备、发现并修复故障； 网络设备很可能种类不同、生产厂家不同、型号不同，如果为每种网络设备都提供一套独立的管理接口和协议，将使网络管理越来越困难。为解决以上两个问题，一套覆盖服务、协议和管理信息库的标准——SNMP孕育而生。 SNMP网络包含3个组件： 被管理的设备每一个被管理的设备自身都在维护一个管理信息库(MIB),用来收集存储管理信息。 SNMP代理（Agent）运行在被管理的设备上的网络管理软件模块，能将MIB中的信息转换成为SNMP协议兼容的格式并发给NMS，同时SNMP Agent使用UPD协议在161端口上接收的NMS请求。。 网络管理系统（NMS）NMS实现监控设备的功能，为网络管理提供相应的处理程序和存储资源。SNMP manager可以使用任意端口向Agent发出请求。SNMP Manager使用162端口接收请求。 SNMP工作原理：通常由一个系统管理一个或多个设备或系统，被管理的系统上运行SNMP代理，并通过SNMP协议对管理系统报告信息，管理系统可以通过SNMP协议询问设备信息，发送配置更新或设备控制请求。所有的信息元数据都以管理信息库(MIBs)的方式描述。 SNMP工作模型如下图： 1.2、协议发展历史&gt; 目前SNMP的最新标准是IETF在2004年通过 RFC 3411–RFC 3418确定的SNMPv3，V3之前的版本在Internet Standard 标准中被废止。 1989年—— SNMPv1 1991年—— RMON(Remote Network Monitoring 远程网络监视)，它扩充了SNMP的功能，包括对LAN的管理及对依附于这些网络的设备的管理。RMON 没有修改和增加SNMPv1，只是增加了SNMP监视子网的能力。 1993年—— SNMPv2(SNMPv1的升级版) 1995年—— SNMPv2正式版，其中规定了如何在基于OSI的网络中使用SNMP 1995年—— RMON扩展为RMON2 1998年—— SNMPv3，一系列文档定义了SNMP的安全性，并定义了将来改进的总体结构，SNMPv3可以和SNMPv2、SNMPv1一起使用。 目前广泛应用的协议版本是SNMPv2c和SNMPv3 图：SNMP发展历程 图：SNMP版本改进 1.3、SNMP相关术语速查 SNMP：Simple Network Management Protocol(简单网络管理协议)，是一个标准的用于管理基于IP网络上设备的协议。 MIB：Management Information Base(管理信息库)，定义代理进程中所有可被查询和修改的参数。 OID：对象标识符，是SNMP代理提供的具有唯一标识的键值，记录在MIB中。MIB 提供数字化OID到可读文本的映射。MIB的数据是树状结构，可以使用snmpB之类的工具进行查询、搜索存储在MIB中的被管理对象。设备厂商可以向IEEE在MIB库申请注册一个分支结点，其下的每个叶子结点对应该设备厂商的实现的功能对象OID。 比如，CloudStack使用的MIB分支是：1.3.6.1.4.1.18060.15 ，其中.1.3.6.4表示该OID是以.iso.org.dod.internet.private开头的，可以从MIB根结点进行完全验证，其后的.1是表示企业，18060 是分配给apache的企业代码，15是分配给CloudStack项目的根代码，所有CloudStack的SNMP OID代码都在1.3.6.1.4.1.18060.15 分支下。 SMI：Structure of Management Information(管理信息结构)，SMI定义了SNMP中使用到的ASN.1类型、语法，并定义了SNMP中使用到的类型、宏、符号等。SMI用于后续协议的描述和MIB的定义。每个版本的SNMP都可能定义自己的SMI。 ASN.1：Abstract Syntax Notation One(抽象语法定义)。用于定义语法的正式语言，在SNMP中定义SNMP的协议数据单元PDU和管理对象MIB的格式。SNMP只使用了ASN.1中的一部分，而且使用ASN.1的语言特性定义了一些自定义类型和类型宏 ，这些组成了SMI。 PDU： Protocol Data Unit(协议数据单元)，它是网络中传送的数据包。每一种SNMP操作，物理上都对应一个PDU。 NMS： Network Management System，网络管理系统，又名网络管理站，简称“管理站”。它是SNMP的总控机，提供统一的用户界面访问支持SNMP的设备，一般提供UI界面，并有统计、分析等功能，是网管系统的总控制台。NMS是网络管理操作的发起者。 Agent： 是SNMP的访问代理，简称“代理”，为设备提供SNMP能力，负责设备与NMS的通信。 Proxy： 代理服务器，对实现不同协议的设备进行协议转换，使非IP协议的设备也能被管理。 Trap： 是由设备主动发出的报警数据，用于提示重要的状态的改变。 BER： Basic Encoding Rule，基本编码规格。描述如何将ASN.1类型的值编码为字符串的方法。它是ASN.1标准的一部分。BER编码将数据分成TLV三部分，T为Tag的缩写，是类型标识；L为Length的缩写，标识类型的长度；V为Value的缩写，标识数据内容。按照TLV的顺序对数据进行编码，生成字节流。SNMP使用BER将SNMP的操作请求和应答编码后进行传输，并用于接收端进行解码。 第2章 SNMP 分析2.1、SNMP 结构模型 SNMP基于TCP/IP协议工作，对网络中支持SNMP协议的设备进行管理。因此管理员可以使用统一的操作进行管理设备，而不必理会设备是什么类型、是哪个厂家生产的。如下图， SNMP支持的网络管理操作，如下图： Get：读取网络设备的状态信息(设备的配置、参数、状态等)。 Set：远程配置设备参数。 Trap：设备主动汇报重要状态信息。 在具体实现上，SNMP为管理员提供了一个网管平台(NMS)，又称为管理站，负责网管命令的发出、数据存储、及数据分析。被监管的设备上运行一个SNMP代理(Agent))，代理实现设备与管理站的SNMP通信。如下图， 管理站与代理端通过MIB进行接口统一，MIB定义了设备中的被管理对象。管理站和代理都实现了相应的MIB对象，使得双方可以识别对方的数据，实现通信。管理站向代理申请MIB中定义的数据，代理识别后，将管理设备提供的相关状态或参数等数据转换为MIB定义的格式，应答给管理站，完成一次管理操作。 网络上的许多设备，路由器、交换机等，都可以通过添加一个SNMP网管模块而增加网管功能。服务器可以通过运行一个网管进程实现。其他服务级的产品也可以通过网管模块实现网络管理，如Oracle、WebLogic都有SNMP进程，运行后就可以通过管理站对这些系统级服务进行管理。 根据管理者和被管理的设备在网络管理操作中的不同职责，SNMP定义了3种角色。如下图， 网络管理系统：又称管理站、NMS。是系统的控制台，向管理员提供界面以获取与改变设备的配置、信息、状态、操作等信息。管理站与Agent进行通信，执行相应的Set和Get操作，并接收代理发过来的警报(Trap)。 代理：Agent是网络管理的代理人，负责管理站和设备SNMP操作的传递。介于管理站和设备之间，与管理站通信并相应管理站的请求，从设备获取相应的数据，或对设备进行相应的设置，来响应管理站的请求。代理也需要具有根据设备的相应状态使用MIB中定义的Trap向管理站发送报告的能力。 代理服务器：Proxy是一种特殊的代理，在不能直接使用SNMP协议的地方，如：异种网络、不同版本的SNMP代理等情况，Proxy代替相关设备向管理站提供一种外观，为设备代理SNMP协议的实现。Proxy做了异种网络或不同版本代理和相应SNMP数据请求的转换工作。如下图， 2.2、ASN.1、BER、SMI、MIB、PDU的关系 ASN.1：高级的数据描述语言。描述数据的类型、结构、组织、及编码方法。包括符号和语法两部分。SNMP使用ASN.1描述PDU和管理学对象信息库MIB。 BER：ASN.1的基本编码规则。描述具体的ASN.1对象如何编码为比特流在网络上传输。SNMP使用BER作为编码方案，数据首先先经过BER编码，再经由传输层协议(一边是UDP)发往接收方。接收方在SNMP端口上收到PDU后，经过BER解码后，得到具体的SNMP操作数据。 SMI：是SNMP的描述方法。规定了使用ASN.1子类型、符号。ASN.1功能强大，但SNMP只用到了其中很小一部分，对于这一部分内容的描述，限定了范围，即为SMI。SMI规定了使用到的ASN.1类型、宏、符号等。SMI是ASN.1的一个子集和超集。 MIB：是SNMP中使用到的管理信息库。定义了数据格式、类型、顺序、意义等，使用SMI中定义的类型和ASN.1中的基本类型对对象进行描述，是一个使用SMI描述的管理信息库。每一类关心的事件都有一组MIB，比如网络接口有一颗MIB树，TCP有一颗MIB树，UDP也有一颗MIB树。 PDU：是SNMP的协议数据单元。PDU是基本的通信格式，使用ASN.1描述，使用BER编码，通过传输层协议传送。 SNMP协议更多细节参考 RFC文档 2.3、SNMP报文格式SNMP请求/响应报文 简单网络管理协议允许网络管理工作站软件与被管理设备中的代理进行通信。这种通信可以包括来自管理工作站的询问消息、来自代理的应答消息或者来自代理给管理工作站的自陷消息。SNMPv1实现起来很简单，其包含5个请求/响应原语： get-request set-request get-next-request get-response trap 报文格式 | IP首部 | UDP首部 | 版本 | 共同体 | PDU类型（0－3） | 请求标识 | 差错状态（0－5） | 差错索引 | 名称 | 值 | 名称 | 值 | … | | PDU类型4 | 企业 | 代理地址 | Trap类型（0－6） | 特定代码 | 时间戳 | 名称 | 值 | … | | PDU类型 | 名称 | | 0 | Get-request | | 1 | Get-next-request | | 2 | Get-response | | 3 | Set-request | | 4 | Trap | 第3章 SNMP API3.1、SNMP4J (JAVA) SNMP4J是一个用Java来实现SNMP(简单网络管理协议)协议的开源项目.它支持以命令行的形式进行管理与响应。SNMP4J是纯面向对象设计与SNMP++(用C++实现SNMPv1/v2c/v3)相类似。 SNMP4J API 提供以下下特性： 支持MD5和SHA验证，DES，3DES,AES128、AES192和AES256加密的SNMPv3。 支持MPv1,MPv2C和MPv3，带执行的可阻塞的信息处理模块。 全部PDU格式。 可阻塞的传输拓扑。支持UPD、TCP、TLS 。 可阻塞的超时模块。 同步和异步请求。 命令发生器以及命令应答器的支持。 基于Apache license的开源免费。 JAVA 1.4.1或更高版本(2.0或更高版本需要jdk1.6及以上的支持)。 基于LOG4J记录日志。 使用GETBULK实现Row-based的有效的异步表格获取。 支持多线程。 参考链接： AGENT++ snmp.software - SNMP Tools and Libraries for Developers SNMP4J - Free Open Source SNMP API for Java 3.2、SNMP++ (C++)SNMP++是一个用C++来实现SNMP(简单网络管理协议)协议的开源项目，支持SNMPv3。 官网:AGENT++ snmp.software - SNMP Tools and Libraries for Developers 3.3、Net-SNMP (Linux)Net-snmp是一个开源的SNMP项目。最新版本为：5.7.2。net-snmp支持snmpv1、snmpv2、snmpv3，支持基于IPv4和IPv6的SNMP应用程序。 Net-snmp提供完整的API用于SNMP应用程序开发，包括C和Perl的API 。 一个功能强大且可扩展的SNMP代理：snmpd开发者可以开发动态模块扩展snmpd，net-snmp内置扩展子代理与主代理的通信协议。 提供众多命令行工具检查和使用SNMP协议 一个图形化的MIB浏览工具 一个Trap接收进程，用于接收和显示Trap，并将Trap记录到日志文件中。 [Net-SNMP的官网](http://www.net-snmp.org/download.html) 3.4、WebNMS for Android直接使用SNMP4J进行Android开发可能会出现问题，WebNMS for Android 具有以下特点： Simple: Add our AdventnetSnmp.jar to your android project and start developing the application. Multi-lingual support: Complete support for SNMPv1, SNMPv2c and SNMPv3. SNMPv3 security: Support for HMAC-SHA-96, HMAC-MD5-96, CBC-DES, CBC-3DES, CFB-AES-128, CFB-AES-192, CFB-AES-256 bit encryption. MIB Loading: Option to load MIB definitions from a pre-compiled file or a Serialized file to boost the performance. IPv6 (Internet Protocol Version 6) support: Provides connectivity with IPv6 and IPv4 based devices. SNMP Broadcasting: Broadcasts snmp packets across the network to auto-discover snmp devices in the network. a lightweight Java library :provides off-the-shelf components for trap and table handling along with basic SNMP operations, such as SNMP GET, SNMP GETNEXT, SNMP GETBULK, and SNMP SET. 官网:WebNMS SNMP API Android Edition - Java SNMP API/ SNMP Stack/ SNMP Library for Android Platform 3.5、SnmpSharpNet SNMP Library for C# 参考 SnmpSharpNet | SNMP Library for C# 3.6、pySNMPpySNMP 是纯 Python 实现的 SNMP v1/v2c/v3 Complete SNMPv1/v2c and SNMPv3 engine support Can act Manager and/or Agent role Manager and Agent side MIB support Asynchronous operations support Pure-Python implementation Runs on Python 2.4 through 3.2 py2exe and .egg friendly Twisted binding 参考SNMP library for Python download | SourceForge.net 3.7、SNMP Agent 相关开发库3.7.1、Agent++ 基于SNMP++实现的一个C++框架，用于SNMP代理的开发。 AGENT++ 支持 Linux, Solaris, HPUX, AIX, and Windows XP, 7, and 8 (VS 2010-2015)，需要 SNMP++ version 3.x 及以上. 由于AGENT++ 是 ANSI C++ 兼容的, 理论上也可以运行在其他平台包括嵌入式系统。 参考AGENT++ API 3.7.2、AgentX++ AgentX协议允许独立自治的sub-agent设备与网关上的master agent设备进行通信。 AgentX++ 在Agent++的基础上实现了完整的AgentX协议支持。 支持平台：Linux, Mac OS X, Solaris, cygwin, and Windows XP/7/8 (only TCP). 参考：AgentX++ API 3.7.3、SmartSNMP agent SmartSNMP是一个小巧的、易配置的网络管理代理，支持SNMPv1/v2c/v3以及AgentX协议。它使用C99和Lua5.1写成。支持PC平台包括Linux和BSD，嵌入式平台如OpenWRT。 支持SNMPv1/v2c/v3，使用Lua写私有mib 参考：FSEN :: SmartSNMP SmithSNMP –SmartSNMP的独立分支版本，项目地址：github 第4章 Net-SNMP编译与开发实验环境: linux-debian 8,net-SNMP 5.3 4.1、搭建SNMP服务环境 安装snmp 1apt-get install snmp snmpd 安装mib库 1apt-get install snmp-mibs-downloader 参考debian-wiki-snmp 查看默认配置文件 12cat /etc/snmp/snmpd.conf cat /etc/snmp/snmp.conf 启动SNMP 12# service snmpd start# snmpd -C -c /etc/snmp/myconfig.conf ###-C不适用默认配置文件 -c指定自定义配置文件 4.2、编译、安装Net-SNMP 源码下载地址 编译 12345 解压源码:# tar -zxvf net-snmp-5.7.3.tar.gz 查看编译配置: ./configure --help开始编译： ./configure make make install 如果编译报错lperl，使用 apt-get install libperl-dev net-snmp提供的命令工具 123456root@kali:~# snmpsnmp-bridge-mib snmp-check snmpget snmpset snmptrapsnmpbulkget snmpconf snmpgetnext snmpstatus snmptrapdsnmpbulkwalk snmpd snmpinform snmptable snmpusmsnmpc snmpdelta snmpkey snmptest snmpvacmsnmpcheck snmpdf snmpnetstat snmptranslate snmpwalk snmpget 模拟snmp的GetRequest操作的工具。用来获取一个或几个管理信息。用来读取管理信息的内容。 snmpgetnext 模拟snmp的GetNextRequest操作的工具。用来获取一个管理信息实例的下一个可用实例数据。一般用来遍历SNMP中的表格数据。 snmpset 模拟snmp的SetRequest操作的工具。用来设置可以写的管理信息。一般用来配置设备或对设备执行操作。 snmpbulkget 模拟snmp的GetBulkRequest操作的工具。用来获取大块的数据。一般在大量读取大块数据时使用，以提高带宽利用率，并且比使用snmpget、snmpgetnext及snmpwalk有更强的容错能力，代理会返回尽可能多的数据，比其它命令更有保证。 snmpwalk 利用GetNextRequest对给定的管理树进行遍历的工具。一般用来对表格类型管理信息进行遍历。 snmptrap 模拟trap的工具。用来发送模拟trap。一般用来测试管理站安装和配置是否正确，或者用来验证开发的Trap接收程序是否可以正常工作。 snmptrapd 接收并显示trap的工具。一般在代理的开发过程中，接收代理发来的Trap，并将PDU细节打印出来，也来测试Trap发送功能是否正常。 snmpinform 模拟发送InformRequest的工具。跟snmptrap类似，用来发送模拟的带应答的Trap，以测试管理站或自己开发的接收程序。 snmptable 使用GetNextRequest 和 GetBulkRequest操作读取表信息，以列表形式显示的工具。 snmpstatus 从SNMP实体读取几个重要的管理信息以确定设备状态的工具。用来简单测定设备状态。 snmpbulkwalk 利用GetBulkRequest实现对给定管理树进行遍历的工具。对表格类型的管理信息进行遍历读取。 snmpdelta 用来监视Integer类型的管理对象，会及时报告值改变情况的工具。用于监测一个设备或开发中的代理。 snmptest 是一个复杂的工具，可以监测和管理一个网络实体的信息，通过SNMP请求操作与管理实体通信。 snmptranslate 将对象名字和标识符相互转换的工具。用于数据格式的对象标识和可读式字符串的数据名称的转换。类似于域名与IP地址的关系。 snmpusm SNMPv3 USM配置工具。用于SNMPv3的用户管理。 snmpvacm 为一个网络实体创建和维护SNMPv3的基于视图访问控制参数的工具。用于维护SNMPv3的视图访问控制。 snmpconf 生成snmpd配置文件的工具。用于生成snmpd的各种配置文件，用作模板，以生成用户级配置文件。 snmpd Net-snmp开发的主代理程序，包括众多标准MIB的实现。还可以使用子代理对其进行扩展，是一个功能强大的SNMP代理。Linux、Unix和Windows系统运行snmpd后，直接具备了SNMP协议支持，可以被管理站管理。许多商业化的Linux中使用snmpd作为系统的SNMP代理。 snmpdf 通过SNMP访问并显示网络实体磁盘利用情况的工具。用来监测网络实体的磁盘。 4.3、测试与使用net-snmp 测试：首先在snmp.conf中注释掉默认的mibs。测试net-snmp是否安装成功,从代理中读取一个管理对象验证代理是否正常运行。读取MIB库中的RFC1213-&gt;system-&gt;sysDescr 对象，该对象表示设备的描述信息。使用net-snmp提供的snmp工具读取，命令如下： 12root@kali:/mnt/hgfs/VMshare# snmpget -v 2c -c public localhost sysDescr.0SNMPv2-MIB::sysDescr.0 = STRING: Linux kali 4.3.0-kali1-amd64 #1 SMP Debian 4.3.3-7kali2 (2016-01-27) x86_64 得到了系统详细信息，说明net-snmp安装成功。 ps. 如果出现如下错误： sysDescr: Unknown Object Identifier (Sub-id not found: (top) -&gt; sysDescr) 这个是因为你缺少snmp所需的mib库，如下安装使用安装最新的mibs库 # sudo sudo apt-get install snmp-mibs-downloader 然后再测试，你还有可能会出错，那是因为snmp配置的mibs不是想要的，你只需要如下操作# sudo &gt; /etc/snmp/snmp.conf把snmp文件清空，或注释掉mibs那行！ net-SNMP用例 查询到主机CPU空闲率为99% 12root@kali:/etc/snmp# snmpwalk -v 2c -c public localhost 1.3.6.1.4.1.2021.11.11.0UCD-SNMP-MIB::ssCpuIdle.0 = INTEGER: 99 详细命令 (1).snmpget命令 snmpget [OPTIONS] AGENT OID [OID]…选项：-v 1|2c|3 指定SNMP版本-c COMMUNITY 指定community string-m MIB[:…] 指定MIB文件 snmpget 不同于 snmpwalk ，必须在命令行给出 . 的格式，不能只给出 object。案例，snmpget -v 2c -c public localhost system.sysDescr.0 #获取设备的描述信息。 12root@kali:~# snmpget -v 2c -c public localhost system.sysDescr.0SNMPv2-MIB::sysDescr.0 = STRING: Linux kali 4.3.0-kali1-amd64 #1 SMP Debian 4.3.3-7kali2 (2016-01-27) x86_64 (2).snmpwalk/snmpbulkwalk 命令 snmpwalk/snmpbulkwalk [OPTIONS] AGENT [OID]选项：-v 1|2c|3 指定SNMP版本-c COMMUNITY 指定community string-m MIB[:…] 指定MIB文件snmpbulkwalk命令： 获取snmp服务器的mib-2各种资讯，snmpbulkwalk用于通过SNMPv2 的SNMP GET BULK请求命令与其它网络实体通信，只能用于snmpv2,速度快于snmpwalk案例，snmpbulkwalk -v 2c -c public 127.0.0.1 .1.3.6.1.2.1.1 #获取mib-2的system的数据。 123456789101112131415161718192021222324252627282930313233343536373839root@kali:~# snmpbulkwalk -v 2c -c public 127.0.0.1 .1.3.6.1.2.1.1SNMPv2-MIB::sysDescr.0 = STRING: Linux kali 4.3.0-kali1-amd64 #1 SMP Debian 4.3.3-7kali2 (2016-01-27) x86_64SNMPv2-MIB::sysObjectID.0 = OID: NET-SNMP-MIB::netSnmpAgentOIDs.10DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (5833280) 16:12:12.80SNMPv2-MIB::sysContact.0 = STRING: Me &lt;me@example.org&gt;SNMPv2-MIB::sysName.0 = STRING: kaliSNMPv2-MIB::sysLocation.0 = STRING: Sitting on the Dock of the BaySNMPv2-MIB::sysServices.0 = INTEGER: 72SNMPv2-MIB::sysORLastChange.0 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORID.1 = OID: SNMP-MPD-MIB::snmpMPDComplianceSNMPv2-MIB::sysORID.2 = OID: SNMP-USER-BASED-SM-MIB::usmMIBComplianceSNMPv2-MIB::sysORID.3 = OID: SNMP-FRAMEWORK-MIB::snmpFrameworkMIBComplianceSNMPv2-MIB::sysORID.4 = OID: SNMPv2-MIB::snmpMIBSNMPv2-MIB::sysORID.5 = OID: SNMP-VIEW-BASED-ACM-MIB::vacmBasicGroupSNMPv2-MIB::sysORID.6 = OID: TCP-MIB::tcpMIBSNMPv2-MIB::sysORID.7 = OID: IP-MIB::ipSNMPv2-MIB::sysORID.8 = OID: UDP-MIB::udpMIBSNMPv2-MIB::sysORID.9 = OID: SNMP-NOTIFICATION-MIB::snmpNotifyFullComplianceSNMPv2-MIB::sysORID.10 = OID: NOTIFICATION-LOG-MIB::notificationLogMIBSNMPv2-MIB::sysORDescr.1 = STRING: The MIB for Message Processing and Dispatching.SNMPv2-MIB::sysORDescr.2 = STRING: The management information definitions for the SNMP User-based Security Model.SNMPv2-MIB::sysORDescr.3 = STRING: The SNMP Management Architecture MIB.SNMPv2-MIB::sysORDescr.4 = STRING: The MIB module for SNMPv2 entitiesSNMPv2-MIB::sysORDescr.5 = STRING: View-based Access Control Model for SNMP.SNMPv2-MIB::sysORDescr.6 = STRING: The MIB module for managing TCP implementationsSNMPv2-MIB::sysORDescr.7 = STRING: The MIB module for managing IP and ICMP implementationsSNMPv2-MIB::sysORDescr.8 = STRING: The MIB module for managing UDP implementationsSNMPv2-MIB::sysORDescr.9 = STRING: The MIB modules for managing SNMP Notification, plus filtering.SNMPv2-MIB::sysORDescr.10 = STRING: The MIB module for logging SNMP Notifications.SNMPv2-MIB::sysORUpTime.1 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.2 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.3 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.4 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.5 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.6 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.7 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.8 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.9 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.10 = Timeticks: (1) 0:00:00.01 (3).snmpwalk命令获取snmp服务器的HOST-RESOURCES的各种资讯。案例，snmpwalk -v 2c -c public localhost system #这条指令用于查看本机系统信息。 123456789101112131415161718192021222324252627282930313233343536373839root@kali:~# snmpwalk -v 2c -c public localhost system SNMPv2-MIB::sysDescr.0 = STRING: Linux kali 4.3.0-kali1-amd64 #1 SMP Debian 4.3.3-7kali2 (2016-01-27) x86_64SNMPv2-MIB::sysObjectID.0 = OID: NET-SNMP-MIB::netSnmpAgentOIDs.10DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (5844056) 16:14:00.56SNMPv2-MIB::sysContact.0 = STRING: Me &lt;me@example.org&gt;SNMPv2-MIB::sysName.0 = STRING: kaliSNMPv2-MIB::sysLocation.0 = STRING: Sitting on the Dock of the BaySNMPv2-MIB::sysServices.0 = INTEGER: 72SNMPv2-MIB::sysORLastChange.0 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORID.1 = OID: SNMP-MPD-MIB::snmpMPDComplianceSNMPv2-MIB::sysORID.2 = OID: SNMP-USER-BASED-SM-MIB::usmMIBComplianceSNMPv2-MIB::sysORID.3 = OID: SNMP-FRAMEWORK-MIB::snmpFrameworkMIBComplianceSNMPv2-MIB::sysORID.4 = OID: SNMPv2-MIB::snmpMIBSNMPv2-MIB::sysORID.5 = OID: SNMP-VIEW-BASED-ACM-MIB::vacmBasicGroupSNMPv2-MIB::sysORID.6 = OID: TCP-MIB::tcpMIBSNMPv2-MIB::sysORID.7 = OID: IP-MIB::ipSNMPv2-MIB::sysORID.8 = OID: UDP-MIB::udpMIBSNMPv2-MIB::sysORID.9 = OID: SNMP-NOTIFICATION-MIB::snmpNotifyFullComplianceSNMPv2-MIB::sysORID.10 = OID: NOTIFICATION-LOG-MIB::notificationLogMIBSNMPv2-MIB::sysORDescr.1 = STRING: The MIB for Message Processing and Dispatching.SNMPv2-MIB::sysORDescr.2 = STRING: The management information definitions for the SNMP User-based Security Model.SNMPv2-MIB::sysORDescr.3 = STRING: The SNMP Management Architecture MIB.SNMPv2-MIB::sysORDescr.4 = STRING: The MIB module for SNMPv2 entitiesSNMPv2-MIB::sysORDescr.5 = STRING: View-based Access Control Model for SNMP.SNMPv2-MIB::sysORDescr.6 = STRING: The MIB module for managing TCP implementationsSNMPv2-MIB::sysORDescr.7 = STRING: The MIB module for managing IP and ICMP implementationsSNMPv2-MIB::sysORDescr.8 = STRING: The MIB module for managing UDP implementationsSNMPv2-MIB::sysORDescr.9 = STRING: The MIB modules for managing SNMP Notification, plus filtering.SNMPv2-MIB::sysORDescr.10 = STRING: The MIB module for logging SNMP Notifications.SNMPv2-MIB::sysORUpTime.1 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.2 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.3 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.4 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.5 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.6 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.7 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.8 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.9 = Timeticks: (1) 0:00:00.01SNMPv2-MIB::sysORUpTime.10 = Timeticks: (1) 0:00:00.01 (4).snmpdelta命令监控网卡流量等信息， 指定共同体为public，协议为snmpv1，以表格的方式输出结果,显示时间戳。下面是实时监控第一块网卡的流量，#监控IF-MIB::ifInUcastPkts(输入流量)，IF-MIB::ifOutUcastPkts(输出流量)，如监控本地的第二块网卡instance为2： 12345678910root@kali:~# snmpdelta -c public -v 1 -Cs -CT 127.0.0.1 IF-MIB::ifInUcastPkts.2 IF-MIB::ifOutUcastPkts.1127.0.0.1 IF-MIB::ifInUcastPkts.2 IF-MIB::ifOutUcastPkts.1[08:23:16 7/13] 5.00 2.00[08:23:17 7/13] 0.00 0.00[08:23:18 7/13] 0.00 0.00[08:23:19 7/13] 3.00 6.00[08:23:20 7/13] 0.00 0.00[08:23:21 7/13] 0.00 0.00[08:23:22 7/13] 3.00 6.00^C 监控网卡丢包情况： 123456789101112root@kali:~# snmpdelta -c public -v 1 -Cs -CT 127.0.0.1 ifInDiscards.1127.0.0.1 IF-MIB::ifInDiscards.1[08:34:16 7/13] 0.00[08:34:17 7/13] 0.00[08:34:18 7/13] 0.00[08:34:19 7/13] 0.00[08:34:20 7/13] 0.00[08:34:21 7/13] 0.00[08:34:22 7/13] 0.00[08:34:23 7/13] 0.00[08:34:24 7/13] 0.00^C (5).snmpnetstat命令查看snmp服务器的端口连接信息， 协议为snmp 2c。案例， 12345678root@kali:~# snmpnetstat -v 2c -c public -Can -Cp tcp 127.0.0.1 Active Internet (tcp) ConnectionsProto Local Address Remote Address State PIDtcp4 192.168.233.128.22 192.168.233.1.64939 ESTABLISHED 0Listening Internet (tcp) ConnectionsProto Local Address PIDtcp4 *.22 0tcp6 [[XXX - inet6 addr.22 0 (6).snmptranslate 命令 在MIB OID 在数字和文字名称之间进行转换。示例：snmptranslate -On -IR HOST-RESOURCES-MIB::hrSystem #用snmptranslate把symbol 格式的HOST-RESOURCES-MIB::hrSystem 转换为 numberic 格式的oid,用-On(输出数字格式的oid),使用 -IR(允许“随机”访问 MIB) 12root@kali:~# snmptranslate -On -IR HOST-RESOURCES-MIB::hrSystem.1.3.6.1.2.1.25.1 snmptranslate -Onf -IR HOST-RESOURCES-MIB::hrSystem #用snmptranslate把symbol 格式的HOST-RESOURCES-MIB::hrSystem 转换为symbol 格式的 oid,用-Onf(输出符号格式的oid) 12root@kali:~# snmptranslate -Onf -IR HOST-RESOURCES-MIB::hrSystem.iso.org.dod.internet.mgmt.mib-2.host.hrSystem snmptranslate -Tp iso.org.dod.internet.mgmt.mib-2 #用snmptranslate打印从指定OID(.1.3.6.1.2.1)开始一直到该OID(.1.3.6.1.2.1)所属的 MIB 子树末端的树型结构信息 snmptranslate -Ta #用snmptranslate导出所有已经加载的 MIB 的数据，每个对象一行 snmptranslate -Tl #用snmptranslate导出所有已经加载的 MIB 的对象，并且输出完整的 oid 路径 snmptranslate -To #用snmptranslate导出已经加载的所有 MIB 的数字格式的 oid ，不含值和类型，仅仅有 oid 而已 snmptranslate -Ts #用snmptranslate只输出文字格式的 OID 名称而已 4.4、修改SNMP配置文件 example.conf模板将net-snmp源码中的example.conf复制到/etc/snmp/ 目录下，并改名为snmpd.conf，先备份原有的snmpd.conf文件 配置允许网络访问 找到【AGENT BEHAVIOUR】，注释掉agentAddress udp:127.0.0.1：161,添加一行agentAddress udp:161。 选择SNMPv2C协议版本 找到【ACTIVE MONITORING】，注释掉trapsink localhost public，添加trap2sink localhost public 设置访问权限 找到【ACCESS CONTROL】,注释掉rocommunity public default -V systemonly，添加rocommunity public default,允许所有访问请求。 开启防火墙 UDP161端口查看防火墙配置：iptables –L –n,添加一条规则记录：iptables -I INPUT -p udp --dport 161 -j ACCEPT，保存规则使防火墙生效iptables-save(debian linux),ubuntu下的保存iptables命令iptables -save。 在Windows下访问MIBs使用工具snmpB，输入远程主机的ip地址，使用如下： 图：查看Memory大小 图：查看系统版本详情 4.5、常用的SNMP 监控 OID 系统信息![系统信息](http://ww2.sinaimg.cn/large/af9df239jw1f5si8iqsrmj20h60cyaba.jpg) 网络接口![网络接口](http://ww3.sinaimg.cn/large/af9df239jw1f5sid6wbk4j20d70ed75i.jpg) CPU及负载 内存及磁盘 System GroupsysDescr 1.3.6.1.2.1.1.1sysObjectID 1.3.6.1.2.1.1.2sysUpTime 1.3.6.1.2.1.1.3sysContact 1.3.6.1.2.1.1.4sysName 1.3.6.1.2.1.1.5sysLocation 1.3.6.1.2.1.1.6sysServices 1.3.6.1.2.1.1.7 Interfaces GroupifNumber 1.3.6.1.2.1.2.1ifTable 1.3.6.1.2.1.2.2ifEntry 1.3.6.1.2.1.2.2.1ifIndex 1.3.6.1.2.1.2.2.1.1ifDescr 1.3.6.1.2.1.2.2.1.2ifType 1.3.6.1.2.1.2.2.1.3ifMtu 1.3.6.1.2.1.2.2.1.4ifSpeed 1.3.6.1.2.1.2.2.1.5ifPhysAddress 1.3.6.1.2.1.2.2.1.6ifAdminStatus 1.3.6.1.2.1.2.2.1.7ifOperStatus 1.3.6.1.2.1.2.2.1.8ifLastChange 1.3.6.1.2.1.2.2.1.9ifInOctets 1.3.6.1.2.1.2.2.1.10ifInUcastPkts 1.3.6.1.2.1.2.2.1.11ifInNUcastPkts 1.3.6.1.2.1.2.2.1.12ifInDiscards 1.3.6.1.2.1.2.2.1.13ifInErrors 1.3.6.1.2.1.2.2.1.14ifInUnknownProtos 1.3.6.1.2.1.2.2.1.15ifOutOctets 1.3.6.1.2.1.2.2.1.16ifOutUcastPkts 1.3.6.1.2.1.2.2.1.17ifOutNUcastPkts 1.3.6.1.2.1.2.2.1.18ifOutDiscards 1.3.6.1.2.1.2.2.1.19ifOutErrors 1.3.6.1.2.1.2.2.1.20ifOutQLen 1.3.6.1.2.1.2.2.1.21ifSpecific 1.3.6.1.2.1.2.2.1.22 IP GroupipForwarding 1.3.6.1.2.1.4.1ipDefaultTTL 1.3.6.1.2.1.4.2ipInReceives 1.3.6.1.2.1.4.3ipInHdrErrors 1.3.6.1.2.1.4.4ipInAddrErrors 1.3.6.1.2.1.4.5ipForwDatagrams 1.3.6.1.2.1.4.6ipInUnknownProtos 1.3.6.1.2.1.4.7ipInDiscards 1.3.6.1.2.1.4.8ipInDelivers 1.3.6.1.2.1.4.9ipOutRequests 1.3.6.1.2.1.4.10ipOutDiscards 1.3.6.1.2.1.4.11ipOutNoRoutes 1.3.6.1.2.1.4.12ipReasmTimeout 1.3.6.1.2.1.4.13ipReasmReqds 1.3.6.1.2.1.4.14ipReasmOKs 1.3.6.1.2.1.4.15ipReasmFails 1.3.6.1.2.1.4.16ipFragsOKs 1.3.6.1.2.1.4.17ipFragsFails 1.3.6.1.2.1.4.18ipFragCreates 1.3.6.1.2.1.4.19ipAddrTable 1.3.6.1.2.1.4.20ipAddrEntry 1.3.6.1.2.1.4.20.1ipAdEntAddr 1.3.6.1.2.1.4.20.1.1ipAdEntIfIndex 1.3.6.1.2.1.4.20.1.2ipAdEntNetMask 1.3.6.1.2.1.4.20.1.3ipAdEntBcastAddr 1.3.6.1.2.1.4.20.1.4ipAdEntReasmMaxSize 1.3.6.1.2.1.4.20.1.5 ICMP GroupicmpInMsgs 1.3.6.1.2.1.5.1icmpInErrors 1.3.6.1.2.1.5.2icmpInDestUnreachs 1.3.6.1.2.1.5.3icmpInTimeExcds 1.3.6.1.2.1.5.4icmpInParmProbs 1.3.6.1.2.1.5.5icmpInSrcQuenchs 1.3.6.1.2.1.5.6icmpInRedirects 1.3.6.1.2.1.5.7icmpInEchos 1.3.6.1.2.1.5.8icmpInEchoReps 1.3.6.1.2.1.5.9icmpInTimestamps 1.3.6.1.2.1.5.10icmpInTimestampReps 1.3.6.1.2.1.5.11icmpInAddrMasks 1.3.6.1.2.1.5.12icmpInAddrMaskReps 1.3.6.1.2.1.5.13icmpOutMsgs 1.3.6.1.2.1.5.14icmpOutErrors 1.3.6.1.2.1.5.15icmpOutDestUnreachs 1.3.6.1.2.1.5.16icmpOutTimeExcds 1.3.6.1.2.1.5.17icmpOutParmProbs 1.3.6.1.2.1.5.18icmpOutSrcQuenchs 1.3.6.1.2.1.5.19icmpOutRedirects 1.3.6.1.2.1.5.20icmpOutEchos 1.3.6.1.2.1.5.21icmpOutEchoReps 1.3.6.1.2.1.5.22icmpOutTimestamps 1.3.6.1.2.1.5.23icmpOutTimestampReps 1.3.6.1.2.1.5.24icmpOutAddrMasks 1.3.6.1.2.1.5.25icmpOutAddrMaskReps 1.3.6.1.2.1.5.26 TCP GrouptcpRtoAlgorithm 1.3.6.1.2.1.6.1tcpRtoMin 1.3.6.1.2.1.6.2tcpRtoMax 1.3.6.1.2.1.6.3tcpMaxConn 1.3.6.1.2.1.6.4tcpActiveOpens 1.3.6.1.2.1.6.5tcpPassiveOpens 1.3.6.1.2.1.6.6tcpAttemptFails 1.3.6.1.2.1.6.7tcpEstabResets 1.3.6.1.2.1.6.8tcpCurrEstab 1.3.6.1.2.1.6.9tcpInSegs 1.3.6.1.2.1.6.10tcpOutSegs 1.3.6.1.2.1.6.11tcpRetransSegs 1.3.6.1.2.1.6.12tcpConnTable 1.3.6.1.2.1.6.13tcpConnEntry 1.3.6.1.2.1.6.13.1tcpConnState 1.3.6.1.2.1.6.13.1.1tcpConnLocalAddress 1.3.6.1.2.1.6.13.1.2tcpConnLocalPort 1.3.6.1.2.1.6.13.1.3tcpConnRemAddress 1.3.6.1.2.1.6.13.1.4tcpConnRemPort 1.3.6.1.2.1.6.13.1.5tcpInErrs 1.3.6.1.2.1.6.14tcpOutRsts 1.3.6.1.2.1.6.15 UDP GroupudpInDatagrams 1.3.6.1.2.1.7.1udpNoPorts 1.3.6.1.2.1.7.2udpInErrors 1.3.6.1.2.1.7.3udpOutDatagrams 1.3.6.1.2.1.7.4udpTable 1.3.6.1.2.1.7.5udpEntry 1.3.6.1.2.1.7.5.1udpLocalAddress 1.3.6.1.2.1.7.5.1.1udpLocalPort 1.3.6.1.2.1.7.5.1.2 SNMP GroupsnmpInPkts 1.3.6.1.2.1.11.1snmpOutPkts 1.3.6.1.2.1.11.2snmpInBadVersions 1.3.6.1.2.1.11.3snmpInBadCommunityNames 1.3.6.1.2.1.11.4snmpInBadCommunityUses 1.3.6.1.2.1.11.5snmpInASNParseErrs 1.3.6.1.2.1.11.6NOT USED 1.3.6.1.2.1.11.7snmpInTooBigs 1.3.6.1.2.1.11.8snmpInNoSuchNames 1.3.6.1.2.1.11.9snmpInBadValues 1.3.6.1.2.1.11.10snmpInReadOnlys 1.3.6.1.2.1.11.11snmpInGenErrs 1.3.6.1.2.1.11.12snmpInTotalReqVars 1.3.6.1.2.1.11.13snmpInTotalSetVars 1.3.6.1.2.1.11.14snmpInGetRequests 1.3.6.1.2.1.11.15snmpInGetNexts 1.3.6.1.2.1.11.16snmpInSetRequests 1.3.6.1.2.1.11.17snmpInGetResponses 1.3.6.1.2.1.11.18snmpInTraps 1.3.6.1.2.1.11.19snmpOutTooBigs 1.3.6.1.2.1.11.20snmpOutNoSuchNames 1.3.6.1.2.1.11.21snmpOutBadValues 1.3.6.1.2.1.11.22NOT USED 1.3.6.1.2.1.11.23snmpOutGenErrs 1.3.6.1.2.1.11.24snmpOutGetRequests 1.3.6.1.2.1.11.25snmpOutGetNexts 1.3.6.1.2.1.11.26snmpOutSetRequests 1.3.6.1.2.1.11.27snmpOutGetResponses 1.3.6.1.2.1.11.28snmpOutTraps 1.3.6.1.2.1.11.29snmpEnableAuthenTraps 1.3.6.1.2.1.11.30 第5章、开源网络管理系统(NMS) 目前已有上百种开源网络管理软件系统，支持SNMP，及WMI、SMTP、ssh等通用型协议，可以提供管理、性能监控以及报警，分析报告等功能。本章主要分析4款人气高，社区活跃的开源NMS：Zenoss core、Nagios Core、OpenNMS、NetXMS。 5.1、Zenoss Core 特点 管理界面专业、直观 支持200+设备数量管理 Zenpack插件丰富 自动识别设备，并创建配置模型 Web管理界面 强大的报告功能 不足 开源版相比商业版功能较弱 需要极高的硬件资源 官方版仅支持RHEL、cent os，社区也支持Debian和Ubuntu 参考 Zenoss Core官网 官方文档 官方Wiki 带有zenoss的VMware镜像下载 5.2、OpenNMS 特点 基于Java的跨平台 网络自动识别 自动配置、自动识别服务 有15000+的SNMP trap插件 多种通知推送方式 可与Asterisk电话平台交互 支持报告和web管理界面 不足 需要完整的JDK 需要PostgreSQL数据库 参考 OpenNMS官网 5.3、NetXMS 特点 原生跨平台 支持多种SQL数据库 支持管理Android设备 支持报警短信、邮件 支持报告 自动发现网络 不足 配置较为复杂 参考 NetXMS — Download 官方wiki 配置和用户手册5.4、Catci 特点 强大的画图功能 依赖snmp流量监测网络 不足 更适合端口流量监控 参考 Catci官网 5.5、Nagios 特点 配置简单 支持自动发现、通知报告 不足 免费版仅提供开源监控引擎 仅支持linux 不带web引擎，需要专门的web服务器来提供web管理界面 5.6、Zabbix 特点 安装配置简单 支持多语言 开源 自动发现服务器与设备 分布式监视和web集中管理 可以无agent进行监视 email通知 主要功能 CPU负荷 内存使用 磁盘使用 网络状况 端口监视 日志监视 参考 Zabbix官网 详解zabbix安装部署（Server端篇）-鸟哥のlinux-ChinaUnix博客 Open-Falcon 小米科技的类似Zabbix的开源监控项目 open-falcon/open-falcon: open-falcon primary repository，all-in-onehttps://github.com/open-falcon/open-falcon 5.7、Prometheus 开源网络监控系统 go语言开发 高维度数据模型 自定义查询语言 可视化数据展示 高效的存储策略 易于运维 提供各种客户端开发库 警告和报警 数据导出 官网：https://prometheus.io/ 5.8、open-falcon 小米科技的开源监控项目 go语言开发 高效并发分布式 在国内广泛应用 5.9、小结目前国内IT企业广泛应用的主流监控系统，商业付费的有： solarwinds 监控宝 阿里云、腾讯云，Amazon，VMware等云平台附带的监控系统，用于监控云主机，服务器； 开源监控系统主要有： Catci，监控分析端口流量。 Nagios，在Zabbix出现之前，和Catci一样被各类企业广泛应用 Zabbix,开源监控方案的第一选择，分布式，自动化监控预警、快速部署优势明显，超大型大型网络会有性能瓶颈。58同城曾为其贡献过插件Zatree，小米科技早期也用的它。 open-falcon,基于go语言，并发高效，支持分布；由小米科技维护的开源监控解决方案。小米科技，金山，百度，腾讯，阿里，美团等大量国内IT企业使用，可管理机器设备1W+； prometheus，新出的开源项目，因为是基于go语言，在性能表现更出色，主要用于Docker监控与云监控，于2016年7月发布1.0版本。 综上，Zabbix是最佳选择。我会在下一章节进行实验，尝试部署、应用Zabbix，用来监控网络中的主机和设备。 第6章、Zabbix部署和应用6.1、安装Zabbix服务 安装web环境 zabbix通过web提供管理界面，需要LAMP或LNMP环境 设置zabbix数据库 使用mysql，要给Zabbix用户授予向相应的权限 安装zabbix服务端 安装zabbix服务，并导入数据库表 添加配置文件并修改对应的zabbix数据库密码 修改Agentd配置文件 配置web前端 进入zabbix前端进一步配置 访问http://zabbixIP/zabbix，配置数据库连接，生成登录界面 启动服务 service zabbix_server start 设置开机启动 123vi /etc/rc.d/rc.local,添加：/usr/local/zabbix/sbin/zabbix_server start/usr/local/zabbix/sbin/zabbix_agentd start 通过浏览器访问zabbix服务 使用zabbix集成应用系统 zabbix安装配置过程较为复杂繁琐，耗费时间 我使用了官方提供的Zabbix Appliance，集成了zabbix 3.0的 Ubuntu 14.04 64位系统镜像 6.2、安装Zabbix Agent zabbix agent 主要负责收集其监控主机的硬件信息和服务信息，zabbix server 也可以不用agent，使用SNMP，WMI，ICMP ping，端口扫描等收集信息。 6.3、Zabbix配置与监控 添加主机 自动发现 配置监控模板 预警邮件通知 6.4、Zabbix SNMP监控 添加SNMP接口的主机 确定要监控的SNMP OID或字符串 SNMP OID列表 监控需要用到的OID 创建监控项item 监控磁盘获取的SNMP信息 第7章、FreeSwitch的SNMP模块7.1、编译mod_SNMP模块 编译所需依赖包 net-snmp-devel or net-snmp-dev openssl-devel or openssl-dev freeswitch默认不编译mod_snmp 在源码目录modules.conf配置要编译的模块，取消对mod_SNMP的注释 然后编译安装freeswitch时会自动编译mod_snmp 在/usr/local/freeswitch/conf/autoload_configs/modules.conf.xml文件中指定启动自动加载此模块。 单独编译模块 如果已安装fs，则可以用make install-mod_XX进行编译 make install-mod_snmp 在fs中加载mod_snmp 进入fs终端后，load mod_snmp 7.2、配置SNMP agentx mod_snmp是使用net-snmp的agentX开发的snmp子代理程序 mod_snmp需要注册到系统的SNMP主代理上，接收主代理提出的请求并返回结果。 主代理可以通过mod_snmp请求相应的OID，获取FS的状态，而无需在fs终端或fs_cli前端使用freeswitch命令 status 运行fs的系统中需要安装net-snmp和snmp-mibs-downloader，且snmpd作为主代理运行。 修改snmpd.conf配置文件 123456vi /etc/snmp/snmpd.conf# 添加以下内容，为freeswitch用户连接snmpd提供权限# agentXPerms SOCKPERMS [DIRPERMS [USER|UID [GROUP|GID]]]agentXPerms 0755 0755 freeswitch daemon#其他要修改的地方参考前文，主要是snmp远程访问，view访问MIB的范围限制 7.3、freeswitch MIB和OID OID节点.1.3.6.1.4.1.27880是fs注册的，整个fs OID tree参考FreeSWITCH OID walk整个OID tree12345678910111213141516snmpwalk.exe -v 1 -c public 192.168.233.129 .1.3.6.1.4.1.27880SNMPv2-SMI::enterprises.27880.1.1.1.0 = Hex-STRING: C0 58 DA 7C 5A 7F 00 00 00 00 00 00 00 00 00 0000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0000 00 00 00 00 00 00 00 00 00 00SNMPv2-SMI::enterprises.27880.1.1.2.0 = STRING: &quot;efc3110f-e4ec-4cb7-8295-7d60bf8f3c29&quot;SNMPv2-SMI::enterprises.27880.1.2.1.0 = Timeticks: (1595704) 4:25:57.04SNMPv2-SMI::enterprises.27880.1.2.2.0 = Counter32: 0SNMPv2-SMI::enterprises.27880.1.2.3.0 = Gauge32: 0SNMPv2-SMI::enterprises.27880.1.2.4.0 = Gauge32: 1000SNMPv2-SMI::enterprises.27880.1.2.5.0 = Gauge32: 0SNMPv2-SMI::enterprises.27880.1.2.6.0 = Gauge32: 0SNMPv2-SMI::enterprises.27880.1.2.7.0 = Gauge32: 30SNMPv2-SMI::enterprises.27880.1.2.8.0 = Gauge32: 0SNMPv2-SMI::enterprises.27880.1.2.9.0 = Gauge32: 0SNMPv2-SMI::enterprises.27880.1.2.10.0 = Gauge32: 0SNMPv2-SMI::enterprises.27880.1.2.11.0 = Gauge32: 0 .1 to .1000: reserved for core .1: core .1.1: identity .1.3.6.1.4.1.27880.1.1.1: FreeSWITCH version string (eg. “1.0.head (git-0cf1d54 2011-01-19 16-36-04 -0500)”) .1.3.6.1.4.1.27880.1.1.2: Core UUID as a string .2: systemStats .1.3.6.1.4.1.27880.1.2.1: FreeSWITCH uptime as SNMP TimerTicks (hundredths of seconds) .1.3.6.1.4.1.27880.1.2.2: Number of sessions since FreeSWITCH was started .1.3.6.1.4.1.27880.1.2.3: Currently active sessions .1.3.6.1.4.1.27880.1.2.4: Maximum allowed sessions .1.3.6.1.4.1.27880.1.2.5: Currently active calls .1.3.6.1.4.1.27880.1.2.6:Current sessions per second .1.3.6.1.4.1.27880.1.2.7: Maximum allowed sessions per second .1.3.6.1.4.1.27880.1.2.8: Peak sessions per second .1.3.6.1.4.1.27880.1.2.9: Peak sessions per second Last Five Minutes .1.3.6.1.4.1.27880.1.2.10: Peak sessions .1.3.6.1.4.1.27880.1.2.11: Peak sessions Last Five Minutes .1000:mod_snmp .1.3.6.1.4.1.27880.1000: mod_snmp .1001-2000:reserved for modules .1.3.6.1.4.1.27880.1001: mod_sofia .1.3.6.1.4.1.27880.1002: mod_skinny .1.3.6.1.4.1.27880.1002.1: Skinny LDAP schema 附录 参考文章1、 SNMP 原理与实战详解 - 51CTO2、Simple Network Management Protocol - Wikipedia3、SNMP介绍，OID及MIB库–新浪博客4、SNMP4J介绍 - jonbb的博客-CSDN.NET5、四大开源网络管理工具详解 - 51CTO.COM]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>SNMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从二次型最优化问题中理解矩阵特征值的意义]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2015-09-12-Intuition-of-Eigen-Value%2F</url>
    <content type="text"><![CDATA[###惯例开场故事在某次从实验室去往食堂的路上，曾发生这样一段对话： 『大师兄，为什么你对算法的理解总是那么透彻呢？为什么我很难看出它背后的思想？』 『因为你去理解一个算法的时候，不能只是看懂它的形，还要去思考它的神啊~』 这就是我天分不够当不了科学家的佐证吧T。T ###从二次型最优化来理解最小化二次型目标函数，其中A为已知的实对称二阶矩阵，，.这个问题的求解很简单，这里以此为例来说明该问题与矩阵特征值的关系。 首先，可以得到目标函数的网格图与等高线图如下。 对矩阵A进行特征分解可以得到其特征向量为[-0.7071, 0.7071], [0.7071, 0.7071]，对应的特征值分别是0.5, 1.5. 观察函数的等高线图可以知道，等高线最密集的地方，函数值变化最快，而这个函数值变化最快的方向归一化后就是[0.7071, 0.7071]，这恰好是矩阵的一个特征向量。同样地，可以观察，等高线最稀疏的地方，函数值变化最慢，变化方向则是矩阵的另一个特征向量。可以看出，矩阵特征值的大小与函数值的变化快慢有关，较大特征值所对应的特征向量方向上函数值的变化较快，较小特征值所对应的特征向量方向上函数值的变化较慢。 进一步，对于实对称矩阵，我们总是可以对其进行相似变化，得到一个以该矩阵特征值为对角线元素的对角阵。，其中，P为正交矩阵，有性质P的逆等于P的转置。把目标函数改写为，其中. 相似变换的作用可以理解为将等高线图进行旋转，于是得到下面经过旋转后的等高线图。 在这张图上说明矩阵特征值的意义。当函数值取1时所对应的椭圆等高线的长轴长度为， 即由矩阵特征值0.5决定。同理，该椭圆短轴长度为，由矩阵特征值1.5决定。 二阶矩阵的理解较为直观。高阶矩阵的道理是一样的。 ###资料 【1】如何理解矩阵特征值]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Math Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛顿法与拟牛顿法（DFP BFGS LBFGS VLBFGS）]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2015-03-23-Newton-QuasiNewton-Method%2F</url>
    <content type="text"><![CDATA[最近做LBFGS的并行，顺便把牛顿法、拟牛顿法顺理一下。 拟牛顿法是求解非线性优化问题最有效的方法之一。考虑无约束的极小化问题，假设为凸函数，且二阶连续可导。 ###原始牛顿法基本思想：在现有极小点估计值的附近对f(x)进行二阶泰勒展开，进而找到下一个极小点的估计值 牛顿法具有二次收敛性，但当目标函数非二次型时，牛顿法不能保证函数稳定地下降（缺点）。 ###阻尼牛顿法每次迭代前需要沿迭代方向做线搜索，寻求最优的步长因子，即 ###拟牛顿法基本思想：不使用二阶偏导数而构造出可以近似Hession或Hession的逆的正定对称阵，在“拟牛顿”的条件下优化目标函数。 先推导拟牛顿条件：在附近对做泰勒展开，取二阶近似项 推出 取，推出 引入记号 ， 推出 (拟牛顿条件) 它迭代过程中的hession矩阵做约束，因此，对hession对近似的，以及对hession的逆做近似的，可以将 或 作为指导。 ####DFP算法（Davidon–Fletcher–Powell formula） 核心：通过迭代的方法，对hession的逆做近似。迭代格式为 （通常） 猜想待定为（具有对称性） 括号中是数值，将其分别简单赋值为1，-1，即 其中向量u,v仍有待确定，由上面 （要此式成立，不妨直接取） 至此，校正矩阵就已经构造出来了 ####BFGS算法（Broyden–Fletcher–Goldfarb–Shanno algorithm）核心公式的推导过程与DFP完全类似，只是互换了其中s{k}和y{k}的位置。BFGS直接逼近Hession矩阵B_k。(公式敲起来太累了，请自行推导) ####LBFGS算法(limited-memory BFGS)不再存储完整的D_k，而是存储计算过程中的向量序列{s}，{y}。当需要矩阵D_k时，利用向量序列的计算来代替。并且，向量序列也不是全部存储，而是固定存最新的m个。 若要实现并行，需要同时在x与梯度（影响y的计算）那儿求一致平均。 ###资料 【1】DFP算法 【2】BFGS算法 【3】LBFGS算法 【4】Large-scale L-BFGS using MapReduce]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Math Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据机器学习初探---南大李武军]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2015-02-04-Group-Meeting%2F</url>
    <content type="text"><![CDATA[每周的组会大概会持续2小时。如果是主讲，就需要花更多的时间去准备报告内容。之前，组会开完我就不管了，缺乏总结思考。而这样子的话实质上意义就不大了，没有内化为自己的知识，也没有什么critical thinking。从现在开始，记录每一次组会的思考。 常言道：亡羊补牢，为时未晚。T.T ###Outline Learning to Hash Distributed Learning Stochastic Learning 有一个形象的比喻是这样说的，大数据是金矿，云计算是采矿技术，大数据机器学习是冶金技术。 大数据机器学习面临的挑战，一是存储，而是计算速度，三是网络。 哈希学习，在内存、硬盘、cpu、通信上有优势；分布式学习在内存、硬盘、cpu上有优势，但会增加通信成本；随机学习在内存、硬盘、cpu方面有优势。 ###Learning to Hash主讲人：大师兄 最近邻搜索在大数据背景下，会出现维数灾难，存储成本也高，查询速度也慢。解决方法之一是保相似性哈希，可以降低维数并减少存储成本。通常用海明距离（hamming distance）来表征哈希值之间的差异。哈希方案也具有较快的查询速度，通常具有常数或者次线性的搜索时间复杂度；即使是穷举搜索也可以被接受，因为海明距离计算起来是很快的。 哈希函数学习的两个阶段： Projection Stage（dimension reduction） Quantization Stage 贡献： Isotropic Hash 思想：学习一个正交阵（幻灯片21页），其目的是让大于某一阈值的feature的重要程度是一样的。 Supervised Hashing with Latent Factor Models Supervised Multimodal Hashing with SCM Multiple-Bit Quantization ###Distributed Learning主讲人：我 主要内容： 文章：Coupled Group Lasso for Web-Scale CTR Prediction 文章：Distributed Power-Law Graph Computing ####文章1为了解决在线广告的CTR（click through rate）预测，即当某广告展示给某用户时，它被该用户点击的概率，通常的方法是LR（logistic regression），即逻辑回归。但LR的一个短板是，因其是线性的，所以无法将用户与广告之间某些微妙的非线性关系纳入。注意LR中，正则项若为2范数平方，称为标准逻辑回归；正则项若为1范数，问题通常被叫做Lasso。所以需要一种新的方法。 这里的贡献是： CGL的似然定义中，可以纳入用户与广告之间的某些非线性关系的考量。 正则项改为参数的2-1范数，目的是是用户特征向量参数W、广告特征向量参数V中更多的行为0，行为0说明该行对应的feature没作用，即达到删除冗余feature的作用。 分布式实现。这个算法具有较好的扩展性，一个master，若干slave，类似于并行计算，从而实现分布式。 ####文章2GP（graph partitioning）图分割的方法有两种：边分割；点分割。点分割在分布式计算中的通信成本会比图分割小，原因在于在不同的machine上，点分割只需保留点的copy，而边分割需要同时保留点与边的copy。 切割degree大的点，即邻居多的点可以降低通信成本。 ###Stochastic Learning主讲人：浩锋 思想：在需要用到所有节点上的信息时，通信代价往往很大，这时可以随机的选取某一个节点上的信息（比如梯度）作为替代品。 ###资料【1】幻灯片 【2】Coupled Group Lasso forWeb-Scale CTR Prediction in Display Advertising 【3】Distributed Power-law Graph Computing:Theoretical and Empirical Analysis]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Math Optimization MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[凸优化的一些基础算法]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2014-11-10-Basic-Algorithms-of-Convex-Opt%2F</url>
    <content type="text"><![CDATA[本文假设读者对凸优化有基本了解，主要归纳一些基础算法，以便查阅。 其中，f，g，h都是凸函数，g是光滑项，h是非光滑项。 ###Gradient Descent ###Proximal Gradient ###Conjugate Gradient是介于最速下降法和牛顿法之间的一个方法，它仅需要利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hession并求逆的缺点。它是解决大型线性方程组最有用的方法之一，也是解决大型非线性最优化最有效的算法之一。 ###Newton见牛顿法与拟牛顿法（DFP BFGS LBFGS VLBFGS） ###Quasi Newton见牛顿法与拟牛顿法（DFP BFGS LBFGS VLBFGS）]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的报告 Decentralized Privacy-Preserving Low-Rank Matrix Completion]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2014-11-26-Presentation-at-Chinese-Academy-of-Science%2F</url>
    <content type="text"><![CDATA[###我的报告 ####Section 0: Introduce MyselfPage 1 Good afternoon everyone! My name is Anya Lin. I’m a second-year master candidate from University of Science and Technology of China. It’s my great pleasure to introduce to you the Decentralized Privacy-Preserving Low-Rank Matrix Completion. It’s a joint work with my supervisor, Prof. Ling from USTC. Before I start, I want to express my thanks to Prof. Ling for his patient instructions and help over the last a few months. Page 2 Here’s the outline of my presentation. First is the introduction. And then the centralized matrix completion problem. We develop a decentralized algorithm in section 3, and our algorithm is derived from a centralized algorithm as I will talk about in section 2. Next, I will introduce the main result of the topology-dependent privacy preservation. At last, it’s the conclusion. ####Section 1: IntroductionPage 3 OK， let’s go into the introduction. Page 4 I’d like to begin with the concept of matrix completion. So what is matrix completion? As we can see in this picture, we have an incomplete matrix, whose entries are known only for a subset of the whole matrix. And the rank of the matrix is very small compared with the size of the matrix. The goal of the matrix completion is to recover all these unknown entries of of the matrix, as the right-side picture shows. Here, Z is the recovery of W. Page 5 There’s many applications of such a problem. Like image processing, recommendation system and so on. Here are 2 examples. The first one is a problem of image processing. The left picture has a lot of noises, or say, only a part of the original picture is known. By using the fact that the original picture is usually low-rank, we can matrix completion to denoise the picture and get a clear version of high quality as the right picture shows. The second example is more close to our lives. It’s related to a recommendation system. As you can see, it’s a webpage of Douban Movie. A user sees a movie, such as Interstella, and then scores it on the website. Here we can imagine a huge matrix with rows representing users and columns the movies. This matrix is incomplete and it’s low-rank. Once this matrix is completed, the website can recommend new movies to users. Page 6 Now, here comes a privacy concern. First what is privacy? Privacy is the values one considers private. In the example we mentioned just now, the users’ scores of the movies are privacy, because one may not want others to know what movies he has seen or likes. Also, the entries of the matrix could be medical records of patients, or selling data of merchants. These data are considered as privacy. Obviously, no one wants the leakage of his privacy. However, in reality there may exist a malicious agent, a bad guy. For some reasons you have to give your private data to it, but you don’t know wether you can trust it or not. In this situation, we need privacy-preservation. Privacy-preservation is the ability to prevent a malicious agent from obtaining or reconstructing the private data. ####Section 2: Centralized Matrix Completion ProblemPage 7 Now let’s go into the centralized matrix completion problem. Page 8 When we are faced with a low-rank matrix completion problem, the intuitive thought would be to minimize the rank of the matrix, but this is a nonconvex problem. Therefore, we insteadly minimize the nuclear-norm the the matrix, since nuclear-norm is the approximation of the rank and it’s convex. Another approach is if the rank of matrix is known to be r as a prior theoreticallyor empirically, we can get the matrix factorization formulation. This approach is advantageous over the nuclear-norm approach since the latter one needs sigular value decomposition, which is computationally expensive and even intractable in decentralized computing. A centralized algorithm called LMaFit to solve this is as the following steps shows. We have to keep in mind that our algorithm is derived from LMaFit. ####Section 3: Decentralized Matrix Completion ProblemPage 9 After the centralized problem, let’s go into the decentralized one. Page 10 In decentralized computing, we have a network composed of L agents. There is an undirected edge between two agents if they can communicate with each other through one hop. The goal of all the agents in such a network is to collaboratively complete a low-rank matrix in a decentralized fashion. Page 11 To be specific, we segment the whole data matrix W into groups of columns. And do the same to Y and Z. Each agent i in the network holds the corresponding Zi, Yi and entries of Wi. However, X cannot be segmented and distributed to agents because the update of X contains the summation of ZiYi’ over all agents. So we let each agent i holds a local copy X(i) of X. After doing this, we get a naive decentralized implementation of LMaFit. At iteration k, each agent i does the following steps respectively. Notice that the update of X requires information aggregation of all agents. So here is the challenge: informationaggregationofallagentsisimpossible in real decentralized network unless every agent is connected to all the other agents. How to deal with this challenge? Page 12 The answer is dynamic average consensus. Recall that each agent i holds a local copy X(i) of X. If we can make sure that X(i) equals to X for all i, the challenge is solved. To do this, we choose c to be 1/L and the update of X becomes the average consensus problem, as we can see in equation (8), X(i) is the average of all the ZiYi’. At iteration k,we formulate the average consensus problem as equation (9). The constraint means that instead of letting all the X(i) to be identical we choose to let each X(i) equals to it neighboring X(j). A key observation is that exact average consensus at every iteration is not necessary. We use EXTRA to do inexact dynamic average consensus, which saves the computational cost. Page 13 Our decentralized algorithm called D-LMaFit is developed as below. Step 1 is the initialization. Step 2, use EXTRA to do the inexact average consensus problem. Step 3, update Y and X respectively. Page 14 The performance of D-LMaFit is shown in these two pictures. (Explain what these pictures indicate to the audience) ####Section 4: Topology-Dependent Privacy PreservationPage 15 Now let’s go into the section of the topology-dependent privacy preservation. Page 16 First compare decentralized algorithm with centralized one. Centralized algorithm needs a fusion center to collect global data. What if the fusion center is a malicious agent? Oops TT, you’ll lose all your privacy. How about the decentralzied algorithm? One important advantage of decentralized algortihm over a centralized is there’s no global data collection, each agent observes part of the raw data and communicates with its neighboring agent(s). It seems safer. But things aren’t so lucky in reality. Because the communication of X(i) among the network may lead to information leakage. Page 17 How does this happen? Suppose in a network as this picture shows, we have a malicious agent M, and M attempts to recover the local data matrices of some other agents through information exchange. M is interested in recovering the local data Wi, or equivalently Yi or Zi of a set of agents i∈I. When the iteration k is large enough, X(i) will be identical. So if a malicious agent M somehow knows other agents’ Yi, it can recover the data Zi of agent i by doing X(M)Yi. So our concern is, is there any possibility that the malicious agent M can somehow obtain Yi of agent i, and thus get Zi, which is private. Page 18 Before going to details, consider two simple topologies. (Explain the two topologies) Under what conditions can not a malicious agent M reconstruct the sensitive information of P and Q ? Page 19 Recall the update of X. If you could just take a look at the equation, you can find that if the topology is as in the left picture, M can reconstruct ZiYi’ and it may be able to solve Yi so that gets the privacy of P and Q. But if the topology is as shown in the right picture. M cannot get the private data of P and Q. Why? (Explain with the equation) M can solve a series of linear inverse problems and calculate the values of ZiYi’, as what we have said just now. Page 20 Now we get a naive conclusion. Page 21 So the privacy-preserving problem boils down to the linear inverse problem. First we define some variables as this. And further we define A and B. Using these definition,the update of X can be represented by (14). Page 22 Rewrite this as a linear time-invariant systems we get (15). In this system, QI selects those row blocks in Q belonging to the agents in I, and BI selects the corresponding columns in B. QIC and BIC selects the other corresponding row blocks and columns which do not belong to the agents in I. Our analysis uses the concept of z-transfer matrix of (15). The concept is from modern control theory. Obviously, rank(T)=rank(TI TIC), since the latter matrix is just a column rearrangement of the former one. Now we are ready to develop our theorem. Page 23 Check the proof of the suffienciency of our theorem, it’s rather straightforward. If this condition is satisfired, then M has full knowledge of all the X(i). So M can solve a series of linear inverse problems. Page 24 The proof of necessity is a little bit complicated. Here’s the only the simplified version of the proof. First we show that to determine a unique sequence of Q􏰇 from V􏰇 , we must have (18). Suppose (18) doesn’t hold, then there exists at least one column of TI that is linearly dependent on the other columns of T. Then there exists a Q with that column nonzero, and satisfies TQ=0. This corresponds to a nonzero input in I, but the output V is zero for all time. Thus this nonzero input cannot be recovered. This contradicts with the hypothesis. So (18) must hold. (Explain these items) Page 25 (Explain these items) ####Section 5: ConclusionPage 26 I’d like to quickly go over the main point of today’s topic. Page 27 First, we propose a decentralized privacy-preserving algorithm, D-LMaFit, to solve the matrix completion problem. We solve dynamic average consensus subproblem inexactly. We prove the topology-dependent privacy-preserving theorem. It provides a guideline of designing a privacy-preserving network. Page 28 Still we’ve got work to do in the future. (Read items) Page 29 I guess that’s it. Thank you all very much for listening. Now if you have any question, please feel free to ask me. ###故事这学期我在中科院数学与系统科学研究院(AMSS)访问。第一次参与这边的讨论班时，我就被惊到了：学生做报告也全程英文，不愧是袁亚湘老师的学生。于是，11月25日，我也在这儿完成了自己第一次的英文学术报告。 报告前3天，我问盛镇醴师兄他们报告前会不会排练，师兄说：“肯定要啊！上次去葡萄牙开会，马士谦师兄已经讲得那么好了，都还又自己私下练习了5、6遍。师兄真的可以做到每句话精确到几秒钟！”太荔枝了有木有TT。 于是我也练习了。果然只有努力了内心才会踏实。在当天的报告中，我不仅不紧张，还在瞅到台下一堆人的专注神情时，心里突然弹幕全开：“哇，这感觉好爽。” 在记录报告之前，插播一段回忆：大三暑假，我参加中国大学生物联网创新创业大赛，正式比赛前一天系里组织答辩练习，我们组讲得一塌糊涂。那一晚，我和向国菲师兄在实验室通宵改幻灯片，准备发言稿，然后一句一句地练习。中途师兄压力太大又累得不行溜出去躲着抽了根烟，回来被我发现了教育了一顿，嗅觉就是这么灵敏没办法。直到凌晨4点，终于觉得还算满意了，两人躺椅子上睡了会儿，当然我被蚊子咬安逸了。早晨7点，寝室开门，两人各自回去洗澡调整状态。9点，开始比赛。不知道为什么突然想起这个，太，美好了。尽管当时觉得真苦逼。 ###资料【1】有哪些高级的英语表达技巧，让人一听就很地道？]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Math Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是 P, NP, NP-complete, NP-hard]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2014-11-09-What-is-NP-Hard%2F</url>
    <content type="text"><![CDATA[###相关概念NP-hard（non-deterministic polynomial-time hard） P：能在多项式时间内解决 NP：不能在多项式时间内解决或不确定能不能在多项式时间内解决，但一旦你找到一个解，只需要多项式时间去验证这个解是正确的 NP-hard：如果一个问题是NP-hard，意味着可以将任意NP问题化约到这个问题。如果可以解这个问题，那么可以轻松地解任意NP问题。 NPC：NP完全问题，所有NP问题在多项式时间内都能化约（Reducibility）到某一NP问题，这一NP问题就是NPC问题，即解决了此NPC问题，所有NP问题也都解决了。 ###资料原文These refer to how long it takes a program to run. Problems in class P can be solved with algorithms that run in polynomial time. Say you have an algorithm that finds the smallest integer in an array. One way to do this is by iterating over all the integers of the array and keeping track of the smallest number you’ve seen up to that point. Every time you look at an element, you compare it to the current minimum, and if it’s smaller, you update the minimum. How long does this take? Let’s say there are n elements in the array. For every element the algorithm has to perform a constant number of operations. Therefore we can say that the algorithm runs in O(n) time, or that the runtime is a linear function of how many elements are in the array. So this algorithm runs in linear time. You can also have algorithms that run in quadratic time (O(n^2)), exponential time (O(2^n)), or even logarithmic time (O(log n)). Binary search (on a balanced tree) runs in logarithmic time because the height of the binary search tree is a logarithmic function of the number of elements in the tree. If the running time is some polynomial function of the size of the input, for instance if the algorithm runs in linear time or quadratic time or cubic time, then we say the algorithm runs in polynomial time and the problem it solves is in class P. ###NPNow there are a lot of programs that don’t (necessarily) run in polynomial time on a regular computer, but do run in polynomial time on a nondeterministic Turing machine. These programs solve problems in NP, which stands for nondeterministic polynomial time. A nondeterministic Turing machine can do everything a regular computer can and more. This means all problems in P are also in NP. An equivalent way to define NP is by pointing to the problems that can be verified in polynomial time. This means there is not necessarily a polynomial-time way to find a solution, but once you have a solution it only takes polynomial time to verify that it is correct. Some people think P = NP, which means any problem that can be verified in polynomial time can also be solved in polynomial time and vice versa. If they could prove this, it would revolutionize computer science because people would be able to construct faster algorithms for a lot of important problems. ###NP-hardWhat does NP-hard mean? A lot of times you can solve a problem by reducing it to a different problem. I can reduce Problem B to Problem A if, given a solution to Problem A, I can easily construct a solution to Problem B. (In this case, “easily” means “in polynomial time.”) If a problem is NP-hard, this means I can reduce any problem in NP to that problem. This means if I can solve that problem, I can easily solve any problem in NP. If we could solve an NP-hard problem in polynomial time, this would prove P = NP. ###NP-completeA problem is NP-complete if the problem is both NP-hard, and in NP. ###参考资料【1】What are P, NP, NP-complete, and NP-hard?]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里巴巴大数据竞赛回顾与总结]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2014-10-01-Alibaba-Big-Data%2F</url>
    <content type="text"><![CDATA[8个月前，苏宇师兄问我对阿里巴巴大数据竞赛感兴趣吗。正好我选修了陈恩红老师的《机器学习与数据挖掘》，很好奇这门课在实际中的应用；身为淘宝用户，也好奇我是如何被推荐的。于是和师兄一起参加比赛，加上计算机学院的刘惠民同学，我们组成了名叫Rosemary三人团队。这次大赛总共有7276支队伍参赛，我们止步于第二赛季。第一赛季排名56；第二赛季排名68。 开放数据 字段 字段说明 提取说明 user_id 用户标记 抽样、字段加密 Time 行为时间 精度到天级别、隐藏年份 action_type 用户对品牌的行为类型 包括点击（0）、购买（1）、加入购物车（2）、收藏（3）四种行为 brand_id 品牌数字ID 抽样、字段加密 ### 比赛任务根据用户4个月在天猫的行为日志，建立用户的品牌偏好，并预测他们在将来一个月内对品牌下商品的购买行为。### 评估指标大赛最终的比赛成绩排名以F1得分为准。准确率：召回率：F1-Score：其中，- N 为参赛队预测的用户数；M 为实际产生成交的用户数量- pBrandsi为对用户i 预测他(她)会购买的品牌列表个数；bBrandsi为用户i 真实购买的品牌个数- hitBrandsi对用户i预测的品牌列表与用户i真实购买的品牌交集的个数### 解读准确率、召回率和F1-Score准确率就是正确预测数目比上预测总数目。召回率就是正确预测数目比上真实总数目。F1-Score是准确率和召回率的调和平均。理论上，准确率与召回率并没有必然的联系；但在实际中，这二者往往此消彼长、相互制约。有研究表明，在不牺牲准确率的情况下，获得一个高召回率是很难的。在赛题环境下举个栗子： 我们预测出 会有3个人买东西，A买品牌a，B买品牌b，C买P品牌c 真实情况是 有4个人买了东西，这4个人分别是A，B，C，D，其中A买a和b，B买b，C买b和c，D买a。 按照我们的预测，准确率达到了100%，而召回率仅为50%。由此可见，想取得一个较好的F1-Score成绩，需要保证预测的数据尽量hit之外，还要保证预测的数据能够覆盖尽量多的真实数据。具体到比赛中，就需要很好地确定每个被预测的用户可能会买多少个品牌。 通过对这三个评判标准的分析，我们初步确定了第一赛季所用的策略。 第一赛季大致的方案是： 加权对用户4种行为次数求和，得到简单的用户ID（行）与品牌ID（列）的兴趣度（值）矩阵。 统计前4个月平均每个月有多少用户购买超过2个品牌的东西，记为B。 根据用户前4个月的购买行为，按照购买力划分用户购买等级。直白解释就是你以前在天猫上平均每个月买了多少次，我就假设你下个月还会买这么多次。 第一次切分兴趣度矩阵，保留前B行矩阵。 对每个用户对不同品牌的兴趣度由高到低排序，再根据每个用户的购买力确定对该用户保留前n列的兴趣数据，并预测他会买这n个数据对应的品牌。 当然，在最初的模型建立起来之后，我们还陆续加了一些规则，比如在时间节截止点(最后N天，N可以根据自己的理解取值)的单独分析。另外，调参数真就是一项体力活。 第二赛季（好困，需要去睡觉Zzz···）细节未完待续 总结]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matlab科研小贴士]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2014-09-11-Matlab-tips%2F</url>
    <content type="text"><![CDATA[使用MATLAB运行算法程序时，可能遇到各种各样的报错。比如，为了保护隐私数据，我在分布式矩阵补全算法中加入随机矩阵之后，某项变量在运行几百步之后会出现NaN报错。我只根据算法顺序去分析问题出现的可能原因，并修改程序。感觉并没有很好地利用MATLAB的强大功能去锁定症结所在。幸运的是，施伟大师兄当时和我在一起，他非常热心地帮我分析问题，教我以后遇到类似状况应该怎么去分析与思考。和大师兄讨论了半小时，感觉自己收获不少。 这篇文章会陆续记录下自己使用MATLAB的体会，以及解决问题的一些技巧。 Clear运行一段代码前通常需要将工作空间里的已有数据清除掉。只需要在编辑有实际意义的代码之前写下如下代码： 1clc; clear; close all; Random Seed为了保证程序在相同环境下运行以便测试某一个或几个改变对于算法的影响，在使用各种random命令时，需要设定固定种子。这样就不会因为每次随机产生的序列不同而影响程序运行结果。设置随机种子的代码如下： 12345678910%% random seed%seed=round(5000*rand); % use this line if you set a random seedseed=3302; % use this line if you set a fixed seed. 3302 can be replaced by other numbers.fprintf('Seed = %d\n',seed); % print the current seed if exist('RandStream','file') RandStream.setGlobalStream(RandStream('mt19937ar','seed',seed));else rand('state',seed); randn('state',seed^2);end NaNNaN是Not a Number的缩写。当某变量显示NaN时，表示该变量是不明确的数值结果。比如0/0、inf/inf等运算会出现NaN报错。遇到这种情况，首先判断NaN出现在哪一步： 123if isnan(norres) %括号里是变量名。判断norres是否为NaN，若是，则在该步暂停程序。 keyboard;end 再在命令窗里单独查看与该变量有关的其他变量，从而排除正常变量，获知究竟是哪个或哪几个变量出了问题，变为无穷大或无穷小。再检查与这些变量有关的算法。 SaveAs若需要比较各参数对算法性能的影响，通常是在程序中修改参数运行，得到算法收敛精度与迭代次数的曲线图。再根据曲线图反向思考修改哪些参数有效。这个过程需要保存产生的大量图片。可以使用hold on命令将所有虚线画在同一张图上，也可以使用saveas将所有图片自动保存。 123456789%% plotfigure(1)semilogy(1:iter,y_axis(1:iter),'b-'); %b：蓝色。－：线段形状set(gca,'fontsize',12);grid on;xlabel('\fontsize&#123;12&#125;\it Iteration'); ylabel('\fontsize&#123;12&#125;\it Normalized residual');legend('\fontsize&#123;12&#125;\it text'); %text：这条蓝色代表什么hold onsaveas(gcf,'filename','fig') %filename：将图片保存为这个名字。fig：保存为fig格式 保存变量数据的命令： 12save(&apos;filename&apos;)save(&apos;filename&apos;,&apos;variables&apos;) 注意，在使用hold on命令时，应该保留上次程序运行后产生的各种数据。即不能在程序中写类似与clear all之类的清除语句，否则上次曲线图也将被删除。 矩阵规范化已知满秩矩阵A，进行下面操作使其所有奇异值均为1。 12[u s v]=svd(A);A=u*v'; 安装CVX 将cvx压缩包解压 将cvx文件夹拷贝至如D:\MATLAB Programs\Compressed Sensing目录下 在Current Folder窗口中打开cvx文件夹 在Command Window中输入cvx_setup 在MATLAB的File菜单下的set path把此路径加上。 把路径加进去后在file→Preferences→General的Toolbox Path Caching里点击update Toolbox Path Cache更新一下 完成 %%分段运行程序 选中%%分段 右键选择evaluate current section]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态一致平均问题算法-EXTRA和DAC]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2014-08-31-Papers-about-average-consensus%2F</url>
    <content type="text"><![CDATA[EXTRA DAC 优缺点比较其中，DAC最大的缺点在于第一次迭代时对于r(-1)时刻的依赖，在实际仿真中，如果需要对动态输入求一致平均，往往并不能获取输入在-1时刻的值。导致在矩阵补全问题中，DAC做不精确的动态一致平均的子问题效果并不好。 而EXTRA却有很好的效果。]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[低秩矩阵补全]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2014-07-25-matrix-completion%2F</url>
    <content type="text"><![CDATA[###问题描述: 如果有这样一个矩阵 矩阵中有部分元素缺失矩阵的秩相较于矩阵维数来说很小，并作为先验已知 我们希望恢复那些缺失的元素，这个问题就是低秩矩阵补全问题。 ###思考过程: 需要恢复一个低秩矩阵直接想法是极小化矩阵的秩但是的优化问题非凸，不好求解核范数是秩的凸近似，所以想到 ###极小化核范数的集中式算法: 求解的集中式算法有很多，比如： singular value thresholding algorithm fixed-point shrinkage algorithm proximal gradient algorithm ADMM 但如果矩阵规模和秩增大，以上算法的计算代价也增大，因为它们都需要求解奇异值分解(SVD)。SVD中求伪逆的步骤运算量大，很耗费资源。因此需要想更好的方法，避免极小化核范数。 ###极小化分解矩阵之积的集中式方法: 将问题写为，其中是对的估计，在元素没有缺失的位置上和的元素相同，,是对的乘法分解。介绍两种求解该问题的方法： nonlinear Gauss-Seidel method nonlinear SOR(Successive Over-Relaxation)-like scheme:LMaFit 其中SOR方法是GS方法的拓展，区别仅在于SOR方法中对于X的更新加了权重，并对权值进行更新。 ###去中心式算法: 当矩阵规模大到一定程度时，集中式算法在计算能力上要求过高，普通计算机也许无法计算。这时，我们需要在由许多普通计算机作为节点组成的网络中运算，这需要实现去中心式计算。去中心式计算式很容易实现的，将,,分别切块放在每个节点上，将作为公共信息在网络各个邻居节点间交换，优化问题形式不变，但需要加上的约束。而这样一个约束就引出了另一个子问题：一致平均(average consensus)问题。 关于一致平均问题的介绍请看： 《对于一致平均问题的理解》 《动态一致平均问题的4篇论文》]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各类范数]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%A7%91%E7%A0%94%2F2014-07-27-norms-of-vector-and-matrix%2F</url>
    <content type="text"><![CDATA[###向量范数 ###矩阵范数 ###矩阵乘积的迹 ###特殊范数 矩阵W的L2-1范数： ###TV范数||L(x)||_1。 其中L是差分算子，x是某种数字信号，在一维情况下，如下所示： ||L(x)||_1 = |x2-x1| + |x3-x2| + |x4-x3| + …… 加TV范数的目的是为了使求得的去噪信号仍然具有分段连续的性质。因为差值的1范数说明差值稀疏，从而说明求得的信号分段连续。]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pip requirements]]></title>
    <url>%2F2017%2F05%2F03%2F%E6%8A%80%E6%9C%AF%2FPython-pip-requirement%2F</url>
    <content type="text"><![CDATA[requirements.txt的作用在很多Python项目中都包含一个requirements.txt文件，里面写的是一些包的名称和版本信息。 描述运行这个项目所需要的环境，包括一些库。 可以使用pip install -r requirements.txt安装这些库。 查找python项目依赖并生成requirements.txt 将整个python环境的依赖包list出来， pip freeze &gt; requirements.txt list某个项目用到的依赖包,使用工具pipreqs ，但有的时候结果会有偏差(源码分析的不准确) pip install pipreqs pipreqs ./]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海豚笔试]]></title>
    <url>%2F2017%2F04%2F13%2F%E6%8A%80%E6%9C%AF%2Fdolphin-exam%2F</url>
    <content type="text"><![CDATA[今天参加了一个实习生招聘的笔试，准备的不充分，错的惨不忍睹。整理一下遇到的题目。 1、Java部分试题有java和c++两种，以前上课学的C和Java早还给老师了，python的BIF用多了，发现连个排序算法都写不好了。 考察java的类继承和静态方法 通过类名来调用子类中的静态变量和静态方法，当父类与子类相同时是，子类会隐藏父类中与其相同的静态变量和静态方法，如果子类中没有与其父类相同的静态变量和静态方法，子类从其父类调用过来的静态变量和静态方法就会表现出来。 通过子类创建对象来用对象名调用子类中的静态变量和静态方法，除非是父类没有的静态变量和静态方法，会显示其子类的静态变量和静态方法。否则，最后显示一定是从父类哪里引用来的静态变量和静态方法。 考察Java String变量的比较及值比较 1234567891011121314151617181920//"=="操作符:用于基本数据类型的比较,判断引用是否指向同一内存块//如果String缓冲池内不存在与其指定值相同的String对象，那么此时虚拟机将为此创建新的String对象，并存放在String缓冲池内。import java.io.*;class test &#123; public static void main (String[] args) throws java.lang.Exception &#123; String s1 = "helloworld"; String s2 = "hello" + "world"; String s0 = "helloworld"; String s3 = new String("helloworld"); System.out.println(s1==s0); // true System.out.println(s2==s0); // true System.out.println(s1==s2); // true System.out.println(s1.equals(s0)); // true System.out.println(s1.equals(s3)); // true System.out.println(s1==s3); //false &#125;&#125;//如果String缓冲池内存在与其指定值相同的String对象，那么此时虚拟机将不为此创建新的String对象，而直接返回已存在的String对象的引用。 ​ 简单的算法，一个32位整数的数组，返回有序排列的2个相邻元素之差的最大值。整数是32位的，时间复杂度o(n)有加分。LeetCode 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115// 写了个最蠢的方法，冒泡排序，然后遍历数组，求最大的差值。时间复杂度在排序，o(n^2);// 数据结构书里只记得o(n*logn)的排序算法，快速排序，插入排序，堆排序，选择排序o(n^2);// 在整数取值范围有限的情况下，计数排序、基数排序和桶排序可以实现空间复杂度O(k),时间复杂度O(n)的排序/* 思路：使用桶排序的原理，但桶的取值设为n-1，而不是整数的取值范围。桶内数据无序，桶间，用后一个桶的min 减去 前一个桶的max即可。*/public class MaxGap&#123; public int maximumGap(int[] num) &#123; int maxGap = 0; // edge case if (num.length &lt; 2) &#123; return maxGap; &#125; // get maximum and minimum int min = num[0]; int max = num[0]; for (int i = 0; i &lt; num.length; i++) &#123; if (num[i] &lt; min) min = num[i]; if (num[i] &gt; max) max = num[i]; &#125; // divide into identical gaps Gap[] gaps = new Gap[num.length - 1]; boolean[] Engaged = new boolean[num.length - 1]; double gap = (double) (max - min) / (double) (num.length - 1); for (int i = 0; i &lt; gaps.length; i++) Engaged[Math.min((int) Math.floor((double) (num[i] - min) / gap), gaps.length - 1)] = true; // assign maximum and minimum for each gap for (int i = 0; i &lt; gaps.length; i++) gaps[i] = new Gap(); for (int i = 0; i &lt; num.length; i++) &#123; int index = (int) Math.floor((double) (num[i] - min) / gap); index = Math.min(index, gaps.length - 1); // lower bound if (gaps[index].low == -1) gaps[index].low = num[i]; else gaps[index].low = Math.min(gaps[index].low, num[i]); // upper bound if (gaps[index].high == -1) gaps[index].high = num[i]; else gaps[index].high = Math.max(gaps[index].high, num[i]); &#125; // find maximum gap for (int i = 0; i &lt; gaps.length; i++) &#123; if (Engaged[i]) &#123; // check its inner gap maxGap = Math.max(gaps[i].high - gaps[i].low, maxGap); // lower all the way int j = i; while (--j &gt;= 0) &#123; if (Engaged[j]) break; &#125; if (j &gt;= 0) maxGap = Math.max(gaps[i].low - gaps[j].high, maxGap); // upper all the way j = i; while (++j &lt; num.length - 2) &#123; if (Engaged[j]) break; &#125; if (j &lt; gaps.length) maxGap = Math.max(gaps[j].low - gaps[i].high, maxGap); &#125; &#125; return maxGap; &#125; class Gap &#123; int low; int high; boolean hasItem; Gap() &#123; low = -1; high = -1; &#125; Gap(int x, int y) &#123; low = x; high = y; &#125; &#125; public static void main(String[] args) &#123; int[] num = &#123;1, 2, 3, 5, 7, 9&#125;; System.out.println((new MaxGap()).maximumGap(num)); &#125;&#125; ​ 2、Python部分 下列不能创建字典的语句是 dict1 = {[1,2,3]: &#39;use&#39;} 12345678910a = &#123;[1,2 ,3]: "user"&#125;Traceback (most recent call last): Python Shell, prompt 6, line 1TypeError: unhashable type: 'list'a = &#123;4:5&#125; a = &#123;&#125;a = &#123;(1,2,3): 'use'&#125;print a&#123;(1, 2, 3): 'use'&#125; Numpy数组的切片操作 3、机器学习 研究发现，买尿布的顾客中80%的也会同时购买啤酒，这属于数据挖掘的哪种问题？关联规则。 为了防止过拟合可以采取的方法，正则化，early stopping，数据集扩增，Dropout(神经网络)。 分类和回归的区别，应用场景，常见的算法 分类和回归的区别在于输出变量的类型。 定量输出称为回归，或者说是连续变量预测；预测明天的气温是多少度，这是一个回归任务；定性输出称为分类，或者说是离散变量预测。预测明天是阴、晴还是雨，就是一个分类任务。 常见算法？好像很多算法思想都既能用于回归，也能用于分类。分类和回归应该有内在联系， 标记一下，以后深入学习后补充 逻辑回归中的常用激励函数 记得神经网络里有sigmod函数做激励函数，逻辑回归？ logistic回归此处留疑，后面再补充 描述你熟悉的神经网络和它们的特征 重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation）， Hopfield网络，自组织映射（Self-Organizing Map, SOM）。学习矢量量化（Learning Vector Quantization， LVQ） 都不熟悉，先去看书了:cry: ​]]></content>
      <categories>
        <category>技术</category>
        <category>其他</category>
      </categories>
      <tags>
        <tag>Exam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017年Q1阅读书单]]></title>
    <url>%2F2017%2F03%2F11%2F%E9%98%85%E8%AF%BB%2FBook-List-2017-Q1%2F</url>
    <content type="text"></content>
      <categories>
        <category>生活</category>
        <category>读书</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[scrapy-redis 分布式爬虫]]></title>
    <url>%2F2017%2F03%2F02%2F%E6%8A%80%E6%9C%AF%2Fscrapy-redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[scrapy-redis 分布式爬虫简介安装和配置 安装redis数据库 pip install scrapy redis-py scrapy-redis 作用和特点 scrapy-redis是为Scrapy提供redis支持以实现分布式爬虫的组件 多个爬虫共享一个redis队列（分配request） 分布式的post处理。将爬到的items也放入redis队列，因而可以实现items的分布式处理。 scrapy-redis仅仅为scrapy提供了部分基于redis的组件，可以查看源码。 pipeline scheluder redis队列替换原有的scrapy队列 过滤器 Duplication 初步使用settings参数123456789101112131415161718192021222324252627282930313233343536# Enables scheduling storing requests queue in redis.SCHEDULER = "scrapy_redis.scheduler.Scheduler"# Ensure all spiders share same duplicates filter through redis.DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"# Don't cleanup redis queues, allows to pause/resume crawls.SCHEDULER_PERSIST = True# Schedule requests using a priority queue. (default)SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'# Alternative queues.#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'# Max idle time to prevent the spider from being closed when distributed crawling.# This only works if queue class is SpiderQueue or SpiderStack,# and may also block the same time when your spider start at the first time (because the queue is empty).#SCHEDULER_IDLE_BEFORE_CLOSE = 10# Store scraped item in redis for post-processing.ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 300&#125;# Specify the host and port to use when connecting to Redis (optional).#REDIS_HOST = 'localhost'#REDIS_PORT = 6379# Specify the full Redis URL for connecting (optional).# If set, this takes precedence over the REDIS_HOST and REDIS_PORT settings.REDIS_URL = 'redis://user:pass@hostname:9001'# Use other encoding than utf-8 for redis.默认utf-8REDIS_ENCODING = 'latin1' scrapy-redis使用方法 首先用scrapy实现一个爬虫，然后在替换其中的组件为scrapy-redis setting里修改： 123456789101112131415# setting.pyBOT_NAME = 'moko1'SPIDER_MODULES = ['moko1.spiders']NEWSPIDER_MODULE = 'moko1.spiders'# 使用scrapy-redis的去重和调度器组件DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"SCHEDULER = "scrapy_redis.scheduler.Scheduler"SCHEDULER_PERSIST = TrueITEM_PIPELINES = &#123;# 会将items放入redis队列中'scrapy_redis.pipelines.RedisPipeline': 400,&#125; spiders里修改： spider类从scrapy_redis.spiders导入，有RedisSpider和RedisCrawlSpider，对应scrapy原来的Spider和CrawlSpider。 start_urls改为从redis中某个key获取，因此redis_key = ‘moko_spider:start_urls’，然后向该key push数据。 直接给出start_urls也是可行的，但是不太符合redis队列及分布式的逻辑，而且不能手动动态添加。 123456789101112131415161718192021# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import Rulefrom ..items import Moko1Itemfrom scrapy_redis.spiders import RedisCrawlSpiderclass MokoSpiderSpider(RedisCrawlSpider): # 修改此处 name = 'moko_spider' allowed_domains = ['moko.cc'] start_urls = ['http://www.moko.cc/moko/post/1.html'] # 修改此处 # redis_key = 'moko_spider:start_urls' rules = ( Rule(LinkExtractor(allow=r'http://www\.moko\.cc/post/\d+\.html'), callback='parse_item', follow=True), ) def parse_item(self, response): item = Moko1Item() item['name'] = response.css("#workNickName::text").extract()[0] return item 确保redis数据库运行，清空数据库flushdb, “moko_spider:dupefilter”保存了我上次执行时已经爬取过的url信息。再次执行会被过滤掉，scrapy的去重机制。 然后启动scrapy爬虫，然后向redis_key = ‘moko_spider:start_urls’中push数据，在redis-cli客户端中执行lpush moko_spider:start_urls https://moko.cc/1.html 如果启用了scrapy_redis.pipelines.RedisPipeline，items会存储在moko_spider:items中。可以将items不断的pop出来，并进行其他处理，如存储等。(感觉这种活应该交给一个pipeline干) 分布式爬取直接运行多个爬虫 上面的单个爬虫默认从localhost的Redis数据库中存取request和爬到的items， 而实现分布式爬虫只需要为爬虫指定Redis数据库的网络位置，所有的爬虫都去redis队列里存取。 12345# settings.pyREDIS_URL = 'redis://user:mima@localhost:6379' # 或者不带密码的REDIS_HOST = 'localhost'REDIS_PORT = 6379 配置redis允许远程访问 修改配置文件/etc/redis.conf 12# bind 127.0.0.1requirepass mima # 设置密码 ​ 关于主从模式 分布式架构一般分为主从模式和P2P模式。有的人认为scrapy-redis中安装有redis数据库的节点就是master，错的！ scrapy-redis中的每只爬虫都是平级的，没有主从之分。每只爬虫都是主动请求任务，执行任务，爬到的数据也可以提交给redis。redis的request队列为空时，爬虫处于饥饿状态。 scrapy-redis仅仅是把scrapy原来得本地队列放入redis数据库中，从通信和数据传输的角度来看，redis像是一个master，而实际上redis对爬虫没做任何控制和操作，只是被动的为它们提供数据。 利用docker部署爬虫 创建一个docker镜像并配置scrapy环境，这样下次就能恢复环境直接使用了 首先在daocloud申请一台胶囊主机，然后ssh登录上去，配置scrapy环境 123456789101112# 使用ubuntu的docker镜像docker run -it daocloud.io/ubuntu:14.04 /bin/bashapt-get update apt-get install lrzszapt-get install python2.7 python-pip python-devapt-get install libxml2-dev libxslt-dev python-lxml # 安装lxmlapt-get install build-essential libssl-dev libffi-devpip install six --upgradepython -m pip install pyparsing appdirspip install cryptographypip install pymongo redis twisted scrapy scrapy-redisexit # 退出docker，记住id，docker id root@ea0d832b19bb 打包上传镜像 123456789101112131415161718# 打包镜像，docker容器的id ea0d832b19bb~$ docker commit -m "ubuntu with scrapy_redis" -a "author——info" ea0d832b19bb scrapy-redissha256:53a605ccc92ab29bba70f9f026c002a9c4afa43fb9b14a9819d5859f51b0d586~$ docker images # 查看镜像# 为镜像打上tagdocker tag scrapy-redis syy2358/scrapy-redis:latest# 上传至dockerhub托管docker login # 先注册并登录dockerhub，创建一个托管仓库scrapy-redisdocker push syy2358/scrapy-redis:lastest # 将镜像上传至dockerhub# 胶囊主机只有2小时，hub上传速度又慢，只好打包镜像文件下载到我的电脑上。docker save ea0d832b19bb &gt; /home/ubuntu/scrapy-redis.tar# 可以在有docker的电脑上恢复该容器，docker load &lt; scrapy-redis.tar# 或者直接拉dockerhub/daocloud上的镜像用就行了docker pull syy2358/scrapy-redisdocker run -it xx.xx# 导出 export 和save的区别- 导入 import# save保存了容器的运行状态，支持回滚，但是数据较大。 ​自己配置环境各种报错，主要是下载链接超时，用daocloud就很顺利 首先自己编译docker镜像容易遇到各种错误，而且dockerhub的镜像push、pull的速度巨慢，估计是被墙了，所以决定改用daocloud在线编译发布镜像，编译和pull的速度都很快。 镜像制作过程： 在自己 GitHub 创建新的 repository 。 将爬虫的代码，包含Dockerfile push 到自己刚创建的 repository。 到 https://dashboard.daocloud.io/build-flows/new ，项目名称 scrapy，选择自己刚在 GitHub 创建的 repository同步代码，开始创建，选择分支：master，手动执行。如果失败，可以先看下流程定义里的构建阶段，修改任务，选择云端Dockerfile。 到 https://dashboard.daocloud.io/packages 选择 scrapy，设置 -&gt; 镜像访问控制 -&gt; 公开,设置tag为latest。 https://dashboard.daocloud.io/packages/选择scrapy后，版本 -&gt; latest 。然后可以部署到已经接入的docker或者云测试环境(右上角打开控制台，能进入web版的终端，在里面执行scrapy crawl spider即可)。 或者在自己的docker环境下，使用docker run -it daocloud.io/blue_whale/scrapy,然后就能看到爬虫在运行了 镜像地址 daocloud.io/blue_whale/scrapy : daocloud上的，速度很快 syy2358/scrapy-redis: dockerhub上的，巨慢 ​ 运行爬虫 上传源码，并从Dockerfile build镜像，然后运行爬虫 1234567891011121314151617181920212223242526272829303132333435# 项目结构.├── docker-compose.yml├── Dockerfile├── moko1│ ├── __init__.py│ ├── items.py│ ├── pipelines.py│ ├── settings.py│ └── spiders│ ├── __init__.py│ ├── moko_spider.py├── requirements.txt└── scrapy.cfg---------------------------# docker-compose.ymlversion: '2'services: spider: build: . volumes: - .:/code------------------------# DockerfileFROM syy2358/scrapy-redisENV PATH /usr/local/bin:$PATHADD . /codeWORKDIR /codeRUN pip install -r requirements.txt# COPY spiders.py /usr/local/lib/python3.5/site-packages/scrapy_redisCMD scrapy crawl moko_spider 我的redis服务器是在腾讯云上的，没有使用docker。 如果redis在docker中运行的话，需要在docker-compose.yml中定义redis的container，将spider和redis link起来，同时redis需要映射端口6379，这样不同的container之间才能相互通信。 使用docker-compose创建container 12345pip install docker-compose rz -E # 上传爬虫源码 docker-compose up #从 docker-compose.yml 中创建 `container` docker-compose scale spider=4 #将 spider 这一个服务扩展到4个container # 会有4个scrapy爬虫运行，处于饥饿状态，因为刚开始start_urls为空，直到我们pushurl去feed爬虫，爬虫才会开始抓取工作。 ​ 方法二，使用已经build好的docker镜像(爬虫代码也已经copy进去了) 123456docker run -it daocloud.io/blue_whale/scrapy### 或者docker run -it syy2358/scrapy-redis### Ctrl+P+Q 将当前container放入后台，回到docker界面docker ps -a ## 查看正在运行的containerdocker attach id # 连接入正在执行的container 退出attach的docker container 用1执行爬虫时真惨，attach上container退不出去，scrapy不停地输出log内容，按啥键都不好使，只好退出ssh重连T.T，重连后发现原来的container仍在运行中。 正常attach上一个container，可以Ctrl+P+Q退出再后台执行，或exit终止运行并退出container。 使用Docker attach命名进入docker容器后： 【场景一】如果要正常退出不关闭容器，请按Ctrl+P+Q进行退出容器。 【场景二】如果使用exit退出，那么在退出容器后会关闭容器，如下图所示。 总结 只需要配置一次scrapy的docker运行环境，上传代码，然后将container打包成镜像，就可以在任何有docker的地方pull下镜像运行。 docker挺有意思的，项目部署非常方便，不过我这个新手对docker只是一知半解。 导出redis中的items linux下使用redis-dump redis-dump -u 127.0.0.1:6379 &gt; db_full.json 将数据导入mongodb中 123456789101112131415161718192021222324#!/usr/bin/env python# -*- coding: utf-8 -*-import jsonimport redisimport pymongodef main(): r = redis.Redis(host='192.168.1.112',port=6379,db=0) client = pymongo.MongoClient(host='localhost', port=27017) db = client['dmoz'] sheet = db['sheet'] while True: # 将队列里的数据逐条pop出来，并插入mongodb中 # process queue as FIFO, change `blpop` to `brpop` to process as LIFO source, data = r.blpop(["dmoz:items"]) item = json.loads(data) sheet.insert(item) try: print u"Processing: %(name)s &lt;%(link)s&gt;" % item except KeyError: print u"Error procesing: %r" % itemif __name__ == '__main__': main() ​]]></content>
      <categories>
        <category>技术</category>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Scrapy</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy_redis源码阅读]]></title>
    <url>%2F2017%2F03%2F01%2F%E6%8A%80%E6%9C%AF%2Fscapy-redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[scapy-redis 源码阅读connection.py 创建redis连接实例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import sixfrom scrapy.utils.misc import load_object # 加载对象from . import defaults# Shortcut maps 'setting name' -&gt; 'parmater name'.SETTINGS_PARAMS_MAP = &#123; 'REDIS_URL': 'url', 'REDIS_HOST': 'host', 'REDIS_PORT': 'port', 'REDIS_ENCODING': 'encoding',&#125;def get_redis_from_settings(settings): """Returns a redis client instance from given Scrapy settings object. This function uses ``get_client`` to instantiate the client and uses ``defaults.REDIS_PARAMS`` global as defaults values for the parameters. You can override them using the ``REDIS_PARAMS`` setting. Parameters ---------- settings : Settings A scrapy settings object. See the supported settings below. Returns ------- server Redis client instance. Other Parameters ---------------- REDIS_URL : str, optional Server connection URL. REDIS_HOST : str, optional Server host. REDIS_PORT : str, optional Server port. REDIS_ENCODING : str, optional Data encoding. REDIS_PARAMS : dict, optional Additional client parameters. """ # 获取Redis的设置参数 params = defaults.REDIS_PARAMS.copy() params.update(settings.getdict('REDIS_PARAMS')) for source, dest in SETTINGS_PARAMS_MAP.items(): val = settings.get(source) if val: params[dest] = val # ``redis_cls``是redis类的路径 if isinstance(params.get('redis_cls'), six.string_types): params['redis_cls'] = load_object(params['redis_cls']) # 通过'redis_cls'得到redis对象，如果if为False，get_redis函数中会加载默认的redis类StrictRedis return get_redis(**params)# Backwards compatible alias.from_settings = get_redis_from_settingsdef get_redis(**kwargs): """Returns a redis client instance. Parameters ---------- redis_cls : class, optional Defaults to ``redis.StrictRedis``. url : str, optional If given, ``redis_cls.from_url`` is used to instantiate the class. **kwargs Extra parameters to be passed to the ``redis_cls`` class. Returns ------- server Redis client instance. """ redis_cls = kwargs.pop('redis_cls', defaults.REDIS_CLS) # defaults.REDIS_CLS 默认值redis.StrictRedis url = kwargs.pop('url', None) if url: return redis_cls.from_url(url, **kwargs) # url不为None，则通过url创建一个Redis连接客户端实例 else: return redis_cls(**kwargs) dupefilter.py 实现去重机制 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131import loggingimport timefrom scrapy.dupefilters import BaseDupeFilterfrom scrapy.utils.request import request_fingerprint # 计算url的指纹from . import defaultsfrom .connection import get_redis_from_settingslogger = logging.getLogger(__name__)# TODO: Rename class to RedisDupeFilter.class RFPDupeFilter(BaseDupeFilter): """Redis-based request duplicates filter. This class can also be used with default Scrapy's scheduler. """ logger = logger def __init__(self, server, key, debug=False): """Initialize the duplicates filter. Parameters ---------- server : redis.StrictRedis The redis server instance. key : str Redis key Where to store fingerprints. debug : bool, optional Whether to log filtered requests. """ self.server = server self.key = key self.debug = debug self.logdupes = True @classmethod def from_settings(cls, settings): """Returns an instance from given settings. This uses by default the key ``dupefilter:&lt;timestamp&gt;``. When using the ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as it needs to pass the spider name in the key. Parameters ---------- settings : scrapy.settings.Settings Returns ------- RFPDupeFilter A RFPDupeFilter instance. """ server = get_redis_from_settings(settings) # XXX: This creates one-time key. needed to support to use this # class as standalone dupefilter with scrapy's default scheduler # if scrapy passes spider on open() method this wouldn't be needed # TODO: Use SCRAPY_JOB env as default and fallback to timestamp. key = defaults.DUPEFILTER_KEY % &#123;'timestamp': int(time.time())&#125; debug = settings.getbool('DUPEFILTER_DEBUG') return cls(server, key=key, debug=debug) @classmethod def from_crawler(cls, crawler): """Returns instance from crawler. Parameters ---------- crawler : scrapy.crawler.Crawler Returns ------- RFPDupeFilter Instance of RFPDupeFilter. """ return cls.from_settings(crawler.settings) def request_seen(self, request): """ 去重判断 如果request已经请求过了，则返回false """ fp = self.request_fingerprint(request) # This returns the number of values added, zero if already exists. # redis客户端的sadd操作，即向集合里添加元素 added = self.server.sadd(self.key, fp) return added == 0 def request_fingerprint(self, request): return request_fingerprint(request) def close(self, reason=''): """Delete data on close. Called by Scrapy's scheduler. 退出时清除URL指纹数据 Parameters ---------- reason : str, optional """ self.clear() def clear(self): """Clears fingerprints data.""" self.server.delete(self.key) def log(self, request, spider): """Logs given request. Parameters ---------- request : scrapy.http.Request spider : scrapy.spiders.Spider """ if self.debug: msg = "Filtered duplicate request: %(request)s" self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) elif self.logdupes: msg = ("Filtered duplicate request %(request)s" " - no more duplicates will be shown" " (see DUPEFILTER_DEBUG to show all duplicates)") self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) self.logdupes = False picklecompat.py 实现序列化， 参考廖雪峰-序列化 12345678910111213141516"""A pickle wrapper module with protocol=-1 by default.redis数据格式有整数和字符串，其他的Python复杂的数据类型需要序列化成字符串后存入redis把变量从内存中变成可存储或传输的过程称之为序列化，"""try: import cPickle as pickle # PY2except ImportError: import pickledef loads(s): return pickle.loads(s)def dumps(obj): return pickle.dumps(obj, protocol=-1) pipelines.py&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869from scrapy.utils.misc import load_objectfrom scrapy.utils.serialize import ScrapyJSONEncoderfrom twisted.internet.threads import deferToThreadfrom . import connection, defaultsdefault_serialize = ScrapyJSONEncoder().encodeclass RedisPipeline(object): """Pushes serialized item into a redis list/queue Settings -------- REDIS_ITEMS_KEY : str Redis key where to store items. REDIS_ITEMS_SERIALIZER : str Object path to serializer function. """ def __init__(self, server, key=defaults.PIPELINE_KEY, serialize_func=default_serialize): """Initialize pipeline. ---------- server : StrictRedis,Redis client instance. key : str,Redis key where to store items. serialize_func : callable,Items serializer function. """ self.server = server self.key = key # defaults.PIPELINE_KEY = '%(spider)s:items',不同的爬虫用不同的key self.serialize = serialize_func @classmethod def from_settings(cls, settings): params = &#123; 'server': connection.from_settings(settings), &#125; if settings.get('REDIS_ITEMS_KEY'): params['key'] = settings['REDIS_ITEMS_KEY'] if settings.get('REDIS_ITEMS_SERIALIZER'): params['serialize_func'] = load_object( settings['REDIS_ITEMS_SERIALIZER'] ) return cls(**params) @classmethod def from_crawler(cls, crawler): return cls.from_settings(crawler.settings) def process_item(self, item, spider): return deferToThread(self._process_item, item, spider) def _process_item(self, item, spider): key = self.item_key(item, spider) data = self.serialize(item) self.server.rpush(key, data) return item def item_key(self, item, spider): """Returns redis key based on given spider. Override this function to use a different key depending on the item and/or spider. """ return self.key % &#123;'spider': spider.name&#125; Queue.py 实现消息队列 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139from scrapy.utils.reqser import request_to_dict, request_from_dictfrom . import picklecompatclass Base(object): """Per-spider base queue class""" def __init__(self, server, spider, key, serializer=None): """Initialize per-spider redis queue. ---------- server : StrictRedis,Redis client instance. spider : Spider,Scrapy spider instance. key: str, Redis key where to put and get messages. serializer : object, Serializer object with ``loads`` and ``dumps`` methods. """ if serializer is None: # Backward compatibility. # TODO: deprecate pickle. serializer = picklecompat if not hasattr(serializer, 'loads'): raise TypeError("serializer does not implement 'loads' function: %r" % serializer) if not hasattr(serializer, 'dumps'): raise TypeError("serializer '%s' does not implement 'dumps' function: %r" % serializer) self.server = server self.spider = spider self.key = key % &#123;'spider': spider.name&#125; self.serializer = serializer def _encode_request(self, request): """Encode a request object""" obj = request_to_dict(request, self.spider) return self.serializer.dumps(obj) def _decode_request(self, encoded_request): """Decode an request previously encoded""" obj = self.serializer.loads(encoded_request) return request_from_dict(obj, self.spider) def __len__(self): """Return the length of the queue""" raise NotImplementedError def push(self, request): """Push a request""" raise NotImplementedError def pop(self, timeout=0): """Pop a request""" raise NotImplementedError def clear(self): """Clear queue/stack""" self.server.delete(self.key)class FifoQueue(Base): """Per-spider FIFO queue""" def __len__(self): """Return the length of the queue""" return self.server.llen(self.key) def push(self, request): """Push a request""" self.server.lpush(self.key, self._encode_request(request)) def pop(self, timeout=0): """Pop a request""" if timeout &gt; 0: data = self.server.brpop(self.key, timeout) if isinstance(data, tuple): data = data[1] else: data = self.server.rpop(self.key) if data: return self._decode_request(data)class PriorityQueue(Base): """Per-spider priority queue abstraction using redis' sorted set""" def __len__(self): """Return the length of the queue""" return self.server.zcard(self.key) def push(self, request): """Push a request""" data = self._encode_request(request) score = -request.priority # We don't use zadd method as the order of arguments change depending on # whether the class is Redis or StrictRedis, and the option of using # kwargs only accepts strings, not bytes. self.server.execute_command('ZADD', self.key, score, data) def pop(self, timeout=0): """ Pop a request timeout not support in this queue class """ # use atomic range/remove using multi/exec pipe = self.server.pipeline() pipe.multi() pipe.zrange(self.key, 0, 0).zremrangebyrank(self.key, 0, 0) results, count = pipe.execute() if results: return self._decode_request(results[0])class LifoQueue(Base): """Per-spider LIFO queue.""" def __len__(self): """Return the length of the stack""" return self.server.llen(self.key) def push(self, request): """Push a request""" self.server.lpush(self.key, self._encode_request(request)) def pop(self, timeout=0): """Pop a request""" if timeout &gt; 0: data = self.server.blpop(self.key, timeout) if isinstance(data, tuple): data = data[1] else: data = self.server.lpop(self.key) if data: return self._decode_request(data)# TODO: Deprecate the use of these names.SpiderQueue = FifoQueue # 先进先出SpiderStack = LifoQueue # 后进先出SpiderPriorityQueue = PriorityQueue # 优先级队列 scheduler.py 调度，对request查重，为spider分配request任务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180import importlibimport sixfrom scrapy.utils.misc import load_objectfrom . import connection, defaults# TODO: add SCRAPY_JOB support.class Scheduler(object): """Redis-based scheduler Settings -------- SCHEDULER_PERSIST : bool (default: False) Whether to persist or clear redis queue. SCHEDULER_FLUSH_ON_START : bool (default: False) Whether to flush redis queue on start. SCHEDULER_IDLE_BEFORE_CLOSE : int (default: 0) How many seconds to wait before closing if no message is received. SCHEDULER_QUEUE_KEY : str Scheduler redis key. SCHEDULER_QUEUE_CLASS : str Scheduler queue class. SCHEDULER_DUPEFILTER_KEY : str Scheduler dupefilter redis key. SCHEDULER_DUPEFILTER_CLASS : str Scheduler dupefilter class. SCHEDULER_SERIALIZER : str Scheduler serializer. """ def __init__(self, server, persist=False, flush_on_start=False, queue_key=defaults.SCHEDULER_QUEUE_KEY, queue_cls=defaults.SCHEDULER_QUEUE_CLASS, dupefilter_key=defaults.SCHEDULER_DUPEFILTER_KEY, dupefilter_cls=defaults.SCHEDULER_DUPEFILTER_CLASS, idle_before_close=0, serializer=None): """Initialize scheduler. Parameters ---------- server : Redis The redis server instance. persist : bool Whether to flush requests when closing. Default is False. flush_on_start : bool Whether to flush requests on start. Default is False. queue_key : str Requests queue key. queue_cls : str Importable path to the queue class. dupefilter_key : str Duplicates filter key. dupefilter_cls : str Importable path to the dupefilter class. idle_before_close : int Timeout before giving up. """ if idle_before_close &lt; 0: raise TypeError("idle_before_close cannot be negative") self.server = server self.persist = persist self.flush_on_start = flush_on_start self.queue_key = queue_key self.queue_cls = queue_cls self.dupefilter_cls = dupefilter_cls self.dupefilter_key = dupefilter_key self.idle_before_close = idle_before_close self.serializer = serializer self.stats = None def __len__(self): return len(self.queue) @classmethod def from_settings(cls, settings): kwargs = &#123; 'persist': settings.getbool('SCHEDULER_PERSIST'), 'flush_on_start': settings.getbool('SCHEDULER_FLUSH_ON_START'), 'idle_before_close': settings.getint('SCHEDULER_IDLE_BEFORE_CLOSE'), &#125; # If these values are missing, it means we want to use the defaults. optional = &#123; # TODO: Use custom prefixes for this settings to note that are # specific to scrapy-redis. 'queue_key': 'SCHEDULER_QUEUE_KEY', 'queue_cls': 'SCHEDULER_QUEUE_CLASS', 'dupefilter_key': 'SCHEDULER_DUPEFILTER_KEY', # We use the default setting name to keep compatibility. 'dupefilter_cls': 'DUPEFILTER_CLASS', 'serializer': 'SCHEDULER_SERIALIZER', &#125; for name, setting_name in optional.items(): val = settings.get(setting_name) if val: kwargs[name] = val # Support serializer as a path to a module. if isinstance(kwargs.get('serializer'), six.string_types): kwargs['serializer'] = importlib.import_module(kwargs['serializer']) server = connection.from_settings(settings) # Ensure the connection is working. server.ping() return cls(server=server, **kwargs) @classmethod def from_crawler(cls, crawler): instance = cls.from_settings(crawler.settings) # FIXME: for now, stats are only supported from this constructor instance.stats = crawler.stats return instance def open(self, spider): self.spider = spider try: # 根据对象名字创建一个对象 self.queue = load_object(self.queue_cls)( server=self.server, spider=spider, key=self.queue_key % &#123;'spider': spider.name&#125;, serializer=self.serializer, ) except TypeError as e: raise ValueError("Failed to instantiate queue class '%s': %s", self.queue_cls, e) try: self.df = load_object(self.dupefilter_cls)( server=self.server, key=self.dupefilter_key % &#123;'spider': spider.name&#125;, debug=spider.settings.getbool('DUPEFILTER_DEBUG'), ) except TypeError as e: raise ValueError("Failed to instantiate dupefilter class '%s': %s", self.dupefilter_cls, e) if self.flush_on_start: self.flush() # notice if there are requests already in the queue to resume the crawl if len(self.queue): spider.log("Resuming crawl (%d requests scheduled)" % len(self.queue)) def close(self, reason): if not self.persist: self.flush() def flush(self): self.df.clear() self.queue.clear() def enqueue_request(self, request): # 如果需要检测去重，且检测到有重复，返回False if not request.dont_filter and self.df.request_seen(request): self.df.log(request, self.spider) return False if self.stats: self.stats.inc_value('scheduler/enqueued/redis', spider=self.spider) # 将request入队，返回True self.queue.push(request) return True def next_request(self): block_pop_timeout = self.idle_before_close # 分发request request = self.queue.pop(block_pop_timeout) if request and self.stats: self.stats.inc_value('scheduler/dequeued/redis', spider=self.spider) return request def has_pending_requests(self): return len(self) &gt; 0 spiders.py 爬虫类， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188from scrapy import signalsfrom scrapy.exceptions import DontCloseSpiderfrom scrapy.spiders import Spider, CrawlSpiderfrom . import connection, defaultsfrom .utils import bytes_to_str# Mixin 混合类型，利用多重继承来简洁的实现组合模式class RedisMixin(object): """Mixin class to implement reading urls from a redis queue.""" redis_key = None redis_batch_size = None redis_encoding = None # Redis client placeholder. server = None def start_requests(self): """Returns a batch of start requests from redis.""" return self.next_requests() def setup_redis(self, crawler=None): """Setup redis connection and idle signal. This should be called after the spider has set its crawler object. """ if self.server is not None: return if crawler is None: # We allow optional crawler argument to keep backwards # compatibility. # XXX: Raise a deprecation warning. crawler = getattr(self, 'crawler', None) if crawler is None: raise ValueError("crawler is required") settings = crawler.settings if self.redis_key is None: self.redis_key = settings.get( 'REDIS_START_URLS_KEY', defaults.START_URLS_KEY, ) self.redis_key = self.redis_key % &#123;'name': self.name&#125; if not self.redis_key.strip(): raise ValueError("redis_key must not be empty") if self.redis_batch_size is None: # TODO: Deprecate this setting (REDIS_START_URLS_BATCH_SIZE). self.redis_batch_size = settings.getint( 'REDIS_START_URLS_BATCH_SIZE', settings.getint('CONCURRENT_REQUESTS'), ) try: self.redis_batch_size = int(self.redis_batch_size) except (TypeError, ValueError): raise ValueError("redis_batch_size must be an integer") if self.redis_encoding is None: self.redis_encoding = settings.get('REDIS_ENCODING', defaults.REDIS_ENCODING) self.logger.info("Reading start URLs from redis key '%(redis_key)s' " "(batch size: %(redis_batch_size)s, encoding: %(redis_encoding)s", self.__dict__) self.server = connection.from_settings(crawler.settings) # The idle signal is called when the spider has no requests left, # that's when we will schedule new requests from redis queue crawler.signals.connect(self.spider_idle, signal=signals.spider_idle) def next_requests(self): """Returns a request to be scheduled or none.""" use_set = self.settings.getbool('REDIS_START_URLS_AS_SET', defaults.START_URLS_AS_SET) fetch_one = self.server.spop if use_set else self.server.lpop # XXX: Do we need to use a timeout here? found = 0 # TODO: Use redis pipeline execution. while found &lt; self.redis_batch_size: data = fetch_one(self.redis_key) # 取出一条数据 if not data: # Queue empty. break req = self.make_request_from_data(data) if req: yield req found += 1 else: self.logger.debug("Request not made from data: %r", data) if found: self.logger.debug("Read %s requests from '%s'", found, self.redis_key) def make_request_from_data(self, data): """Returns a Request instance from data coming from Redis. By default, ``data`` is an encoded URL. You can override this method to provide your own message decoding. Parameters ---------- data : bytes Message from redis. """ url = bytes_to_str(data, self.redis_encoding) return self.make_requests_from_url(url) def schedule_next_requests(self): """Schedules a request if available""" # TODO: While there is capacity, schedule a batch of redis requests. for req in self.next_requests(): self.crawler.engine.crawl(req, spider=self) # 当爬虫空闲时 def spider_idle(self): """Schedules a request if available, otherwise waits.""" # XXX: Handle a sentinel to close the spider. self.schedule_next_requests() raise DontCloseSpiderclass RedisSpider(RedisMixin, Spider): """Spider that reads urls from redis queue when idle. Attributes ---------- redis_key : str (default: REDIS_START_URLS_KEY) Redis key where to fetch start URLs from.. redis_batch_size : int (default: CONCURRENT_REQUESTS) Number of messages to fetch from redis on each attempt. redis_encoding : str (default: REDIS_ENCODING) Encoding to use when decoding messages from redis queue. Settings -------- REDIS_START_URLS_KEY : str (default: "&lt;spider.name&gt;:start_urls") Default Redis key where to fetch start URLs from.. REDIS_START_URLS_BATCH_SIZE : int (deprecated by CONCURRENT_REQUESTS) Default number of messages to fetch from redis on each attempt. REDIS_START_URLS_AS_SET : bool (default: False) Use SET operations to retrieve messages from the redis queue. If False, the messages are retrieve using the LPOP command. REDIS_ENCODING : str (default: "utf-8") Default encoding to use when decoding messages from redis queue. """ @classmethod def from_crawler(self, crawler, *args, **kwargs): obj = super(RedisSpider, self).from_crawler(crawler, *args, **kwargs) obj.setup_redis(crawler) return objclass RedisCrawlSpider(RedisMixin, CrawlSpider): """Spider that reads urls from redis queue when idle. Attributes ---------- redis_key : str (default: REDIS_START_URLS_KEY) Redis key where to fetch start URLs from.. redis_batch_size : int (default: CONCURRENT_REQUESTS) Number of messages to fetch from redis on each attempt. redis_encoding : str (default: REDIS_ENCODING) Encoding to use when decoding messages from redis queue. Settings -------- REDIS_START_URLS_KEY : str (default: "&lt;spider.name&gt;:start_urls") Default Redis key where to fetch start URLs from.. REDIS_START_URLS_BATCH_SIZE : int (deprecated by CONCURRENT_REQUESTS) Default number of messages to fetch from redis on each attempt. REDIS_START_URLS_AS_SET : bool (default: True) Use SET operations to retrieve messages from the redis queue. REDIS_ENCODING : str (default: "utf-8") Default encoding to use when decoding messages from redis queue. """ @classmethod def from_crawler(self, crawler, *args, **kwargs): obj = super(RedisCrawlSpider, self).from_crawler(crawler, *args, **kwargs) obj.setup_redis(crawler) return obj]]></content>
      <categories>
        <category>技术</category>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis学习笔记]]></title>
    <url>%2F2017%2F03%2F01%2F%E6%8A%80%E6%9C%AF%2Fredis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[redis学习笔记no sql：redis，mongodb，Memcached redis：Remote Directory Server，远程字典服务器。 键值数据类型支持： ●字符串String类型 set、get、getset。。。 ●散列Hash类型 类似python的字典 hset、hget、hdel、hgetall、hkeys、hvals ●列表List类型 lpush、rpush、lpop、rpop、llen ●集合Set类型 sadd、smembers、srem、spop、sdiff(差集)、sinter(交集)、sunion(并集)、scard(长度)、 ●有序集合Zset类型 ZADD key [NX|XX][CH] [INCR] score member [score member ...] zscore、zcard、zrank、 一个键最多存储512MB 官网 1、下载安装linux下安装apt-get install redis-server,启动service redis-server start redis-server ：服务器 redis-benchmark：性能测试工具 redis-cli：命令行客户端 redis-check-dump：RDB文件检测工具 redis-check-aof：AOF文件修复工具 启动命令 redis-server redis.windows.conf 设置Redis服务 由于上面虽然启动了redis，但是只要一关闭cmd窗口，redis就会消失。所以要把redis设置成windows下的服务。 redis-server --service-install redis.windows-service.conf --loglevel verbose 卸载服务：redis-server –service-uninstall 开启服务：redis-server –service-start 停止服务：redis-server –service-stop 测试： redis-cli -h 127.0.0.1 -p 6379 参数配置 启动参数 redis-server redis.windows.conf –port xxx –loglevel notice 命令行客户端中动态修改 127.0.0.1:6379&gt; CONFIG get loglevel1) “loglevel”2) “notice”127.0.0.1:6379&gt; CONFIG GET port1) “port”2) “6379”127.0.0.1:6379&gt; CONFIG SET loglevel warningOK127.0.0.1:6379&gt; CONFIG GET loglevel1) “loglevel”2) “warning” 配置文件redis.conf port 6379 默认端口 bind 127.0.0.1，默认绑定的主机地址 timeout 0，当客户端闲置多久之后关闭连接，0代表没有启动这个选项 loglevel notice，日志的记录级别 # debug：很详细的信息，适合开发和测试 # verbose ：包含很多不太有用的信息 # notice ：比较适合生产环境 # warning ：警告信息 logfile stdout代表日志的记录方式，默认为标准输出 databases 16代表默认数据库的数量16个,默认的数据库编号从0开始，select 1 选择数据库1 save 300 10 –300秒内将10个更改同步到磁盘 save 900 1 dbfilename dump.rdb，指定本地数据库文件名，默认为dump.rdb dir ./,指定本地数据库的存放目录，默认是当前目录 requirepass password 设置认证 ​ ​ 2、客户端命令行2.1、命令返回消息类型： 状态信息 127.0.0.1:6379&gt; set test “test_value” OK 127.0.0.1:6379&gt; ping PONG 错误回复 127.0.0.1:6379&gt; xx (error) ERR unknown command ‘xx’ 整数 127.0.0.1:6379&gt; DBSIZE (integer) 1 字符串 127.0.0.1:6379&gt; get a “1” 127.0.0.1:6379&gt; get test “test_value” 127.0.0.1:6379&gt; get aa (nil) nil表示空的结果 多行字符串 127.0.0.1:6379&gt; keys * 1) “b” 2) “test” 3) “a” 2.2、常用命令‘redis-cli -h 127.0.0.1 -p 6379 -a passwd’ select 0 选择0号数据库 exit quit 断开连接 shutdown 同时关闭服务器 参考文档http://doc.redisfans.com/ 2.3、使用python redis-py连接Redispip install redis redis-py没有实现select 1234r = redis.Redis(host='localhost', port=6379, db=0, password=None)print r.set('a', 'a_value', ex=None, px=None, nx=False, xx=False)print r.get('a')print r.config_get('loglevel') 使用pipeline提高执行效率 1234567r = redis.Redis(host='localhost', port=6379, db=1)p = r.pipeline()p.set('k1', 'v1')p.set('qqq', 2)p.incr('num1')p.execute()print r.keys('*') 3、事务事务可以理解为一些列操作的集合，或一个过程。要么成功（全部执行），要么失败（全部都不被执行）。 开启事务 EXEC 执行事务 事务的执行 监视key WATCH counter1 counter2 如果在执行事务之前key如果被其它命令改动，事务就被打断了，不会执行。 UNWATCH:取消WATCH命令对所有key的监视 取消事务 DISCARD 错误处理 语法错误 错误的语法不会被添加到队列中，自然也不会被执行。 运行出错 事务中QUEUED的语句，在运行中出错，其他的命令也会成功执行。Redis事务不支持回滚。 4、缓存和Redis生存时间redis的键值可以设置生存时间，到期后自动删除。 EXPIRE（设置过期的秒数）/EXPIREAT（在指定的时间戳进行删除） PEXPIRE（设置过期的微秒数）/PEXPIREAT（在指定的时间戳进行删除） PERSIST（永不过期，持久化） TTL （得到某个键的生存时间） PTTL （得到某个键的生存时间） 将近期访问过得数据保存到redis中。如果数据不在Redis中，再去数据库中取。 如果大量的使用这种缓存键而且生存时间设置的过长，Redis会占用大量内存。 如果缓存键的生存时间设置太短的话，可能会导致缓存的命中率过低，内存利用率低。 5、消息队列5.1、生产者-消费者模式123456789101112131415161718# coding: utf-8import redisclass Task: def __init__(self): self.rcon = redis.Redis(db=5) self.queue = 'task:prodcons:queue' def process_task(self): while True: task = self.rcon.blpop(self.queue, 0)[1] # blpop阻塞式的pop print 'Task: %s' % taskTask().process_task()# 向'task:prodcons:queue'里lpush数据后，会有数据打印出来 5.2、发布-订阅模式12345678910111213141516171819# coding: utf-8import redisclass Task: def __init__(self): self.rcon = redis.Redis(db=5) self.ps = self.rcon.pubsub() self.ps.subscribe('task:pubsub:channel') # 订阅频道 def process_task(self): for i in self.ps.listen(): # 是个生成器 if i['type'] == 'message': print 'Task: %s' % i['data']Task().process_task()# 向'task:pubsub:channel'里publish数据后，每个订阅该频道的程序都会会打印打印相同的消息出来]]></content>
      <categories>
        <category>技术</category>
        <category>redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy线性运算]]></title>
    <url>%2F2017%2F02%2F28%2F%E6%8A%80%E6%9C%AF%2Fnumpy-liner-skills%2F</url>
    <content type="text"><![CDATA[线性代数 解方程组 Numpy和Scipy模块 Numpy进行简单的描述性统计计算 Numpy进行线性代数运算 Numpy计算特征值和特征向量 Numpy随机数 创建掩码式Numpy数组 描述性统计计算12345678910# 计算numpy数组的平均值，中位数，最大值，最小值和标准差import numpy as npfrom scipy.stats import scoreatpercentile # 就算第N%的数值data = np.loadtxt('ex1.csv', delimiter=',', usecols=(1,), skiprows=1, unpack=True)print dataprint "Max:", data.max(), np.max(data)print "Min:", data.min(), np.min(data)print "Mean:", data.mean(), np.mean(data)print "Std方差:", data.std(), np.std(data)print "Median:", np.median(data), scoreatpercentile(data, 50) [ 2. 6. 10.] Max: 10.0 10.0 Min: 2.0 2.0 Mean: 6.0 6.0 Std方差: 3.26598632371 3.26598632371 Median: 6.0 6.0 线性代数运算 numpy.linalg 或 scipy.linalg 矩阵求逆运算，转置，计算特征值，求解线性方程组或计算行列式矩阵可用numpy.ndarray的一个子类表示。 1234567891011121314# 创建一个矩阵A = np.mat("2 4 6;4 2 6;10 -4 18")print A# 矩阵求逆A_inverse = np.linalg.inv(A)print A_inverseprint "如果矩阵是不可逆的，可以用pinv求伪逆矩阵"A_inverse = np.linalg.pinv(A)print A_inverse# A*A- = IX = A * A_inverseprint X# 计算误差print X - np.eye(3) [[ 2 4 6] [ 4 2 6] [10 -4 18]] [[-0.41666667 0.66666667 -0.08333333] [ 0.08333333 0.16666667 -0.08333333] [ 0.25 -0.33333333 0.08333333]] 如果矩阵是不可逆的，可以用pinv求伪逆矩阵 [[-0.41666667 0.66666667 -0.08333333] [ 0.08333333 0.16666667 -0.08333333] [ 0.25 -0.33333333 0.08333333]] [[ 1.00000000e+00 -1.66533454e-15 4.44089210e-16] [ 6.66133815e-16 1.00000000e+00 3.88578059e-16] [ 1.11022302e-15 -1.88737914e-15 1.00000000e+00]] [[ 8.88178420e-16 -1.66533454e-15 4.44089210e-16] [ 6.66133815e-16 -1.22124533e-15 3.88578059e-16] [ 1.11022302e-15 -1.88737914e-15 8.88178420e-16]] 求解线性方程组， solve() 和 dot() 12345678A = np.mat("1 -2 1;0 2 -8;-4 5 9")b = np.array([0, 8, 9])# Ax=b，求x，用np.linalg.solvex = np.linalg.solve(A, b)print x# Ax=b,求解b，用np.dotb1 = np.dot(A, x)print b1 [ 155. 88. 21.] [[ 0. 8. 9.]] 若存在常数λ及n维非零向量X，使得Ax=λx，则称λ是矩阵A的特征值，x是A属于特征值λ的特征向量.λ = np.linalg.eigvals(A)(λ, x) = np.linalg.eig(A) 12345678A = np.mat("3,-2;1,0")a, x = np.linalg.eig(A)b = np.linalg.eigvals(A)print a, b, x# 验证Ax=λxfor i in range(len(x)): print 'A*x&#123;&#125;'.format(i), np.dot(A, x[:, i]) print 'a*x&#123;&#125;'.format(i), a[i] * x[:, i] [ 2. 1.] [ 2. 1.] [[ 0.89442719 0.70710678] [ 0.4472136 0.70710678]] A*x0 [[ 1.78885438] [ 0.89442719]] a*x0 [[ 1.78885438] [ 0.89442719]] A*x1 [[ 0.70710678] [ 0.70710678]] a*x1 [[ 0.70710678] [ 0.70710678]] numpy随机数，随机相关函数位于np.random模块。连续分布：正态分布和对数正态分布离散分布：几何分布，超几何分布和二项分布 123456789101112131415161718192021222324# 用np.random.binomial()模拟二项分布,提供相同的种子，则产生的随机结果相同from matplotlib import pyplot as plt%matplotlib inline# B(0.5, 9) 生成执行10000次的博弈结果序列 outcome = np.random.binomial(9, 0.5, size=10000)plt.subplot(121)plt.plot(np.arange(10000), outcome)money = np.zeros(10000)my_money = 1000i = 0for x in outcome: if x &lt; 5: money[i] = my_money - 1 elif x &lt; 10: money[i] = my_money + 1 my_money = money[i] i = i + 1 print moneyplt.subplot(122)plt.plot(np.arange(10000), money)plt.show() [ 999. 1000. 999. ..., 972. 971. 970.] ​ 正态分布采样np.random模块提供了很多表示连续随机分布的函数 Draw random samples from a normal (Gaussian) distribution. np.random.normal(loc, scale, size)loc : float,Mean (“centre”) of the distribution.scale : float,Standard deviation (spread or “width”) of the distribution.size : int or tuple of ints, optional,Output shape. 12345678N = 10000normal_values = np.random.normal(size=N)dummy, bins, dummy = plt.hist(normal_values, np.sqrt(N), normed=True, lw=1) # dunmmy中文蠢货，假的；bins箱子，也就是直方图的条条sigma = 1mu = 0fx = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp(- (bins - mu) ** 2 / (2 * sigma ** 2))plt.plot(bins, fx, lw=2) ## lw: linewidthplt.show() 正态检测：检查样本是否符合正态分布scipy.stats里实现了一些正态分布的检测方法。 创建掩码式数组掩码式数组可以忽略无效的、残缺的数据。如忽略负值和极值]]></content>
      <categories>
        <category>技术</category>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib笔记]]></title>
    <url>%2F2017%2F02%2F26%2F%E6%8A%80%E6%9C%AF%2Fuse-matplotlib%2F</url>
    <content type="text"><![CDATA[matplotlib matplotlib API pandas中绘图函数 1%pylab Using matplotlib backend: Qt5Agg Populating the interactive namespace from numpy and matplotlib 123import matplotlib.pyplot as pltimport numpy as npimport pandas as pd 1%matplotlib inline Figure 和Subplot创建一个figure后，还需要在上面添加子图，然后调用子图的绘图方法绘图。直接使用plot绘图，图像绘制在最后一次选定的子图 1234567891011from numpy.random import randnfig = plt.figure()sp1 = fig.add_subplot(2, 2, 1) # 2*2布局的第一个子图sp1.plot(randn(50).cumsum(), 'k--') # k--黑色虚线图sp2 = fig.add_subplot(2, 2, 2)sp2.hist(randn(100), bins=20, color='k', alpha=0.3) ## 直方图，20个箱子sp3 = fig.add_subplot(2, 2, 3)sp3.scatter(np.arange(30), np.arange(30) + randn(30) * 3, color = 'b')# sp4 = fig.add_subplot(2, 2, 4) 12345# 使用plt.subplot()创建子图fig, axes = plt.subplots(2, 2) ## 返回画布和子图对象的数组axes[0][0].plot(range(10), range(10))## 调整子图间的距离, wspace和hspace 子图间百分比fig.subplots_adjust(left=1, right=2, wspace=0, hspace=0) 颜色、标记、线型plot函数接受一组X和y坐标，还有格式字符串。表示颜色和线型的字符串,如“g–”,表示绿色的折线“go–”, 绿色O点折线，marker=’o’也可以在plot中指定参数 linestyle=’–’ color=’g’ color值可以用缩写词或RGB值，“#CECECE” 线形图的点可以加上标记marker，更容易看出点的位置 label 图线的label drawstyle 绘图方式 fig的方法 fig.xlabel(‘xxxx’) fig.ylabel(‘y’) fig.xlim() 1234567plt.plot(randn(30).cumsum(), 'go--', label='yyy')plt.plot(randn(30).cumsum(), color='b', marker='*', linestyle='--', label='xxx')plt.plot(randn(30).cumsum(), color='r', drawstyle='steps-post', label='zzz')plt.legend(loc='best') # 图例，label的位置为bestplt.xlabel('X')plt.ylabel('random')plt.show() 12plt.xlim() # 当前x轴的范围plt.xlim([-10,10]) # 调整x轴的范围 (-10, 10) 123456#### 注解以及在subplot上绘图ax = plt.subplot(1, 1, 1)ax.plot(range(10), range(3, 13), 'g*--', label='line')ax.legend('best')ax.text(2, 5, 'Here!', family='monospace', fontsize=10)ax.annotate('Look Here!', xy=(3, 6), arrowprops=dict(facecolor='black')) #,family='monospace', fontsize=10) pandas中的绘图函数线型图Series和Dataframe的plot方法默认生成的是线型图 柱状图添加参数kind=bar或barh，生成柱状图和水平方向的柱状图 直方图和密度图直方图是一种可以对值频率进行离散化显示的柱状图，plot.hist()密度图：计算“可能会产生观测数据的连续发布的估计”，一般是将该分布近似为一组核（如高斯正态分布）分布。 散布图观察两组一维数据之间关系的有效方法 123from pandas import Series, DataFrames = Series(randn(10).cumsum(), index=range(0, 100, 10))s.plot() 12345678fig, axes = plt.subplots(2, 2)s.plot(ax=axes[0][0])frame = DataFrame(randn(10, 4).cumsum(0), columns=list('ABCD'), index=range(0, 100, 10))frame.plot(ax=axes[0][1])frame.plot(ax=axes[0][1])### 柱状图s.plot(ax=axes[1][0], kind='barh')frame.plot(ax=axes[1][1], kind='bar') 123456fig, axes = plt.subplots(2, 2)## 堆积柱状图s.plot(ax=axes[0][0], kind='bar', stacked=True, alpha=0.3) frame.plot(ax=axes[0][1], kind='bar', stacked=True, alpha=0.5)s.value_counts().plot(ax=axes[1][0], kind='bar', stacked=True, alpha=0.3)frame.ix[:, 2:8].plot(ax=axes[1][1], kind='bar', stacked=True, alpha=0.5) 123456fig, axes = plt.subplots(1, 2)s = Series(randn(100).cumsum(), index=range(0, 1000, 10))# 直方图s.hist(ax=axes[0], bins=40)# 密度图s.plot(ax=axes[1], kind='kde', color='k') 12s.hist(bins=40, normed=True)s.plot(kind='kde', style='g--') 12### 散点图plt.scatter(frame['A'], frame['D']) 1pd.scatter_matrix(frame, diagonal='kde') array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021169438&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x00000000213EB4A8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x00000000214B22B0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021559710&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021667208&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000002174F978&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000002181B5F8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x00000000218DDCC0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021A2D9B0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021AA00B8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021B9DE80&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021BBE278&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021D61400&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021E70E10&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021F677F0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000022036240&gt;]], dtype=object)]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记]]></title>
    <url>%2F2017%2F02%2F25%2F%E6%8A%80%E6%9C%AF%2FPandas-skills%2F</url>
    <content type="text"><![CDATA[Pandas数据结构基本功能汇总和计算描述统计处理缺失数据层次化索引其他123from pandas import DataFrame, Seriesimport pandas as pdimport numpy as np Series12345678910# series类似一维数组，由一组数据和数据的标签（索引）组成s = Series([11, 32, -223, 114])print sprint s.indexprint s.values# 索引可以直接赋值修改s.index = ['a', 'b', 'c', 'd']print ss2 = Series([11, 32, -223, 114], index=['a', 'b', 'c', 'd'])print s2['a'], '\n', s2[['a', 'd']] 0 11 1 32 2 -223 3 114 dtype: int64 RangeIndex(start=0, stop=4, step=1) [ 11 32 -223 114] a 11 b 32 c -223 d 114 dtype: int64 11 a 11 d 114 dtype: int64 Numpy数组运算的结果会保留索引和值之间的链接 根据布尔型数组进行过滤，标量乘法，数学函数等 1234567891011121314print s2 &gt; 11 ## 得到布尔型数组print s2[s2 &gt; 11] ## 根据布尔型数组筛选对应为true的元素## 可以将series理解为定长字典，因为是key-value的对应关系，series也可以由字典对象创建d = &#123;'Alice': 98, 'Bob': 87, 'Cc': 67&#125;score1 = Series(d)print score1print 'Alice' in score1## 同时传入字典和index索引student = ['Alice', 'Cc', 'Sam', 'Wandou']score2 = Series(d, index=student) # 会根据index查找字典中相应的数据，找不到的对应index的值为NaN，表示缺失或非数字print score2print score2.isnull()print score2.notnull() a False b True c False d True dtype: bool b 32 d 114 dtype: int64 Alice 98 Bob 87 Cc 67 dtype: int64 True Alice 98.0 Cc 67.0 Sam NaN Wandou NaN dtype: float64 Alice False Cc False Sam True Wandou True dtype: bool Alice True Cc True Sam False Wandou False dtype: bool 12345678## 在算术运算中，会自动对其不同索引的数据print score1, score2print score1 + score2 ## 索引和数据都有一个name属性score2.name = 'score'score2.index.name = 'name'print score2 Alice 98 Bob 87 Cc 67 dtype: int64 Alice 98.0 Cc 67.0 Sam NaN Wandou NaN dtype: float64 Alice 196.0 Bob NaN Cc 134.0 Sam NaN Wandou NaN dtype: float64 name Alice 98.0 Cc 67.0 Sam NaN Wandou NaN Name: score, dtype: float64 DataFramedataframe是一种表格型的数据结构，含有一组有序的列，每列的数值类型可以不同。dataframe既有行索引，又有列索引，可以看做Series组成的字典。 1234567891011121314151617181920212223242526## 构建DataFrame，传入等长的列表或Numpy数组province = ['Beijing', 'Shandong', 'Shanghai', 'Hubei', 'Hunan']area = [50, 156, 300, 250, 260]pop = [5000, 16000, 9000, 10000, 8000]data = &#123;'province': province, 'people': pop, 'area': area&#125;frame1 = DataFrame(data)print frame1# 指定列的排列顺序frame2 = DataFrame(data, columns=['province', 'area', 'people'])print frame2# 加入自定义索引frame3 = DataFrame(data, columns=['province', 'area', 'people'], index=data['province'])print frame3# 获取列print frame3['area']print frame3.area# 获取行print frame3.ix['Shandong']# 添加一列或修改一列，frame3['GDP_Unit'] = '亿元' # 一列所有数据都改为该值frame3['GDP'] = [150, 160, 170, 180, 190] print frame3frame3.GDP = Series(&#123;'Beijing': 0, 'Shandong': -1, 'Shanghai': 2, 'hah': 90&#125;)print frame3# 如果值是列表，要等长。# 如果是Series，则严格按照Series的索引赋值该列，series中无相应index的，frame中该值改为Na area people province 0 50 5000 Beijing 1 156 16000 Shandong 2 300 9000 Shanghai 3 250 10000 Hubei 4 260 8000 Hunan province area people 0 Beijing 50 5000 1 Shandong 156 16000 2 Shanghai 300 9000 3 Hubei 250 10000 4 Hunan 260 8000 province area people Beijing Beijing 50 5000 Shandong Shandong 156 16000 Shanghai Shanghai 300 9000 Hubei Hubei 250 10000 Hunan Hunan 260 8000 Beijing 50 Shandong 156 Shanghai 300 Hubei 250 Hunan 260 Name: area, dtype: int64 Beijing 50 Shandong 156 Shanghai 300 Hubei 250 Hunan 260 Name: area, dtype: int64 province Shandong area 156 people 16000 Name: Shandong, dtype: object province area people GDP_Unit GDP Beijing Beijing 50 5000 亿元 150 Shandong Shandong 156 16000 亿元 160 Shanghai Shanghai 300 9000 亿元 170 Hubei Hubei 250 10000 亿元 180 Hunan Hunan 260 8000 亿元 190 province area people GDP_Unit GDP Beijing Beijing 50 5000 亿元 0.0 Shandong Shandong 156 16000 亿元 -1.0 Shanghai Shanghai 300 9000 亿元 2.0 Hubei Hubei 250 10000 亿元 NaN Hunan Hunan 260 8000 亿元 NaN 1frame3.GDP.isnull() Beijing False Shandong False Shanghai False Hubei True Hunan True Name: GDP, dtype: bool 123## 删除一列del frame3['GDP']print frame3.columns Index([u&apos;province&apos;, u&apos;area&apos;, u&apos;people&apos;, u&apos;GDP_Unit&apos;], dtype=&apos;object&apos;) ​ 使用嵌套字典创建dataframe 外层的字典key作为列索引，内层的字典key作为行索引。frame可以转置。内层的字典key会被合并、排序最终成为行索引，显示指定了index的除外 123456789data = &#123;'province': &#123;'Shandong': 'Shangdong', 'Beijing': 'Beijing', 'Hubei': 'Hubei'&#125;, 'GDP': &#123;'Shanghai': 15000, 'Shandong': 20000, 'Hubei': 7000&#125;, 'people': &#123;'Shandong': 100000, 'Hubei': 20000&#125; &#125;frame = DataFrame(data) ## 缺省的值设为NaNprint frameframe = DataFrame(data, index=['Guangzhou', 'Beijing', 'Shanghai', 'Shandong'])print frameprint frame.T GDP people province Beijing NaN NaN Beijing Hubei 7000.0 20000.0 Hubei Shandong 20000.0 100000.0 Shangdong Shanghai 15000.0 NaN NaN GDP people province Guangzhou NaN NaN NaN Beijing NaN NaN Beijing Shanghai 15000.0 NaN NaN Shandong 20000.0 100000.0 Shangdong Guangzhou Beijing Shanghai Shandong GDP NaN NaN 15000 20000 people NaN NaN NaN 100000 province NaN Beijing NaN Shangdong 可以用于构造DataFrame的数据 二维ndarray 值为数组，列表或元组的字典， 每个数据序列必须等长，作为dataframe的一列 值为Series的字典， 每个series成为frame的一列，如果没有指定index索引，则series内的索引会成为行索引 嵌套字典 。。。 1frame3.values ## frame的values是ndarray， ndarray的数据类型会选择能兼容各列数据的类型 array([[&apos;Beijing&apos;, 50L, 5000L, &apos;\xe4\xba\xbf\xe5\x85\x83&apos;], [&apos;Shandong&apos;, 156L, 16000L, &apos;\xe4\xba\xbf\xe5\x85\x83&apos;], [&apos;Shanghai&apos;, 300L, 9000L, &apos;\xe4\xba\xbf\xe5\x85\x83&apos;], [&apos;Hubei&apos;, 250L, 10000L, &apos;\xe4\xba\xbf\xe5\x85\x83&apos;], [&apos;Hunan&apos;, 260L, 8000L, &apos;\xe4\xba\xbf\xe5\x85\x83&apos;]], dtype=object) 索引对象Index对象的值是不能不被修改的包含的方法有 append 连接一个index对象 diff 计算差集，并得到一个index intersect 计算交集 union 计算并集 isin 计算index各值是否在给定的集合内，返回一个布尔型数组 delete 删除索引i处的元素，并得到新的index drop insert is_monotonic 是否单调 is_unique 是否有重复值 unique 计算index中唯一值得数组 123456## 重新索引对象, 生成新的series或frameprint s.indexq = s.reindex(['e', 'd', 'c', 'b', 'a']) # 另外一个可选参数，fill_value=0设置默认值或method='填充方法'，ffill、pad向前填充，bfill或backfill向后填充print qp = frame3.reindex(index=[], columns=[], method='ffill') # 可以同时重新index和column，值填充只能按行应用 Index([u&apos;a&apos;, u&apos;b&apos;, u&apos;c&apos;, u&apos;d&apos;], dtype=&apos;object&apos;) e NaN d 114.0 c -223.0 b 32.0 a 11.0 dtype: float64 索引、选取和过滤 行列索引 布尔型dataframe索引 1234print frame3['area']print frame3[:4] # 选取的列print frame3[frame3['area'] &gt; 160] # 根据布尔型选取frame3.ix[:2, ['area', 'people']] # 同时选取行和列 Beijing 50 Shandong 156 Shanghai 300 Hubei 250 Hunan 260 Name: area, dtype: int64 province area people GDP_Unit Beijing Beijing 50 5000 亿元 Shandong Shandong 156 16000 亿元 Shanghai Shanghai 300 9000 亿元 Hubei Hubei 250 10000 亿元 province area people GDP_Unit Shanghai Shanghai 300 9000 亿元 Hubei Hubei 250 10000 亿元 Hunan Hunan 260 8000 亿元 area people Beijing 50 5000 Shandong 156 16000 12print frame3.iloc[1,:2] # 根据位置整数选取print frame3.loc[:, ['area', 'people']] # 同时根据行列lable选取 province Shandong area 156 Name: Shandong, dtype: object area people Beijing 50 5000 Shandong 156 16000 Shanghai 300 9000 Hubei 250 10000 Hunan 260 8000 算数运算和数据对齐计算是在两个对象索引的并集上运算，相同索引的值进行运算，不重叠的索引处引入NaN值。缺省值NaN在运算中会传播。在运算add、sub、div、mul中传入参数fill_value=0可以指定一个填充值。DataFrame和Series进行运算时，会产生广播。 函数应用和映射Numpy的元素级函数可用于操作Pandas对象 np.abs(frame) 和其他函数 frame.apply(funcs) frame.applymap(funcs) series.map() 123456789101112131415frame = DataFrame(np.random.randn(4, 3), columns=list('abc'), index=range(1, 5))frame2 = np.abs(frame)print frameprint frame2f1 = lambda x: x.max()print frame.apply(f1) # 将函数应用到frame的列print frame.max() ## 很多统计函数已被实现，没必要用applyprint frame.apply(f1, axis=1) # 将函数应用于frame的行## apply的函数除了返回值外，也可返回多个值组成的series，framef2 = lambda x: Series([x.min(), x.max()], index=['min', 'max'])print frame.apply(f2)## 如果要在元素级上应用函数，frame使用applymap(),series使用map()f3 = lambda x: '%.2f' %xprint frame.applymap(f3)print frame.a.map(f3) a b c 1 0.811815 0.501822 0.225176 2 0.171434 -1.446678 0.148284 3 -0.187686 -1.017413 1.145296 4 -0.596069 -0.376504 -0.298326 a b c 1 0.811815 0.501822 0.225176 2 0.171434 1.446678 0.148284 3 0.187686 1.017413 1.145296 4 0.596069 0.376504 0.298326 a 0.811815 b 0.501822 c 1.145296 dtype: float64 a 0.811815 b 0.501822 c 1.145296 dtype: float64 1 0.811815 2 0.171434 3 1.145296 4 -0.298326 dtype: float64 a b c min -0.596069 -1.446678 -0.298326 max 0.811815 0.501822 1.145296 a b c 1 0.81 0.50 0.23 2 0.17 -1.45 0.15 3 -0.19 -1.02 1.15 4 -0.60 -0.38 -0.30 1 0.81 2 0.17 3 -0.19 4 -0.60 Name: a, dtype: object 排序和排名123456s = Series(range(5), index=list('qwert'))print s.sort_index(ascending=False)print s.sort_values(ascending=False) # 缺省值会排序放到最后print frame.sort_index(axis=1, ascending=False)print frame.sort_values('a')print frame.rank(method='first') w 1 t 4 r 3 q 0 e 2 dtype: int64 t 4 r 3 e 2 w 1 q 0 dtype: int64 c b a 1 0.225176 0.501822 0.811815 2 0.148284 -1.446678 0.171434 3 1.145296 -1.017413 -0.187686 4 -0.298326 -0.376504 -0.596069 a b c 4 -0.596069 -0.376504 -0.298326 3 -0.187686 -1.017413 1.145296 2 0.171434 -1.446678 0.148284 1 0.811815 0.501822 0.225176 a b c 1 4.0 4.0 3.0 2 3.0 1.0 2.0 3 2.0 2.0 4.0 4 1.0 3.0 1.0 Index索引可以是重复的 选取了重复index的数据，返回的时series，非重复的直接返回其值frame的index也是可以重复的 123s = Series(range(5), index=list('qaqaz'))print s.aprint s.z a 1 a 3 dtype: int64 4]]></content>
      <categories>
        <category>技术</category>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy ImagePipeline下载gif图片和自定义图片文件名称]]></title>
    <url>%2F2017%2F02%2F16%2F%E6%8A%80%E6%9C%AF%2Fscrapy-image-download%2F</url>
    <content type="text"><![CDATA[scrapy图片下载保留gif格式和自定义图片名称继承ImagesPipeline并重写部分函数 get_media_requests 返回下载图片的Request file_path 返回文件名，文件名是sha1哈希image url生成的image_guid convert_image 该函数将图片转化成rgb 模式的jpg格式，避免重新下载近期下载过的图片 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130######################## pipelines.py ######################################## -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.htmlimport pymongofrom scrapy.pipelines.images import ImagesPipelinefrom scrapy.http import Request# from scrapy.exceptions import DropItemfrom PIL import Imagetry: from cStringIO import StringIO as BytesIOexcept ImportError: from io import BytesIOclass JokerPipeline(object): collection_name = 'joke' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get("MONGO_DATABASE", "items") ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): # data = self.db[self.collection_name].find(&#123;&#125;, &#123;'_id': 0, 'url': 1&#125;) # url_finished = set(x['url'] for x in data) # if item['pic-url']: # # raise DropItem("%s has been crawled!" % item) # pic_name = item['data-id'] + '.' + item['pic-url'].plite('.')[-1] # else: self.db[self.collection_name].insert(dict(item)) return item class ImageDownloadPipeline(ImagesPipeline): default_headers = &#123; 'accept': 'image/webp,image/*,*/*;q=0.8', 'accept-encoding': 'gzip, deflate, sdch, br', 'accept-language': 'zh-CN,zh;q=0.8,en;q=0.6', 'cookie': 'bid=yQdC/AzTaCw', 'referer': 'https://www.douban.com/photos/photo/2370443040/', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36', &#125; def get_media_requests(self, item, info): # 下载图片 for image_url in item['pic_url']: self.default_headers['referer'] = image_url yield Request(image_url, headers=self.default_headers, meta=&#123;'item': item&#125;) # # 使用meta是我想用item里的某个字段做图片的名字， def file_path(self, request, response=None, info=None): item = request.meta['item'] # 通过上面的meta传递过来item # 图片文件名，request.url.split('/')[-1].split('.')[-1]得到图片后缀jpg,png,gif image_guid = item['data-id'] + '-' + request.url.split('/')[-1] # 使用有中文的item字段做目录时，用path=''.join(item['no1']),否则路径会变成\u97e9\u56fd\u6c7d\u8f66\u6807\u5fd7\xxx.jpg filename = u'full/&#123;0&#125;'.format(image_guid) return filename # 图片格式转换，如果是gif，则不转换，可能会出问题 def convert_image(self, image, size=None): # 自己添加的 if image.format == 'GIF': buf = BytesIO() image.save(buf, 'GIF') return image, buf # 原始代码 if image.format == 'PNG' and image.mode == 'RGBA': background = Image.new('RGBA', image.size, (255, 255, 255)) background.paste(image, image) image = background.convert('RGB') elif image.mode != 'RGB': image = image.convert('RGB') if size: image = image.copy() image.thumbnail(size, Image.ANTIALIAS) buf = BytesIO() image.save(buf, 'JPEG') return image, buf # 重写下载完图片的处理方法image_downloaded def check_gif(self, image): if image.format == 'GIF': return True # The library reads GIF87a and GIF89a versions of the GIF file format. return image.info.get('version') in ['GIF89a', 'GIF87a'] def persist_gif(self, key, data, info): root, ext = os.path.splitext(key) if not key.endswith(('.gif', '.GIF')): key = key + '.gif' absolute_path = self.store._get_filesystem_path(key) self.store._mkdir(os.path.dirname(absolute_path), info) f = open(absolute_path, 'wb') # use 'b' to write binary data. f.write(data) def image_downloaded(self, response, request, info): checksum = None for key, image, buf in self.get_images(response, request, info): if checksum is None: buf.seek(0) checksum = md5sum(buf) if key.startswith('full') and self.check_gif(image): # Save gif from response directly. self.persist_gif(key, response.body, info) else: width, height = image.size self.store.persist_file( key, buf, info, meta=&#123;'width': width, 'height': height&#125;, headers=&#123;'Content-Type': 'image/jpeg'&#125;) # self.store.persist_image(key, image, buf, info) return checksum]]></content>
      <categories>
        <category>技术</category>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何优雅的使用Linux]]></title>
    <url>%2F2017%2F02%2F12%2F%E6%8A%80%E6%9C%AF%2Fhow-to-use-Linux%2F</url>
    <content type="text"><![CDATA[如何优雅的使用Linux 作为学习/开发环境使用，而不是娱乐。写这边文章是为了下次重装系统快速恢复，系统使用以为简洁、优雅、高效为目标。 1、选择 发行版选择Debian 之前一直使用Ubuntu，大概过那么一两个月就会出现啥问题，然后不得不重装系统，我严重Ubuntu16.04的桌面有bug，开2个软件窗口就卡的爹妈不认了。现在决定再也不用Ubuntu了，改用Debian8后稳定多了，图形桌面很少崩溃。关于centos和openSUSE，只在搭建服务器使用过，个人使用还是更习惯Debian系的。 桌面选择 Ubuntu的unity真tm丑，紫色的主题我也是忍了好久才适应下来。现在改用gnome3。 2、卸载liboffice事实上我连wps也没装，以前是装机必备，最后发现一共使用的次数不超过10次，office那套还是交给windows吧。文档记录使用txt和markdown。 完全卸载sudo apt-get purge libreoffice? 然后安装Typora markdown工具。参考官网, 1234567sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAE# 添加密钥和源在 /etc/apt/sources.list中添加一条'deb https://typora.io ./linux/'sudo apt-get update# 安装 typorasudo apt-get install typora 3、更改主目录文件夹名为英文打开终端，在终端中输入命令: $ export LANG=en_US $ xdg-user-dirs-gtk-update 在弹出的窗口中询问是否将目录转化为英文路径,勾上不再提示并同意更改， $ export LANG=zh_CN $ xdg-user-dirs-gtk-update 关闭终端,并注销或重启.下次进入系统.主目录的中文转英文就完成了~ 4、美化Debian刚装上以后真是丑啊，尤其是白底黑字的shell终端，让人抓狂。 这里美化仅仅是设置gtk主题，gnome-shell主题，改变terminal的颜色等，以美观实用为主，花里胡哨的东西请去windows娱乐。相关文件下载 安装gnome-tweak-tool apt-get install gnome-tweak-tool 这个工具不能用root运行。按下win，搜索tweak，点优化工具打开。 在扩展中启用user-themes，然后gnome shell主题才可用。下面几个插件推荐安装 安装gnome主题 主题可以去网上下载，我直接从Kali Linux上copy出来的。将themes文件夹内容放入/usr/share/themes/下即可，操作之前先备份。 安装gnome-shell主题 备份替换/usr/share/gnome-shell/内容。 字体 备份替换 /usr/share/fonts 内容。如果使用主题后，登陆桌面崩溃，则很有可能缺少字体。 改变壁纸和锁屏、登陆背景 壁纸锁屏可以在设置中修改。 登陆背景，替换gnome-shell主题内的图片，位置 /usr/share/gnome-shell/theme/KaliLogin.png 安装配置terminator 修改终端内的文字高亮，自动代码补全。shell环境变量配置文件有 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# /etc/profile 用户登陆时加载，全局配置，# /etc/bash.bashrc 打开shell时配置shell变量，如果~/.bashrc文件存在则会使用其配置，# ~/.bashrc，仅对该用户有效，user和root下的建议都改# .bashrc文件修改# If not running interactively, don't do anythingcase $- in *i*) ;; *) return;;esac# don't put duplicate lines or lines starting with space in the history.# See bash(1) for more optionsHISTCONTROL=ignoreboth# append to the history file, don't overwrite itshopt -s histappend# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)HISTSIZE=1000HISTFILESIZE=2000# check the window size after each command and, if necessary,# update the values of LINES and COLUMNS.shopt -s checkwinsize# If set, the pattern "**" used in a pathname expansion context will# match all files and zero or more directories and subdirectories.#shopt -s globstar# make less more friendly for non-text input files, see lesspipe(1)#[ -x /usr/bin/lesspipe ] &amp;&amp; eval "$(SHELL=/bin/sh lesspipe)"# set variable identifying the chroot you work in (used in the prompt below)if [ -z "$&#123;debian_chroot:-&#125;" ] &amp;&amp; [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot)fi# set a fancy prompt (non-color, unless we know we "want" color)case "$TERM" in xterm-color) color_prompt=yes;;esac# uncomment for a colored prompt, if the terminal has the capability; turned# off by default to not distract the user: the focus in a terminal window# should be on the output of commands, not on the promptforce_color_prompt=yesif [ -n "$force_color_prompt" ]; then if [ -x /usr/bin/tput ] &amp;&amp; tput setaf 1 &gt;&amp;/dev/null; then # We have color support; assume it's compliant with Ecma-48 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fifiif [ "$color_prompt" = yes ]; then PS1='$&#123;debian_chroot:+($debian_chroot)&#125;\[\033[01;31m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ 'else PS1='$&#123;debian_chroot:+($debian_chroot)&#125;\u@\h:\w\$ 'fiunset color_prompt force_color_prompt# If this is an xterm set the title to user@host:dircase "$TERM" inxterm*|rxvt*) PS1="\[\e]0;$&#123;debian_chroot:+($debian_chroot)&#125;\u@\h: \w\a\]$PS1" ;;*) ;;esac# enable color support of ls and also add handy aliasesif [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors &amp;&amp; eval "$(dircolors -b ~/.dircolors)" || eval "$(dircolors -b)" alias ls='ls --color=auto' #alias dir='dir --color=auto' #alias vdir='vdir --color=auto' #alias grep='grep --color=auto' #alias fgrep='fgrep --color=auto' #alias egrep='egrep --color=auto'fi# some more ls aliases#alias ll='ls -l'#alias la='ls -A'#alias l='ls -CF'# Alias definitions.# You may want to put all your additions into a separate file like# ~/.bash_aliases, instead of adding them here directly.# See /usr/share/doc/bash-doc/examples in the bash-doc package.if [ -f ~/.bash_aliases ]; then . ~/.bash_aliasesfi# enable programmable completion features (you don't need to enable# this, if it's already enabled in /etc/bash.bashrc and /etc/profile# sources /etc/bash.bashrc).if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi 然后右键终端，修改配置文件中背景为透明。背景透明简直太爽了，可以在敲命令时，透过终端看文档或浏览器，而不用将终端窗口挪来挪去。 ​ 5、python开发环境 安装pyenv 自动安装,不推荐 12$ curl -L https://raw.githubusercontent.com/yyuu/pyenv-installer/master/bin/pyenv-installer | bash$ pyenv update 手动git 123456789101112# 安装依赖# sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utilsgit clone git://github.com/yyuu/pyenv.git ~/.pyenv# 用户和root的我都改了,使用了绝对路径，没用$HOMEvim ~/.bashrcexport PYENV_ROOT="/home/syy/.pyenv"export PATH="/home/syy/.pyenv/bin:$PATH"eval "$(pyenv init -)"eval "$(pyenv virtualenv-init -)"source ~/.bashrc# exec $SHELL 安装virtualenv插件 1git clone https://github.com/yyuu/pyenv-virtualenv.git ~/.pyenv/plugins/pyenv-virtualenv 使用 12345678910# 安装# pyenv install --list# pyenv install -v XXX 安装位置/home/syy/.pyenv/versions/# 卸载# pyenv uninstall XXX# pyenv versions (查看所有版本)# pyenv global 3.3.5 切换版本# pyenv virtualenv 2.7.1 newvir 创建虚拟环境# pyenv activate newvir 切换进入newvir# pyenv deactivate 退出当前虚拟环境]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy笔记]]></title>
    <url>%2F2017%2F02%2F11%2F%E6%8A%80%E6%9C%AF%2Fuse-Numpy%2F</url>
    <content type="text"><![CDATA[Numpy数组入门1 Numpy数组对象Numpy数组 数组中的元素类型必须一致 数组的空间大小可以确定，可以运用向量化运算，底层c实现，速度快 数组下标0开始 1234567import numpy as npa = np.arange(5) # 创建0-4的一维数组b = np.array([0, 1, 2, 3, 4])print aprint a.dtype # a 的数据类型print a.dtype.itemsize # 对象数据类型占的字节数a.shape # a的形状，输出元组为每一维的长度 [0 1 2 3 4] int32 4 (5L,) 2 创建多维数组1234m = np.array([a, a, a]) # array的参数为list或list的嵌套listprint mprint m.dtypem.shape [[0 1 2 3 4] [0 1 2 3 4] [0 1 2 3 4]] int32 (3L, 5L) 3 选择numpy数组元素1print m[2, 4], m[0, 0], m[1] # a[m, n]确定第m行n列的元素 4 0 [0 1 2 3 4] 4 Numpy的数值类型123456print m.dtype.typen = np.array(m, dtype='uint32') # 定义数据类型为无符号32位print n.dtypeprint float(62) # 数据类型转换print np.sctypeDict.keys()[-10:] # 部分dtype的字符码print n.dtype.char # n的类型的字符码 &lt;type &apos;numpy.int32&apos;&gt; uint32 62.0 [&apos;a&apos;, &apos;short&apos;, &apos;e&apos;, &apos;i&apos;, &apos;clongfloat&apos;, &apos;m&apos;, &apos;Object0&apos;, &apos;int64&apos;, &apos;i2&apos;, &apos;int0&apos;] L 5 一维数组的切片与索引1print a[1:3], a[2::2], a[::-1] [1 2] [2 4] [4 3 2 1 0] 6 处理数组型状 x.shape x.shape = (m, n) x.rashape(s, m, n) x.resize(s, m, n) x.ravel() x.flatten() x.transpose() 1234567891011121314c = np.arange(24)print c.shapeb = c.reshape(2, 3, 4) # 生成2个3x4的新数组， c不变b.resize(2, 3, 4) # b会改变print be.shape = (3, 8) # 1维数组变成形状为3x8的数组print e, type(e)e = b.ravel() # 将多维数组变为一维数组，返回的是原数组的视图d = b.flatten() # 将多维数组变为一维数组，生成真实的数组，分配存储内存print b, d, eprint b.shape, d.shape, c.shape# 数组转置t = b.transpose()print t (24L,) [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] [[ 0 1 2 3 4 5 6 7] [ 8 9 10 11 12 13 14 15] [16 17 18 19 20 21 22 23]] &lt;type &apos;numpy.ndarray&apos;&gt; [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] (2L, 3L, 4L) (24L,) (24L,) [[[ 0 12] [ 4 16] [ 8 20]] [[ 1 13] [ 5 17] [ 9 21]] [[ 2 14] [ 6 18] [10 22]] [[ 3 15] [ 7 19] [11 23]]] 数组堆叠 x.vstack() x.dstack() x.hstack() x.column_stack() x.row_stack() x.concatenate() 12345678910111213141516171819202122232425262728a = np.arange(9).reshape(3, 3)b = a * 2print a, b# 水平叠加x1 = np.hstack((a, b))x2 = np.concatenate((a, b), axis=1) # axis=1,水平连接；axis=0，垂直连接;默认为0print x1print x2# 垂直叠加x3 = np.vstack((a, b))x4 = np.concatenate((a, b), axis=0)print x3, x4# 深度叠加， 沿着第三个坐标轴方向叠加一组数据x5 = np.dstack((a, b))print x5# 列式堆叠 堆叠一维数组时，按列码处理进行水平叠加，处理二维数组同水平叠加hstack()one1 = np.arange(3)one2 = one1 * 2x6 = np.column_stack((one1, one2))x7 = np.column_stack((a, b))print x6, x7# 行式堆叠 堆叠一维数组时，按行码进行垂直叠加，处理二维数组时同垂直叠加vstack()x8 = np.row_stack((one1, one2))x9 = np.row_stack((a, b))print x8, x9 [[0 1 2] [3 4 5] [6 7 8]] [[ 0 2 4] [ 6 8 10] [12 14 16]] [[ 0 1 2 0 2 4] [ 3 4 5 6 8 10] [ 6 7 8 12 14 16]] [[ 0 1 2 0 2 4] [ 3 4 5 6 8 10] [ 6 7 8 12 14 16]] [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 0 2 4] [ 6 8 10] [12 14 16]] [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 0 2 4] [ 6 8 10] [12 14 16]] [[[ 0 0] [ 1 2] [ 2 4]] [[ 3 6] [ 4 8] [ 5 10]] [[ 6 12] [ 7 14] [ 8 16]]] [[0 0] [1 2] [2 4]] [[ 0 1 2 0 2 4] [ 3 4 5 6 8 10] [ 6 7 8 12 14 16]] [[0 1 2] [0 2 4]] [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 0 2 4] [ 6 8 10] [12 14 16]] 数组拆分，横向，纵向，深度方向 hsplit() vsplit() dsplit() split() 123456789101112a = np.arange(9).reshape(3,3)print a# 沿着横向拆分成三个部分，也就是3列b = np.hsplit(a, 3)print bprint np.split(a,3, axis=1) # axis=1沿着横向拆分，axis=0，沿着纵向拆分，默认为0# 沿着纵向拆分print np.vsplit(a, 3)print np.split(a, 3, axis=0)# 沿着深度方向分割，要有一个三维数组,可以理解为面包片叠一起切条c = np.arange(36).reshape(3, 2, 6)print np.dsplit(c, 6) [[0 1 2] [3 4 5] [6 7 8]] [array([[0], [3], [6]]), array([[1], [4], [7]]), array([[2], [5], [8]])] [array([[0], [3], [6]]), array([[1], [4], [7]]), array([[2], [5], [8]])] [array([[0, 1, 2]]), array([[3, 4, 5]]), array([[6, 7, 8]])] [array([[0, 1, 2]]), array([[3, 4, 5]]), array([[6, 7, 8]])] [array([[[ 0], [ 6]], [[12], [18]], [[24], [30]]]), array([[[ 1], [ 7]], [[13], [19]], [[25], [31]]]), array([[[ 2], [ 8]], [[14], [20]], [[26], [32]]]), array([[[ 3], [ 9]], [[15], [21]], [[27], [33]]]), array([[[ 4], [10]], [[16], [22]], [[28], [34]]]), array([[[ 5], [11]], [[17], [23]], [[29], [35]]])] Numpy的属性 1234567891011print a.ndim, a.size, a.itemsize, a.nbytes, a.size * a.itemsize# 维度，元素个数，元素占字节数，数组占字节数print a.shape, a.dtypeprint a.T # a的转置数组，如果数组是一维，那么得到的是一个数组的视图b = np.array([1+2j, 3-4j])print b.dtype, b.real, b.imag # 复数的实部和虚部,数组的数据类型为复数a.flat # 把a转成一维数组的一个迭代器对象，可以用for遍历print a.flat[2:5]a.flat = 0 # a中所有数据都变成0print a 2 9 4 36 36 (3L, 3L) int32 [[0 0 0] [0 0 0] [0 0 0]] complex128 [ 1. 3.] [ 2. -4.] [0 0 0] [[0 0 0] [0 0 0] [0 0 0]] 数组的转换 tolist() astype() 12print a.tolist() # 把np数组转化为py的列表print a.astype('float') # 改变a的元素类型 [[0, 1, 2], [3, 4, 5], [6, 7, 8]] [[ 0. 1. 2.] [ 3. 4. 5.] [ 6. 7. 8.]] 7 创建数组的视图和拷贝 copy() view() 1234567891011121314151617import scipy.miscimport matplotlib.pyplot as plt%matplotlib inline# 显示图片face = scipy.misc.face()acopy = face.copy()aview = face.view()plt.subplot(221)plt.imshow(face)plt.subplot(222)plt.imshow(acopy)plt.subplot(223)plt.imshow(aview)aview.flat = 0plt.subplot(224)plt.imshow(aview)# 改变view视图后，原数组同样被改变，copy不受影响 8 花式索引花式索引，通过坐标x y的迭代器序列，索引一系列的元素 1234567face = scipy.misc.face()xmax = face.shape[0] ymax = face.shape[1]print xmax, ymaxface[range(xmax), range(xmax)] = 0 # 对角线上的像素点值改为0，sorry，非n*n方阵face[range(xmax-1, -1, -1), range(xmax)] = 0 # range(start, stop, step) 不包含stop，因此是n-1，-1plt.imshow(face) 768 1024 9 基于位置变量的索引方法1234567# 通过位置列表索引x = np.arange(xmax)y = np.arange(ymax)np.random.shuffle(x)np.random.shuffle(y)plt.imshow(face[np.ix_(x, y)]) # np.ix_() 输入n个1维数组，返回n个n维数组 # a[np.ix_([1,3],[2,5])] 的结果 [[a[1,2] a[1,5]], [a[3,2] a[3,5]]] 10 用bool型变量索引Numpy数组用布尔型变量索引数组 123456789x = (np.arange(xmax)%4 == 0)face = scipy.misc.face()f2 = face.copy()face[x, x] = 0 # 对角线上索引整除4的元素值设为0plt.subplot(121)plt.imshow(face)plt.subplot(122)f2[(face &gt; face.max()/4) &amp; (face &lt; 3*face.max()/4)] = 0 # 数组中值大于最大值1/4小于3/4的置为0plt.imshow(f2) 11 numpy数组的广播数组要和标量相乘的话，标量要根据数组的形状进行相应的扩展]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ipython使用技巧]]></title>
    <url>%2F2017%2F02%2F10%2F%E6%8A%80%E6%9C%AF%2Fuse-ipython-noteboke%2F</url>
    <content type="text"><![CDATA[ipython使用技巧 ipython –pylab pylab开关，ipython启动时自动导入np，sci，和matplotlib ipython –pylab inline 进入pylab模式，且图片内嵌到网页中 %logstart 保存会话，记录日志； %logstop关闭日志记录功能 !date !+shell命令，执行系统shell命令 c = !date %hist 显示历史命令 %hist -g print 在输入的历史命令中搜索print % xx magic function，单独一行时可以省略% jupyter notebook使用技巧 jupyter notebook 启动nootbook jupyter nbconvert –to markdown xxxx.ipynb ​ 快捷键命令模式 (按键 Esc 开启) Enter : 转入编辑模式 Shift-Enter : 运行本单元，选中下个单元 Ctrl-Enter : 运行本单元 Alt-Enter : 运行本单元，在其下插入新单元 Y : 单元转入代码状态 M :单元转入markdown状态 R : 单元转入raw状态 1 : 设定 1 级标题 2 : 设定 2 级标题 3 : 设定 3 级标题 4 : 设定 4 级标题 5 : 设定 5 级标题 6 : 设定 6 级标题 Up : 选中上方单元 K : 选中上方单元 Down : 选中下方单元 J : 选中下方单元 Shift-K : 扩大选中上方单元 Shift-J : 扩大选中下方单元 A : 在上方插入新单元 B : 在下方插入新单元 X : 剪切选中的单元 C : 复制选中的单元 Shift-V : 粘贴到上方单元 V : 粘贴到下方单元 Z : 恢复删除的最后一个单元 D,D : 删除选中的单元 Shift-M : 合并选中的单元 Ctrl-S : 文件存盘 S : 文件存盘 L : 转换行号 O : 转换输出 Shift-O : 转换输出滚动 Esc : 关闭页面 Q : 关闭页面 H : 显示快捷键帮助 I,I : 中断Notebook内核 0,0 : 重启Notebook内核 Shift : 忽略 Shift-Space : 向上滚动 Space : 向下滚动 编辑模式 ( Enter 键启动) Tab : 代码补全或缩进 Shift-Tab : 提示 Ctrl-] : 缩进 Ctrl-[ : 解除缩进 Ctrl-A : 全选 Ctrl-Z : 复原 Ctrl-Shift-Z : 再做 Ctrl-Y : 再做 Ctrl-Home : 跳到单元开头 Ctrl-Up : 跳到单元开头 Ctrl-End : 跳到单元末尾 Ctrl-Down : 跳到单元末尾 Ctrl-Left : 跳到左边一个字首 Ctrl-Right : 跳到右边一个字首 Ctrl-Backspace : 删除前面一个字 Ctrl-Delete : 删除后面一个字 Esc : 进入命令模式 Ctrl-M : 进入命令模式 Shift-Enter : 运行本单元，选中下一单元 Ctrl-Enter : 运行本单元 Alt-Enter : 运行本单元，在下面插入一单元 Ctrl-Shift– : 分割单元 Ctrl-Shift-Subtract : 分割单元 Ctrl-S : 文件存盘 Shift : 忽略 Up : 光标上移或转入上一单元 Down :光标下移或转入下一单元 ​]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Notebook</tag>
        <tag>Ipython</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy 入门学习]]></title>
    <url>%2F2016%2F12%2F19%2F%E6%8A%80%E6%9C%AF%2FScrapy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Scrapy学习笔记1、安装scrapy windows下安装 如果按照网上的方法自行安装scrapy，会出现各种错误，折腾很长时间。这里安装Anaconda2程序，会自带scrapy；如果默认没带的话，打开Anaconda Prompt，执行命令：conda listconda install scrapy安装完后，使用scrapy时如果提示openssl错误，去下载对应的openssl，安装即可。 Linux下安装 比较简单，# apt-get install python-dev# pip install scrapy 2、初步使用2.1、命令行工具scrapy12345678910111213141516171819202122232425# scrapy startproject my_spider # 创建scrapy项目.目录结构├── scrapy.cfg # 项目配置文件└── my_spider # 项目python模块, 之后将在此加入代码 ├── __init__.py ├── items.py ├── pipelines.py ├── settings.py └── spiders # 放置spider的目录 └── __init__.py# cd my_spider# scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt; # 使用模板生成spider文件，位于spiders文件夹下,默认使用basic模板# scrapy genspider -l 列出所有模板# scrapy genspider -d basic 查看basic模板# scrapy list 列出所有的爬虫# scrapy 显示scrapy可用的命令# scrapy &lt;command&gt; -h 查看command命令的详细信息# scrapy version -v Scrapy : 1.1.1 lxml : 3.6.0.0 libxml2 : 2.9.3 Twisted : 16.5.0 ...# scrapy bench 运行基准测试,可以测试scrapy是否正常# scrapy runspider spider_file.py 2.2、基本流程 startproject和genspider 首先分析要抓取的目标网站，在items.py中定义要抓取的数据对象 123456789101112import scrapyclass DmozItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() def __init__(self): # ？如果想初始化某些字段 scrapy.Item.__init__(self) self.title = '' def __str__(self): # ？默认控制台会输出所有的数据，包括抓取的页面源码 return 'crawling %s' % self.title 编写spider，解析网页(xpath/css/re)，提取数据到item对象 12345678910111213141516171819import scrapyfrom tutorial.items import DmozItemclass DmozSpider(scrapy.spider.Spider): name = "dmoz" #唯一标识，启动spider时即指定该名称 allowed_domains = ["dmoz.org"] start_urls = [ "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/", "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/" ] # 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。 def parse(self, response): for sel in response.xpath('//ul/li'): # 使用response.selector的css或xpath解析网页 item = DmozItem() # 类字典对象 item['title'] = sel.xpath('a/text()').extract() item['link'] = sel.xpath('a/@href').extract() item['desc'] = sel.xpath('text()').extract() yield item ## 返回item数据对象 ''' 每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给parse()函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。 ''' 保存数据 在执行scrapy crawl 命令时加入-o xxx.csv参数可以将item保存到csv文件中。也可在settings中设置FEED_URI和FEED_FORMAT 12FEED_URI = 'file:///tmp/export.csv'FEED_FORMAT = 'CSV' 将spider爬取到的item进一步处理并存储到数据库，需要在pipelines.py中实现自己的pipeline对象。 process_item(item, spider) # 数据经过pipeline时，都要调用该方法，最后返回一个item。 open_spider(spider) #当spider被开启时，这个方法被调用。 close_spider(spider) #当spider被关闭时，这个方法被调用，可以再爬虫关闭后进行相应的数据处理。 下面是存储数据到mongodb的一个实例： 12345678910111213141516171819202122232425class GushiciPipeline(object): collection_name = 'gushi' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get("MONGO_DATABASE", "items") ) # 在settings.py中定义 MONGO_URI = 'mongodb://localhost:27017' MONGO_DATABASE = 'test2' def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].insert(dict(item)) return item ​ 最后在settings.py中添加我们定义的pipeline：ITEM_PIPELINES = {&#39;blog_crawl.pipelines.SQLiteStorePipeline&#39;: 1} 1为优先级，越小级别越高 执行爬虫任务 12# scrapy list 查看所有的spider# scrapy crawl dmoz [-o xxx.csv|json|xml] 执行dmoz爬虫,-o指定结果输出到文件 通过python调用cmd命令执行爬虫 123# main.py 根目录下from scrapy import cmdlinecmdline.execute("scrapy crawl mindjet_muban".split()) 这样方便用ide调试 3、如何解析网页 scrapy.Selector有四个基本的方法： xpath()：参数是xpath表达式 css()：输入css表达式 extract()：序列化该节点为unicode字符串并返回list列表； re()：输入正则表达式，返回unicode字符串list列表；注意 正则效率低，可读性差 这四个方法返回的都是包含所有匹配节点的list列表，可嵌套使用css和xpath 3.1、xpathxpath w3school 12345678910111213141516171819202122/html/head/title: 选择HTML文档中 &lt;head&gt; 标签内的 &lt;title&gt; 元素/html/head/title/text(): 选择上面提到的 &lt;title&gt; 元素的文字//td: 选择所有的 &lt;td&gt; 元素//div[@class="mine"]: 选择所有具有 class="mine" 属性的 div 元素/x 是选取子节点x，//x 是递归选取全部的x.//x 从当前节点开始匹配，@是选择属性----- 路径表达式 -----/div/p[1] #选择第一个p元素/div/p[last()] #选择最后一个p元素/div/p[last()-2] #选择倒数第三个p元素/div/p[position()&lt;3] #选择前2个元素/div/p[x&gt;35.00]/name #选择x属性大于35.00的p元素下的name元素/div/p[x&gt;35.00]/@aaa #匹配所有属性aaa的值/div[@class] #选择含有class属性的div//div[@class="wp-pagenavi"]/a[not(@title)] #不含title属性的a标签/div[@class='lang'] #选择class值为lang的div/node() #根元素下所有的节点（包括文本节点，注释节点等）/text() #查找文档根节点下的所有文本节点----- 通配符 -----* #匹配任意元素@* #匹配任意属性exp1 | exp2 #返回2个表达式匹配结果的合集 3.2、css123456.con1 #选择class为con1的全部标签div.con1 #选择class为con1的全部div标签div p #递归选择div下的全部p标签div &gt; p #选择div的全部子标签pa::text #选择a标签的文本img::attr(href) #选择img标签的href属性的值 3.3、re返回unicode字符串的list，该对象无法继续使用css或xpath 3.4、extractresponse.xpath(&quot;//div[@class=&#39;son5&#39;]/p/a&quot;).css(&quot;::attr(href)&quot;).extract()[0] 该语句执行后得到的结果是字符串‘/view_12390.aspx’ 4、多级页面的爬取 主要是spider中的回调函数，下面分析爬古诗词网站为例，爬取诗文的标题、作者、朝代、url、原文，翻译注释 4.1、爬取前分析 [a] start_url：http://so.gushiwen.org/type.aspx [b] 不同朝代的诗文列表url：http://so.gushiwen.org/type.aspx?p=1&amp;c=先秦 ，参数p是页码，c是朝代，一共12个朝代，页码最多的有200页 [c] 诗文url：http://so.gushiwen.org/view_1.aspx [d] 翻译和注释url：http://so.gushiwen.org/fanyi_1.aspx 最后，abcd是递进关系，我们需要的item在bcd中都能找到一部分信息。注意b有很多页要抓，思路是“下一页”url。 4.2、代码示例 basic爬虫继承scrapy.Spider, crawl爬虫继承scrapy.CrawlSpider 基于basic模板的爬虫 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# -*- coding: utf-8 -*-import scrapyfrom gushici.items import GushiciItemclass TangsiSpider(scrapy.Spider): name = "tangsi" allowed_domains = ["gushiwen.org"] start_urls = ( 'http://so.gushiwen.org/type.aspx', ) def parse(self, response): # 获取不同朝代诗词的列表入口,利用meta字典传参到下一个回调函数，并调用get_lists函数 destinies = response.css("div.cont&gt;a") for destiny in destinies: # item = GushiciItem() # item['destiny'] = destiny.css("::text").extract() url = destiny.css("::attr(href)").extract()[0] destiny_url = response.urljoin(url) # 此处是分类列表的url # item['url'] = full_url destiny_text = destiny.css("::text").extract()[0] yield scrapy.Request(destiny_url, meta=&#123;"destiny": destiny_text&#125;, callback=self.get_lists) def get_lists(self, response): # 爬当前页的所有诗词的url，title，作者，并进入下一级调爬诗文内容；爬到“下一页”url后交给get_lists处理 destiny = response.meta['destiny'] divs = response.css("div.sons") for div in divs: title = div.css('p &gt; a[target]::text').extract()[0] full_url = response.urljoin(div.css('p &gt; a[target]::attr(href)').extract()[0]) author = div.xpath("p[2]/text()").extract()[0] # score = response.re(u'(\d+\.\d+)')[0] yield scrapy.Request(full_url, meta=&#123;'title': title, 'url': full_url, 'author': author, 'destiny': destiny&#125;, callback=self.get_contents) # 递归爬下一页的诗词url，入口是网页中的下一页标签url next_urls = response.xpath('//a[@style="width:60px;"]').css("::attr(href)").extract() for next_url in next_urls: next_page = response.urljoin(next_url) yield scrapy.Request(next_page, meta=&#123;"destiny": destiny&#125;, callback=self.get_lists) # 获取诗词的评分，文本，翻译页的url，接收父级页面解析出来的标题，作者，朝代，创建item def get_contents(self, response): item = GushiciItem() item['title'] = response.meta['title'] item['url'] = response.meta['url'] item['destiny'] = response.meta['destiny'] item['author'] = response.meta['author'] try: item['score'] = response.xpath('//div[@class="pingfen"]//div[@class="line1"]/span/text()').extract()[0] except Exception: item['score'] = u'评分不足10人' ps = response.xpath("//div[@class='son2']/p") text = "" if len(ps) &gt; 3: for p in ps[3:]: text += ''.join(p.css("::text").extract()) + '\n' else: text += ''.join(response.xpath("//div[@class='son2']/text()").extract()).replace('\n', '') item['text'] = text try: fanyi_url = response.xpath("//div[@class='son5']/p/a").css("::attr(href)").extract()[0] fanyi_url = response.urljoin(fanyi_url) yield scrapy.Request(fanyi_url, callback=self.get_fanyi, meta=&#123;'item': item&#125;) except Exception: item['translate'] = '' yield item # 获取翻译文本 def get_fanyi(self, response): # 前面是利用meta字典传递已抓到的内容，这里是将item对象装入meta传递 item = response.meta['item'] ps = response.xpath('//div[@class="shangxicont"]/p') fanyi = '' for p in ps[:-1]: fanyi += ''.join(p.css("::text").extract()) + '\n' item['translate'] = fanyi yield item ​ 网站没有反扒措施，但这个爬虫最终抓到7795条文章，像宋词至少有1万+，但在该分类下200页后的内容就是空的，也就是说web只给我们提供了200页的宋词。 基于crawl模板的爬虫 crawl类的爬虫更强大一点，Rule方法可以根据正则匹配所有满足条件的url，并调用相应回调函数处理 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom gushici.items import GushiciItemclass SongciSpider(CrawlSpider): name = 'songci' allowed_domains = ['so.gushiwen.org'] start_urls = ['http://so.gushiwen.org/type.aspx'] rules = ( # 这是分类页的url，全部加入到抓取列表中 Rule(LinkExtractor(allow=r"http://so\.gushiwen\.org/type\.aspx\?p=\d+&amp;c=.+?")), # 这是诗文详情页url，全部调用parse_item解析 Rule(LinkExtractor(allow=r'http://so\.gushiwen\.org/view_\d+\.aspx'), callback='parse_item', follow=True), ) def parse_item(self, response): item = GushiciItem() try: item['title'] = response.xpath('//h1/text()').extract()[0] item['url'] = response.url item['destiny'] = response.xpath('//div[@class="son2"]/p[1]//text()').extract()[-1] item['author'] = response.xpath('//div[@class="son2"]/p[2]//text()').extract()[-1] except Exception: pass try: item['score'] = response.xpath('//div[@class="pingfen"]//div[@class="line1"]/span/text()').extract()[0] except Exception: item['score'] = u'评分不足10人' ps = response.xpath("//div[@class='son2']/p") text = "" if len(ps) &gt; 3: for p in ps[3:]: text += ''.join(p.css("::text").extract()) + '\n' else: text += ''.join(response.xpath("//div[@class='son2']/text()").extract()).replace('\n', '') item['text'] = text try: fanyi_url = response.xpath("//div[@class='son5']/p/a").css("::attr(href)").extract()[0] fanyi_url = response.urljoin(fanyi_url) yield scrapy.Request(fanyi_url, callback=self.get_fanyi, meta=&#123;'item': item&#125;) except Exception: item['translate'] = '' yield item # 获取翻译文本 def get_fanyi(self, response): item = response.meta['item'] ps = response.xpath('//div[@class="shangxicont"]/p') fanyi = '' for p in ps[:-1]: fanyi += ''.join(p.css("::text").extract()) + '\n' item['translate'] = fanyi yield item basic爬虫是循环查找下一页的url来抓取，这里crawl爬虫我们直接用正则匹配列表页url和诗文详情页url，另外“标题文本朝代作者”也都改为在详情页获取了，“翻译”仍需从下一级网页获取。crawl爬了8700条数据，我在settings设置了download_delay为0.5秒，爬取效果是100秒约90条数据，一共用了2h48m。 pipelines的Dropitem 我先把basic抓到的放入mongodb了，再用crawl爬虫时，我改写了pipelines，如果item在数据库中，就丢弃，否则存入monggodb，避免数据重复。 123456789from scrapy.exceptions import DropItemdef process_item(self, item, spider): data = self.db[self.collection_name].find(&#123;&#125;, &#123;'_id': 0, 'url': 1&#125;) url_finished = set(x['url'] for x in data) # 获取已爬url的集合 if item['url'] in url_finished: raise DropItem("%s has been crawled!" % item) else: self.db[self.collection_name].insert(dict(item)) return item ​ 5、Scrapy框架解读 Scrapy基于Twisted异步网络库处理网络通信 5.1、整体框架 各组件及功能： Scrapy Engine: 用来处理整个系统的数据流处理, 触发事务(框架核心) Scheduler: 调度器，接收引擎发过来的Request, 压入队列中, 由引擎调度，将Request发送给Downloader。 可以理解为要爬取URL的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址 Downloader: 根据Scheduler给出的Request，去下载网页内容, 并将网页内容返回给Spiders解析。Scrapy下载器是基于twisted的异步模型，因此网页下载的结果并不是有序的。 Spiders: 爬虫有2个作用：从网页中提取自己需要的信息实体(Item)，并将item发送到item pipeline进行后续处理；从中网页提取出进一步爬取的URL,并发送给Scheduler。 Pipeline: 负责处理爬虫从网页中抽取的实体：数据清洗(整理、查重、验证有效性)、数据保存等。 Downloader Middlewares: 位于Scrapy引擎和下载器之间的中间件，主要是处理Scrapy引擎与下载器之间的请求及响应。可以在此处设置请求的代理、cookie？ Spider Middlewares: 介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。 Scheduler Middewares: 介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。 ​ 5.2、主要对象 Spider Selector Items &amp; Item Pipeline &amp; Feed exports Requests Responses Logging 日志模块，使用python内置的logging模块，logging.log(logging.WARNING, &quot;This is a warning&quot;),可以在setting中配置logging属性。 同时Spider对象含有logger对象，spider内也可使用self.logger.info(&#39;msg&#39;) self.logger.warning(&#39;msg&#39;) statscollectors MailSender 12345678910111213141516from scrapy.mail import MailSenderfrom xxx import settings#------ spider文件，在spider对象close时发邮件，重写close方法 def close(self, spider, reason): mailer = MailSender.from_settings(settings) mailer.send(to=['xx@xx.com'], subject=u'爬虫结束', body='test!'+str(reason))#-----settings文件# 下面是在settings文件中配置邮件服务器# 发送邮件MAIL_FROM = 'xxx@xxx.com' #邮件中的fromMAIL_HOST = 'smtp.xxx.com' #邮件服务器地址，端口MAIL_PORT = 25MAIL_USER = '登录名'MAIL_PASS = '密码'MAIL_TLS = False # 默认邮件服务器不开启TLS SSL安全连接MAIL_SSL = False 6、下载图片和文件6.1、激活Media pipeline在settings中设置如下： 1234567891011121314# 启用pipelineITEM_PIPELINES = &#123;'scrapy.pipelines.images.ImagesPipeline': 1, 'scrapy.pipelines.files.FilesPipeline': 1&#125;# 1.2版默认存放在./full，路径是相对.cfg配置文件# 即使指定存储位置，也会在该目录下生成full子文件夹IMAGES_STORE = '/path/to/valid/dir'# 定义图片url的item域IMAGES_URLS_FIELD = 'field_name_for_your_images_urls'# 定义图片下载结果存放的item域IMAGES_RESULT_FIELD = 'field_name_for_your_processed_images'# 文件和图片定义类似FILES_STORE = '/path/to/valid/dir'FILES_URLS_FIELD = 'field_name_for_your_files_urls'FILES_RESULT_FIELD = 'field_name_for_your_processed_files' 6.2、items中定义123import scrapyimage_url = scrapy.Field()download_success = scrapy.Field() 注意：image_url 的接收对象是image url的列表。 download_success:包含字典的列表，由ImagePipeline自动填充，包含image的url，本地存储位置，文件校验值： [{&#39;url&#39;: &#39;http://img.tupianzj.com/uploads/allimg/161226/9-161226154443.jpg&#39;, &#39;path&#39;: &#39;full/46ae5ec4f4c0905e48a41d569abe8e1aaa832f29.jpg&#39;, &#39;checksum&#39;: &#39;c35693197b1217ac8cafd7f7f318ef33&#39;}] 6.3、spiders编写 只需要给item[‘image_url’]传url的列表，并将item返回 这是爬图片之家QQ表情图的一个例子。 123456789101112131415161718192021222324252627282930313233# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom gaoxiao_pic.items import GaoxiaoPicItemclass FunPicSpider(CrawlSpider): name = 'fun_pic' allowed_domains = ['tupianzj.com'] start_urls = ['http://www.tupianzj.com/gaoxiao/biaoqing/', ] rules = ( Rule(LinkExtractor(allow=r'http://www\.tupianzj\.com/gaoxiao/gx/biaoqing\.html')), Rule(LinkExtractor(allow=r'http://www\.tupianzj\.com/gaoxiao/biaoqing/list[_\d]+\.html')), Rule(LinkExtractor(allow=r'http://img\.tupianzj\.com/uploads/allimg/\d+/.+\.(?:gif|jpg|png|bmp)'), callback='parse_item'), Rule(LinkExtractor(allow=r'http://www\.tupianzj\.com/gaoxiao/biaoqing/\d+/[_\d]+.html'), callback='parse_item', follow=True), ) def parse_item(self, response): i = GaoxiaoPicItem() if str(response.url).split('.')[-1].lower() in ['jpg', 'gif', 'png', 'jpeg', 'bmp']: i['image_url'] = [response.url] else: try: url = response.xpath("//div[@id='bigpic']/a/img/@src").extract()[0] i['image_url'] = [response.urljoin(url)] # 这里每个页面只有一张图片, print i['image_url'] except Exception, e: print e.message return i items.py: 123class GaoxiaoPicItem(scrapy.Item): image_url = scrapy.Field() download_success = scrapy.Field() settings.py: 12345678910# 启用pipelineITEM_PIPELINES = &#123;'scrapy.pipelines.images.ImagesPipeline': 1, # 'scrapy.pipelines.files.FilesPipeline': 1 &#125;# 定义图片存储位置（必须）IMAGES_STORE = '.'# 定义图片url的item域IMAGES_URLS_FIELD = 'image_url'# 存储下载结果IMAGES_RESULT_FIELD = 'download_success' 7、设置headers7.1、设置默认headers1234567891011# settings中可以设置浏览器默认headers，DEFAULT_REQUEST_HEADERS = &#123; 'accept': 'image/webp,/;q=0.8', 'accept-language': 'zh-CN,zh;q=0.8', 'referer': 'https://www.baidu.com/', 'user-agent': 'Mozilla/5.0 (Windows NT 6.3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36',&#125; 7.2、为每个download 分配随机UA和代理 UA 1234567891011121314# --------------settings.py--------------------DOWNLOADER_MIDDLEWARES = &#123; 'xxx.middlewares.RandomUserAgentMiddleware': 400, 'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,&#125;# ------创建middlewares.py ----------------------import fakerf = faker.Factory().create()# user_agent = f.user_agent() # 随机生成的浏览器UA，都次调用返回结果都不同class RandomUserAgentMiddleware(object): def process_request(self, request, spider): request.headers.setdefault('User-Agent', f.user_agent()) #log.msg('&gt;&gt;&gt;&gt; UA %s'%request.headers) 代理 123456789101112131415161718192021222324252627282930# -------------------settings.py--------# 启用代理DOWNLOADER_MIDDLEWARES = &#123; 'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 110, 'xxx.middlewares.ProxyMiddleware': 100,&#125;# 在setting中写死了代理列表，实际环境在需要代理时可以从数据库代理池获取PROXY_LIST = [ &#123;'ip_port': '111.11.228.75:80', 'user_pass': ''&#125;, &#123;'ip_port': '120.198.243.22:80', 'user_pass': ''&#125;, &#123;'ip_port': '111.8.60.9:8123', 'user_pass': ''&#125;, &#123;'ip_port': '101.71.27.120:80', 'user_pass': ''&#125;, &#123;'ip_port': '122.96.59.104:80', 'user_pass': ''&#125;, &#123;'ip_port': '122.224.249.122:8088', 'user_pass': ''&#125;,]# -----创建middlewares.py, 配置代理-----------------from xxx.settings import PROXY_LISTimport randomimport base64class ProxyMiddleware(object): def process_request(self, request, spider): proxy = random.choice(PROXIES_LIST) if proxy['user_pass'] is not None: request.meta['proxy'] = "http://%s" % proxy['ip_port'] encoded_user_pass = base64.encodestring(proxy['user_pass']) request.headers['Proxy-Authorization'] = 'Basic ' + encoded_user_pass else: request.meta['proxy'] = "http://%s" % proxy['ip_port'] 8、登陆Post和Cookie8.1、POST表单登陆豆瓣登陆页面有验证码比无验证时post表单提交的数据多了captcha-id和captcha-solution。有验证码时下载图片，手动输入。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom scrapy.http import Request, FormRequest, HtmlResponsefrom douban.items import DoubanItemimport faker, requests, osfrom PIL import Imageclass DoubanSpiderSpider(CrawlSpider): def _requests_to_follow(self, response): """重写加入cookiejar的更新""" if not isinstance(response, HtmlResponse): return seen = set() for n, rule in enumerate(self._rules): links = [l for l in rule.link_extractor.extract_links(response) if l not in seen] if links and rule.process_links: links = rule.process_links(links) for link in links: seen.add(link) r = Request(url=link.url, callback=self._response_downloaded) # 下面这句是我重写的 r.meta.update(rule=n, link_text=link.text, cookiejar=response.meta['cookiejar']) yield rule.process_request(r) name = 'douban_spider' allowed_domains = ['douban.com'] start_urls = ['https://www.douban.com/'] rules = ( Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True), ) f = faker.Factory().create() user_agent = f.user_agent() # 随机生成的浏览器UA，都次调用返回结果都不同 headers = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Cache-Control': 'max-age=0', 'Referer': 'https://www.douban.com', 'User-Agent': user_agent, # 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36', 'Accept-Encoding': 'gzip, deflate, sdch, br', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Host': 'accounts.douban.com', 'Connection': 'keep-alive' &#125; your_email = '1XXX' your_password = 'XXXX' postdata = &#123;'source': 'None', 'redir': 'https://www.douban.com/', 'form_email': your_email, 'form_password': your_password, # 'captcha-solution': vcode, # 'captcha-id': captcha, 'login': '登录' &#125; def start_requests(self): # 访问登录页面 # Scrapy通过使用meta['cookiejar']来支持单spider追踪多cookie session。默认情况下其使用一个cookie jar(session)，不过可以传递一个标示符来使用多个。如meta=&#123;'cookiejar': 1&#125;这句，后面那个1就是标示符。 return [Request(url='https://www.douban.com/accounts/login', headers=self.headers, meta=&#123;'cookiejar': 1&#125;, callback=self.post_login)] def post_login(self, response): print 'Preparing login====', response.url # 如果登陆要验证码，则post数据中加入验证码的id和值 if response.body.find('captcha_image') &gt; 0: captcha_url = response.xpath('//img[@id="captcha_image"]/@src').extract()[0] print u'验证码的URL：%s' % captcha_url with open('v.jpg', 'wb') as f: f.write(requests.get(captcha_url, verify=False).content) with open('v.jpg', 'rb') as f: image = Image.open(f) image.show() self.postdata['captcha-solution'] = raw_input('请输入图片验证码:\n') os.remove('v.jpg') self.postdata['captcha-id'] = response.xpath('//input[@name="captcha-id"]/@value').extract()[0] print self.postdata return [FormRequest.from_response(response, headers=self.headers, meta=&#123;'cookiejar': response.meta['cookiejar']&#125;, formdata=self.postdata, callback=self.after_login, )] def after_login(self, response): # 登陆之后,访问的网页内容应该会包含用户信息 self.headers['Host'] = 'www.douban.com' print response.body with open('a.txt', 'w') as f: f.write(response.body) item = DoubanItem() item['main_page'] = response.body return Request('https://www.douban.com/doumail/', headers=self.headers, meta=&#123;'cookiejar': response.meta['cookiejar'], 'item': item&#125;, callback=self.openfile) def openfile(self, response): item['doumail_page'] = response.body return item 8.2、使用Cookie登陆豆瓣使用浏览器或requests模拟登陆，然后将cookie传进scrapy的spider。 由于每次从浏览器复制出来的cookies或headers都是字符串形式，手工改成字典太累了，写个小程序： 1234567891011121314151617181920212223242526272829# 从chrome浏览器的Request Header中，点击view source，然后复制header字符串headers_str = '''Host: www.douban.comConnection: keep-aliveCache-Control: max-age=0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Upgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36Referer: https://www.douban.com/people/108243932/Accept-Encoding: gzip, deflate, sdch, brAccept-Language: zh-CN,zh;q=0.8Cookie: viewed="1770782"; bid=SrGB-eJRKEE; gr_user_id=a37ff9ef-33cc-4de7-971b-c161f747a77b; ll="118254"; ps=y; dbcl2="108243932:kVKKGRcAZVg"; _ga=GA1.2.1495924407.1465701053; ck=LRet; _pk_ref.100001.8cb4=%5B%22%22%2C%22%22%2C1483063011%2C%22https%3A%2F%2Faccounts.douban.com%2Fregister%22%5D; __utmt=1; ap=1; push_noty_num=0; push_doumail_num=0; _pk_id.100001.8cb4=438cca0c1cd12666.1482926660.4.1483063910.1483060769.; _pk_ses.100001.8cb4=*; __utma=30149280.1495924407.1465701053.1483060678.1483063011.6; __utmb=30149280.24.9.1483063909873; __utmc=30149280; __utmz=30149280.1483021458.4.3.utmcsr=accounts.douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/register; __utmv=30149280.10824; _vwo_uuid_v2=02A1FB5802A3379A6C48F734CB328D35|33e02434e1d90a57e4c63094703cde0f'''headers = &#123;&#125;for i in headers_str.split('\n'): j = i.replace(': ', ':', 1).split(':', 1) headers[j[0]] = j[1]if 'Cookie' in headers: cookies_str = headers['Cookie']else: # 把cookie字符串从浏览器copy进来 cookies_str = ''' '''cookies = &#123;&#125;for i in cookies_str.split(';'): q = i.split('=') key = q[0].replace(' ', '', 1) value = q[1].replace(' ', '', 1) cookies[key] = value# print cookiesprint headers.pop('Cookie')print headers 使用cookie登陆豆瓣： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Ruleclass DoubanCookieSpider(CrawlSpider): name = 'douban_cookie' allowed_domains = ['douban.com'] start_urls = ['https://www.douban.com/mine/', 'https://www.douban.com/doumail/', 'https://www.douban.com/people/108243932/', 'https://www.douban.com/mine/orders/' ] cookies = &#123;'ck': 'LRet', 'ps': 'y', '__utmz': '30149280.1483021458.4.3.utmcsr', '__utmv': '30149280.10824', 'push_doumail_num': '0', '__utmt': '1', 'bid': 'SrGB-eJRKEE', 'push_noty_num': '0', '_ga': 'GA1.2.1495924407.1465701053', '_pk_ref.100001.8cb4': '%5B%22%22%2C%22%22%2C1483063011%2C%22https%3A%2F%2Faccounts.douban.com%2Fregister%22%5D', '_vwo_uuid_v2': '02A1FB5802A3379A6C48F734CB328D35|33e02434e1d90a57e4c63094703cde0f', 'ap': '1', 'dbcl2': '"108243932:kVKKGRcAZVg"', '_pk_id.100001.8cb4': '438cca0c1cd12666.1482926660.4.1483063910.1483060769.', '_pk_ses.100001.8cb4': '*', 'gr_user_id': 'a37ff9ef-33cc-4de7-971b-c161f747a77b', '__utma': '30149280.1495924407.1465701053.1483060678.1483063011.6', '__utmb': '30149280.24.9.1483063909873', '__utmc': '30149280', 'll': '"118254"', 'viewed': '"1770782"'&#125; headers = &#123;'Accept-Language': 'zh-CN,zh;q=0.8', 'Accept-Encoding': 'gzip, deflate, sdch, br', 'Connection': 'keep-alive', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36', 'Host': 'www.douban.com', 'Referer': 'https://www.douban.com/', 'Cache-Control': 'max-age=0', 'Upgrade-Insecure-Requests': '1'&#125; def start_requests(self): return [scrapy.Request('https://www.douban.com/', headers=self.headers, cookies=self.cookies, meta=&#123;'cookiejar': 1&#125;, callback=self.after_set_cookie)] def after_set_cookie(self, response): req = [] self.headers['Referer'] = response.url for url in self.start_urls: req.append(scrapy.Request(url=url, headers=self.headers, meta=&#123;'cookiejar': response.meta['cookiejar']&#125;, callback=self.parse_item)) return req def parse_item(self, response): print response.url filename = response.url.split('/')[-2] + '.html' with open(filename, 'w') as f: f.write(response.body) # 保存的网页中应该有登陆账号后的个人信息 9、JS 、XHR分析 主要是分析js请求的url。挖个坑，暂时不打算填。 不好解决的话，还是用selenium + phantomjs吧 10、Scrapy Redis分布式爬虫10.1、安装redis windows 下载地址：https://github.com/rgl/redis/downloads 安装完成后， 运行redis服务器的命令：安装目录下的redis-server.exe 运行redis客户端的命令：安装目录下的redis-cli.exe Linux 1234567$sudo apt-get install redis-server# 启动 Redis$ redis-server &amp;# 或者$ service redis-server start# 查看 redis 是否启动？$ redis-cli 12345# vi /etc/redis/redis.conf#注释bind#bind 127.0.0.1# 如果要设置密码，取消注释requirepassrequirepass mypasswd 修改配置文件后，要重启服务生效。有密码的连接方式，redis-cli -a mypasswd -h 172.16.28.24 -p 6379 还是单开一篇笔记来写吧！ 11、记录一些error11.1、url相关 ValueError: Missing scheme in request url: h 这种问题一般是url出错了，要以http开头，如果是从网页解析的url，可以用 response.urljoin(relative_url) 生成绝对路径。 如果在下载图片中出错，是因为image_field必须是图片url的列表，只存了一个url字符串的话，scrapy遍历url时取出的第一个对象是http url的第一个字符h，故报错h是非法url。 太坑了，花了半个小时才发现原因。 注意twisted库的版本，在conda中先装twisted再装scrapy]]></content>
      <categories>
        <category>技术</category>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 配置SNMP服务]]></title>
    <url>%2F2016%2F12%2F14%2F%E6%8A%80%E6%9C%AF%2Flinux-snmp-server%2F</url>
    <content type="text"><![CDATA[Linux服务器配置snmp服务安装snmp server 安装snmp 1apt-get install snmp snmpd 安装mib库 1apt-get install snmp-mibs-downloader 参考debian-wiki-snmp 查看默认配置文件 12cat /etc/snmp/snmpd.conf cat /etc/snmp/snmp.conf 启动SNMP 12# service snmpd start# snmpd -C -c /etc/snmp/myconfig.conf ### -C不适用默认配置文件 -c指定自定义配置文件 如有需要，安装net-snmp 源码下载地址 编译 12345# tar -zxvf net-snmp-5.7.3.tar.gz ### 解压源码# ./configure --help ### 查看编译配置# ./configure ### 开始编译# make# make install 如果编译报错lperl，使用 apt-get install libperl-dev 使用net-snmp工具 查询到主机CPU空闲率为99% # snmpwalk -v 2c -c public localhost 1.3.6.1.4.1.2021.11.11.0 UCD-SNMP-MIB::ssCpuIdle.0 = INTEGER: 99 修改snmp配置文件/etc/snmp/snmpd.conf 配置允许网络访问,找到【AGENT BEHAVIOUR】，注释掉`agentAddress udp:127.0.0.1：161`,添加一行`agentAddress udp:161` 选择SNMPv2C协议版本,找到【ACTIVE MONITORING】，注释掉`trapsink localhost public`，添加`trap2sink localhost public` 设置访问权限,找到【ACCESS CONTROL】,注释掉`rocommunity public default -V systemonly`，添加`rocommunity public default `,允许所有访问请求。 /etc/snmp/snmp.conf 注释掉开头第一行 开启防火墙161端口 查看防火墙规则 iptables –L –n 添加规则 iptables -I INPUT -p udp --dport 161 -j ACCEPT 保存生效 iptables-save Debian iptables -save ubuntu 常用Linux监控OID 系统信息 网络接口 CPU及负载 内存及磁盘 System Group sysDescr 1.3.6.1.2.1.1.1 sysObjectID 1.3.6.1.2.1.1.2 sysUpTime 1.3.6.1.2.1.1.3 sysContact 1.3.6.1.2.1.1.4 sysName 1.3.6.1.2.1.1.5 sysLocation 1.3.6.1.2.1.1.6 sysServices 1.3.6.1.2.1.1.7 Interfaces Group ifNumber 1.3.6.1.2.1.2.1 ifTable 1.3.6.1.2.1.2.2 ifEntry 1.3.6.1.2.1.2.2.1 ifIndex 1.3.6.1.2.1.2.2.1.1 ifDescr 1.3.6.1.2.1.2.2.1.2 ifType 1.3.6.1.2.1.2.2.1.3 ifMtu 1.3.6.1.2.1.2.2.1.4 ifSpeed 1.3.6.1.2.1.2.2.1.5 ifPhysAddress 1.3.6.1.2.1.2.2.1.6 ifAdminStatus 1.3.6.1.2.1.2.2.1.7 ifOperStatus 1.3.6.1.2.1.2.2.1.8 ifLastChange 1.3.6.1.2.1.2.2.1.9 ifInOctets 1.3.6.1.2.1.2.2.1.10 ifInUcastPkts 1.3.6.1.2.1.2.2.1.11 ifInNUcastPkts 1.3.6.1.2.1.2.2.1.12 ifInDiscards 1.3.6.1.2.1.2.2.1.13 ifInErrors 1.3.6.1.2.1.2.2.1.14 ifInUnknownProtos 1.3.6.1.2.1.2.2.1.15 ifOutOctets 1.3.6.1.2.1.2.2.1.16 ifOutUcastPkts 1.3.6.1.2.1.2.2.1.17 ifOutNUcastPkts 1.3.6.1.2.1.2.2.1.18 ifOutDiscards 1.3.6.1.2.1.2.2.1.19 ifOutErrors 1.3.6.1.2.1.2.2.1.20 ifOutQLen 1.3.6.1.2.1.2.2.1.21 ifSpecific 1.3.6.1.2.1.2.2.1.22 IP Group ipForwarding 1.3.6.1.2.1.4.1 ipDefaultTTL 1.3.6.1.2.1.4.2 ipInReceives 1.3.6.1.2.1.4.3 ipInHdrErrors 1.3.6.1.2.1.4.4 ipInAddrErrors 1.3.6.1.2.1.4.5 ipForwDatagrams 1.3.6.1.2.1.4.6 ipInUnknownProtos 1.3.6.1.2.1.4.7 ipInDiscards 1.3.6.1.2.1.4.8 ipInDelivers 1.3.6.1.2.1.4.9 ipOutRequests 1.3.6.1.2.1.4.10 ipOutDiscards 1.3.6.1.2.1.4.11 ipOutNoRoutes 1.3.6.1.2.1.4.12 ipReasmTimeout 1.3.6.1.2.1.4.13 ipReasmReqds 1.3.6.1.2.1.4.14 ipReasmOKs 1.3.6.1.2.1.4.15 ipReasmFails 1.3.6.1.2.1.4.16 ipFragsOKs 1.3.6.1.2.1.4.17 ipFragsFails 1.3.6.1.2.1.4.18 ipFragCreates 1.3.6.1.2.1.4.19 ipAddrTable 1.3.6.1.2.1.4.20 ipAddrEntry 1.3.6.1.2.1.4.20.1 ipAdEntAddr 1.3.6.1.2.1.4.20.1.1 ipAdEntIfIndex 1.3.6.1.2.1.4.20.1.2 ipAdEntNetMask 1.3.6.1.2.1.4.20.1.3 ipAdEntBcastAddr 1.3.6.1.2.1.4.20.1.4 ipAdEntReasmMaxSize 1.3.6.1.2.1.4.20.1.5 ICMP Group icmpInMsgs 1.3.6.1.2.1.5.1 icmpInErrors 1.3.6.1.2.1.5.2 icmpInDestUnreachs 1.3.6.1.2.1.5.3 icmpInTimeExcds 1.3.6.1.2.1.5.4 icmpInParmProbs 1.3.6.1.2.1.5.5 icmpInSrcQuenchs 1.3.6.1.2.1.5.6 icmpInRedirects 1.3.6.1.2.1.5.7 icmpInEchos 1.3.6.1.2.1.5.8 icmpInEchoReps 1.3.6.1.2.1.5.9 icmpInTimestamps 1.3.6.1.2.1.5.10 icmpInTimestampReps 1.3.6.1.2.1.5.11 icmpInAddrMasks 1.3.6.1.2.1.5.12 icmpInAddrMaskReps 1.3.6.1.2.1.5.13 icmpOutMsgs 1.3.6.1.2.1.5.14 icmpOutErrors 1.3.6.1.2.1.5.15 icmpOutDestUnreachs 1.3.6.1.2.1.5.16 icmpOutTimeExcds 1.3.6.1.2.1.5.17 icmpOutParmProbs 1.3.6.1.2.1.5.18 icmpOutSrcQuenchs 1.3.6.1.2.1.5.19 icmpOutRedirects 1.3.6.1.2.1.5.20 icmpOutEchos 1.3.6.1.2.1.5.21 icmpOutEchoReps 1.3.6.1.2.1.5.22 icmpOutTimestamps 1.3.6.1.2.1.5.23 icmpOutTimestampReps 1.3.6.1.2.1.5.24 icmpOutAddrMasks 1.3.6.1.2.1.5.25 icmpOutAddrMaskReps 1.3.6.1.2.1.5.26 TCP Group tcpRtoAlgorithm 1.3.6.1.2.1.6.1 tcpRtoMin 1.3.6.1.2.1.6.2 tcpRtoMax 1.3.6.1.2.1.6.3 tcpMaxConn 1.3.6.1.2.1.6.4 tcpActiveOpens 1.3.6.1.2.1.6.5 tcpPassiveOpens 1.3.6.1.2.1.6.6 tcpAttemptFails 1.3.6.1.2.1.6.7 tcpEstabResets 1.3.6.1.2.1.6.8 tcpCurrEstab 1.3.6.1.2.1.6.9 tcpInSegs 1.3.6.1.2.1.6.10 tcpOutSegs 1.3.6.1.2.1.6.11 tcpRetransSegs 1.3.6.1.2.1.6.12 tcpConnTable 1.3.6.1.2.1.6.13 tcpConnEntry 1.3.6.1.2.1.6.13.1 tcpConnState 1.3.6.1.2.1.6.13.1.1 tcpConnLocalAddress 1.3.6.1.2.1.6.13.1.2 tcpConnLocalPort 1.3.6.1.2.1.6.13.1.3 tcpConnRemAddress 1.3.6.1.2.1.6.13.1.4 tcpConnRemPort 1.3.6.1.2.1.6.13.1.5 tcpInErrs 1.3.6.1.2.1.6.14 tcpOutRsts 1.3.6.1.2.1.6.15 UDP Group udpInDatagrams 1.3.6.1.2.1.7.1 udpNoPorts 1.3.6.1.2.1.7.2 udpInErrors 1.3.6.1.2.1.7.3 udpOutDatagrams 1.3.6.1.2.1.7.4 udpTable 1.3.6.1.2.1.7.5 udpEntry 1.3.6.1.2.1.7.5.1 udpLocalAddress 1.3.6.1.2.1.7.5.1.1 udpLocalPort 1.3.6.1.2.1.7.5.1.2 SNMP Group snmpInPkts 1.3.6.1.2.1.11.1 snmpOutPkts 1.3.6.1.2.1.11.2 snmpInBadVersions 1.3.6.1.2.1.11.3 snmpInBadCommunityNames 1.3.6.1.2.1.11.4 snmpInBadCommunityUses 1.3.6.1.2.1.11.5 snmpInASNParseErrs 1.3.6.1.2.1.11.6 NOT USED 1.3.6.1.2.1.11.7 snmpInTooBigs 1.3.6.1.2.1.11.8 snmpInNoSuchNames 1.3.6.1.2.1.11.9 snmpInBadValues 1.3.6.1.2.1.11.10 snmpInReadOnlys 1.3.6.1.2.1.11.11 snmpInGenErrs 1.3.6.1.2.1.11.12 snmpInTotalReqVars 1.3.6.1.2.1.11.13 snmpInTotalSetVars 1.3.6.1.2.1.11.14 snmpInGetRequests 1.3.6.1.2.1.11.15 snmpInGetNexts 1.3.6.1.2.1.11.16 snmpInSetRequests 1.3.6.1.2.1.11.17 snmpInGetResponses 1.3.6.1.2.1.11.18 snmpInTraps 1.3.6.1.2.1.11.19 snmpOutTooBigs 1.3.6.1.2.1.11.20 snmpOutNoSuchNames 1.3.6.1.2.1.11.21 snmpOutBadValues 1.3.6.1.2.1.11.22 NOT USED 1.3.6.1.2.1.11.23 snmpOutGenErrs 1.3.6.1.2.1.11.24 snmpOutGetRequests 1.3.6.1.2.1.11.25 snmpOutGetNexts 1.3.6.1.2.1.11.26 snmpOutSetRequests 1.3.6.1.2.1.11.27 snmpOutGetResponses 1.3.6.1.2.1.11.28 snmpOutTraps 1.3.6.1.2.1.11.29 snmpEnableAuthenTraps 1.3.6.1.2.1.11.30]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML语法基础]]></title>
    <url>%2F2016%2F12%2F13%2F%E6%8A%80%E6%9C%AF%2FHTML-basic-grammar%2F</url>
    <content type="text"><![CDATA[HTML语法基础 学习django的时候发现还是要懂点html语法的 语法 html是一种展示网页的标记语言 由标签和被其标记的内容组成，标签的属性控制着显示的方式 &lt;tag key=&quot;value&quot;&gt;被标记的内容&lt;/tag&gt; 语法不区分大小写，默认使用小写 语法不区分回车、空格和缩进，为了代码可读和逻辑清晰，应该保持严格的缩进风格，必要时添加注释 注释内容&lt;!-- XXXXX --!&gt;，不会被显示 字符字体：保留字符无法直接使用，如&lt; &gt; 会和标签符号产生歧义，可以使用&amp;lt 或 &amp;#60表示小于号 HTML基本结构123456789101112131415161718&lt;html lang="en"&gt;&lt;!--html标签是文档开始和结束的标志--&gt;&lt;head&gt; &lt;!--HTML文件头标记，用来包含文件的基本信息，比如网页的标题、关键字，在 可以放&lt;title&gt;&lt;/title&gt;、&lt;meta&gt;&lt;/meta&gt;、&lt;style&gt;&lt;/style&gt;等等标记--&gt; &lt;!--注意:在&lt;head&gt;&lt;/head&gt;标记内的内容不会在浏览器中显示--&gt; &lt;meta charset="UTF-8"&gt; &lt;!--&lt;meta&gt; 元素可提供有关某个 HTML 元素的元信息 (meta-information)，比如描述、针对搜索引擎的关键词以及刷新频率。--&gt; &lt;title&gt;这是网页的标题&lt;/title&gt; &lt;!--网页的“主题”，显示在浏览器的标题栏，网页的标题不能太长，要短小精悍，能具体反应页面的内容，&lt;title&gt;&lt;/title&gt;标记中不能包含其他标记--&gt;&lt;/head&gt;&lt;body&gt;&lt;!--HTML文档的主体标记--&gt;&lt;!--功能：&lt;body&gt;...&lt;/body&gt;是网页的主体部分，在此标记之间可以包含如&lt;p&gt;&lt;/p&gt;、&lt;h1&gt;&lt;/h1&gt;、&lt;br&gt;、&lt;hr&gt;等等标记，这些内容组成了我们所看见的网页--&gt;&lt;!--body的属性有：背景颜色 bgcolor="red",文本颜色text="green",链接颜色 link="blue"，--&gt;&lt;!-- 已访问过链接的颜色vlink="yellow",正在被点击连接的颜色 alink="red"--&gt;&lt;/body&gt;&lt;/html&gt; 格式标记12345678910&lt;br&gt; 强制换行，后面的内容会显示在下一行&lt;p&gt;...&lt;/p&gt; 段落标记&lt;center&gt;...&lt;/center&gt; 居中对齐标记：让段落或者是文字相对于父标记居中显示&lt;pre&gt;...&lt;/pre&gt; 预格式化标记：保留预先编排好的格式&lt;li&gt;第一个&lt;/li&gt; 列表项目,默认无序&lt;ul&gt;...&lt;/ul&gt; 声明无序列表，内嵌套&lt;li&gt;&lt;ol&gt;...&lt;/ol&gt; 声明列表有序内嵌套&lt;li&gt;,属性type=&quot;[1|A|a|I|i]&quot; value=&quot;序列起始值&quot;&lt;dl&gt; &lt;dt&gt; &lt;dd&gt; 定义性列表，对列表条目进行简短的说明&lt;hr&gt; 水平分割线&lt;div&gt;...&lt;/div&gt; 分区显示/层标记，可以多层嵌套，用来编排一大段HTML代码 文本标记12345678910111213&lt;h1&gt;...&lt;/h1&gt; h1~6 1级标题文本最大，6级标题文本最小&lt;font size=&apos;3&apos; color=&apos;green&apos; face=&quot;微软雅黑&quot;&gt;有字体格式的文字&lt;/font&gt;&lt;b&gt;粗体字&lt;/b&gt;&lt;i&gt;斜体字&lt;/i&gt;&lt;sub&gt;下标字体&lt;/sub&gt;&lt;sup&gt;上标字体&lt;/sup&gt;&lt;tt&gt;打印机字体&lt;/tt&gt;&lt;cite&gt;引用方式的字体，通常是斜体&lt;/cite&gt;&lt;em&gt;强调字体，通常是斜体&lt;/em&gt;&lt;strong&gt;强调字体，通常是粗体&lt;/strong&gt;&lt;small&gt;小型字体&lt;/small&gt;&lt;big&gt;大型字体&lt;/big&gt;&lt;u&gt;带下划线的字&lt;/u&gt; 图像标签123456789&lt;img src=&quot;路径/文件名.图片格式&quot; width=&quot;属性值&quot; height=&quot;属性值&quot; border=&quot;属性值&quot; alt=&quot;属性值&quot;&gt;属性： 作用src 指定加载文件的路径width 指定图片的宽度，单位px、em、cm、mmheight 指定图片的高度，单位px、em、cm、mmborder 指定图标的边框宽度，单位px、em、cm、mmalt 当网页上的图片被加载完成后，鼠标移动到上面去，会显示这个图片指定的属性文字 如果图像没有下载或者加载失败，会用文字来代替图像显示 搜索引擎可以通过这个属性的文字来抓取图片 超链接1234567891011&lt;a href=&quot;&quot; target=&quot;打开方式&quot; name=&quot;页面锚点名称&quot; &gt;链接文字或者图片&lt;/a&gt;href 链接的地址，可以是网页或视频、图片、音乐等等target 定义超链接的打开方式target取值 _blank:在一个新的窗口中打开链接 _seif(默认值):在当前窗口中打开链接 _parent:在父窗口中打开页面（框架中使用较多） _top:在顶层窗口中打开文件（框架中使用较多）name 指定页面的锚点名称使用a标签实现页内跳转&lt;a href=&quot;#id1&quot;&gt;点击后跳到本页id值为id1的tag位置&lt;/a&gt; HTML表格12345678910111213141516171819202122232425262728293031&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt;&lt;table width="80%" border="1" height="100" align="center"&gt; &lt;!-- width 和 height的值可以是px或父级的%值， border是表格外框的宽度，默认值是0； align是表格的显示位置，left默认，right，center--&gt; &lt;!-- cellspacing，单元格之间的间距，默认是2px；cellpadding 单元格内容与单元格边框的显示距离，单位像素 --&gt; &lt;!-- frame 控制表格边框最外层的四条线框；rules 控制是否显示以及如何显示单元格之间的分割线；--&gt; &lt;caption align="bottom"&gt;表格的标题&lt;/caption&gt; &lt;!--caption位于table之后，tr表格行之前,其align属性取值 top、bottom、left、right，指定标题的位置--&gt; &lt;tr&gt; &lt;!-- 对于每一个表格行，都是由一对&lt;tr&gt;...&lt;/tr&gt;标记表示，每一行&lt;tr&gt;标记内可以嵌套多个&lt;td&gt;或者&lt;th&gt;标记 --&gt; &lt;th&gt;班级&lt;/th&gt; &lt;!--td 和th标签，必须在tr内，th是表头的单元格标记，即表格的首行 --&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;th&gt;成绩&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;四年级一班&lt;/td&gt; &lt;!--td 表格的数据单元格标记--&gt; &lt;td&gt;张三&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; HTML表单 form表单 1234567891011121314151617181920212223242526&lt;div&gt; &lt;form action="." method="post" name="login"&gt; &lt;!--&lt;form action="服务器端地址（接受表单内容的地址）" name="表单名称" method="post|get"&gt;...&lt;/form&gt;--&gt; &lt;!--get方式提交时，会将表单的内容附加在URL地址的后面，所以限制了提交的内容的长度，不超过8192个字符，且不具备保密性--&gt; &lt;!--post方式提交时，将表单中的数据一并包含在表单主体中，一起传送到服务器中处理，没有数据大小限制--&gt; &lt;!-- action=表单数据的处理程序的URL地址,为空则使用当前文档的URL地址，如果不需要使用,action="no"--&gt; &lt;!-- enctype,设置表单的资料的编码方式; target,和超链接的属于类似，用来指定目标窗口--&gt; &lt;p&gt;用户名 &lt;input type="text" name="login_name" value="QQ or Email" maxlength="20" size="10"&gt;&lt;/p&gt; &lt;p&gt;密码 &lt;input type="password" name="login_passwd" value="" size="10"&gt;&lt;/p&gt; &lt;p&gt;性别：&lt;input type="radio" name="sex" checked="checked"&gt;男&lt;input type="radio" name="sex"&gt;女&lt;/p&gt; &lt;p&gt;&lt;input type="hidden" value="隐藏的内容" name="mihiddenma" size="10"&gt;&lt;/p&gt; &lt;p&gt;爱好：&lt;input type="checkbox" name="tiyu" checked="checked"&gt;体育&lt;input type="checkbox" name="changge"&gt;唱歌&lt;br&gt;&lt;/p&gt; &lt;p&gt;&lt;br&gt; 地址： &lt;select name="dizhi"&gt; &lt;option value="sichuan"&gt;四川&lt;/option&gt; &lt;option value="beijing"&gt;北京&lt;/option&gt; &lt;option value="shanghai"&gt;上海&lt;/option&gt; &lt;/select&gt; &lt;/p&gt; &lt;p&gt;自我介绍 &lt;br&gt; &lt;textarea cols="35" rows="10" name="自我介绍"&gt;介绍一下你自己呗！&lt;/textarea&gt; &lt;/p&gt; &lt;p&gt;&lt;input type="submit" value="提交注册"&gt;&lt;/p&gt; &lt;p&gt;&lt;input type="reset" value="重置"&gt;&lt;/p&gt; &lt;p&gt;&lt;input type="button" value="一个按钮"&gt;&lt;/p&gt; &lt;/form&gt;&lt;/div&gt; input标签 12345678910111、文本域/密码框： &lt;input type=&quot;text/password&quot; name=&quot;&quot; value=&quot;框内初始化值&quot; size=&quot;框长&quot; maxlength=&quot;最大字符数&quot;&gt;2、提交，重置和普通按钮 &lt;input type=&quot;submit/reser/button&quot;&gt;3、单选框和复选框 &lt;input type=&quot;radio/checkbox&quot; checked=&quot;checked&quot;&gt; checked=&quot;checked&quot; 会默认该项被选中4、隐藏域 &lt;input type=&quot;hidden&quot; value=&quot;&quot; name=&quot;&quot;&gt; 一般用来提交验证信息5 多行文本 &lt;textarea name=&quot;name&quot; rows=&quot;value&quot; cols=&quot;value&quot; value=&quot;value&quot;&gt; ... ... &lt;/textarea&gt; rows和 cols指定行数和列数6、下拉菜单&lt;select name=&quot;&quot; size=&quot;列表高度&quot; multiple&gt; ### 如果使用mutiple关键字，则是一个下拉列表，不使用时是下拉框 &lt;option value=&quot;value&quot; selected&gt;选项1&lt;/option&gt; ### 指定了selected关键字是默认选中项，value的值是要发送给服务器上的数据 &lt;option value=&quot;value&quot;&gt;选项2&lt;/option&gt; &lt;option value=&quot;value&quot;&gt;选项3&lt;/option&gt;&lt;/select&gt;]]></content>
      <categories>
        <category>技术</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>Html</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mongodb 学习笔记]]></title>
    <url>%2F2016%2F12%2F01%2F%E6%8A%80%E6%9C%AF%2Fmongodb%20%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[ubuntu安装，mongodb apt-get install mongodb 安装好之后，服务自动运行，service mongodb stop停止服务，找到数据库存储位置/var/lib/mongodb/和日志文件位置/var/log/mongodb/mongodb.log,然后手动运行服务mongod --dbpath /var/lib/mongodb/ --logpath /var/log/mongodb/mongodb.log --logappend &amp; 如果想作为服务一直运行 修改/etc/mongodb.conf参数，允许远程登录和身份认证， 123456789101112131415161718192021# mongodb.conf# Where to store the data.dbpath=/var/lib/mongodb#where to loglogpath=/var/log/mongodb/mongodb.loglogappend=true# 允许远程注释掉此处bind_ip = 127.0.0.1# 修改端口在此处port = 27017# Enable journaling, http://www.mongodb.org/display/DOCS/Journalingjournal=true# 默认是关闭认证的，开启账号密码认证改此处# Turn on/off security. Off is currently the default#noauth = trueauth = true# Verbose logging output.verbose = true 然后运行服务service mongodb restart，本地连接mongodb后，给某个数据库添加用户，然后重启service。给admin添加的用户有数据库管理员权限。 1234567891011121314MongoDB shell version: 2.4.9connecting to: test&gt; show dbsadmin (empty)local 0.078125GB&gt; use adminswitched to db admin&gt; db.addUser('mongo','mongo123')&#123; "user" : "mongo", "readOnly" : false, "pwd" : "6a524e081c761e95c4d0b0096840d87c", "_id" : ObjectId("591078d7a129373b2f059bb3")&#125; 连接mongodb 指定数据库及身份信息 mongo ip/dbname -u username -p password 先连接，后认证 123mongo ipuse dbnamedb.auth('username', 'password') URI MONGO_URI = &#39;mongodb://localhost:27017&#39; MONGO_URI = &#39;mongodb://user:pass@localhost:27017&#39; mongodb 使用技巧 pymongo api find 查询非空 db.mycollection.find({&quot;pic_url&quot;:{$ne:null}}); 查询某个key的值 db.mycollection.find({&quot;IMAGE URL&quot;:{$ne:&#39;&#39;}}); ​ ​]]></content>
      <categories>
        <category>技术</category>
        <category>mongodb</category>
      </categories>
      <tags>
        <tag>Mongodb 数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python cookbook 笔记1]]></title>
    <url>%2F2016%2F11%2F16%2F%E6%8A%80%E6%9C%AF%2Fpython-cookbook-note1%2F</url>
    <content type="text"><![CDATA[数据结构和算法 多变量同时赋值 12345678910111213141516171819202122# 左右两边必须数目一致，右边为列表或任意可迭代对象l = [&apos;a&apos;, 1, 345]a, b, c = lprint(a, b, c)a, b, c = &apos;qwe&apos;print(a, b, c)# 变量数小于赋值数，使用*args表达式a, *others1 = l*others2, b = lprint(a, b, others1, others2) #others是列表类型# 处理字符串分割时，或变长元组的序列时，使用*args变量来占位，很方便line = &apos;nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false&apos;uname, *fields, homedir, sh = line.split(&apos;:&apos;)records = [(&apos;foo&apos;, 1, 2),(&apos;bar&apos;, &apos;hello&apos;),(&apos;foo&apos;, 3, 4),(&apos;a&apos;, 4, 5, 6),]for tag, *args in records:if tag == &apos;foo&apos;: 保存一个迭代对象的最后N个元素使用大小为N的队列实现，队列 123456from collections import dequeq = deque(maxlen=3)q.append(1)q.appendleft(0)q.pop()q.popleft() 获得最大或最小的N个元素 123456789101112131415161718heapq模块有两个函数：nlargest()和nsmallest()可以完美解决这个问题。import heapqnums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]print(heapq.nlargest(3, nums)) # Prints [42, 37, 23]print(heapq.nsmallest(3, nums)) # Prints [-4, 1, 2]两个函数都能接受一个关键字参数，用于更复杂的数据结构中：portfolio = [&#123;&apos;name&apos;: &apos;IBM&apos;, &apos;shares&apos;: 100, &apos;price&apos;: 91.1&#125;,&#123;&apos;name&apos;: &apos;AAPL&apos;, &apos;shares&apos;: 50, &apos;price&apos;: 543.22&#125;,&#123;&apos;name&apos;: &apos;FB&apos;, &apos;shares&apos;: 200, &apos;price&apos;: 21.09&#125;,&#123;&apos;name&apos;: &apos;HPQ&apos;, &apos;shares&apos;: 35, &apos;price&apos;: 31.75&#125;,&#123;&apos;name&apos;: &apos;YHOO&apos;, &apos;shares&apos;: 45, &apos;price&apos;: 16.35&#125;,&#123;&apos;name&apos;: &apos;ACME&apos;, &apos;shares&apos;: 75, &apos;price&apos;: 115.65&#125;]cheap = heapq.nsmallest(3, portfolio, key=lambda s: s[&apos;price&apos;])expensive = heapq.nlargest(3, portfolio, key=lambda s: s[&apos;price&apos;])译者注：上面代码在对每个元素进行对比的时候，会以price的值进行比较。heapq.heapify(nums) //生成堆 字典一个键映射多个值将字典对应的多个值放到列表或集合里,集合元素是无序且不重复的 123456789from collections import defaultdictd = defaultdict(list)d[&apos;a&apos;].append(1)d[&apos;a&apos;].append(2)d[&apos;b&apos;].append(4)d = defaultdict(set)d[&apos;a&apos;].add(1)d[&apos;a&apos;].add(2)d[&apos;b&apos;].add(4) 有序字典在迭代或序列化时，希望字典是有序的，有序dict是普通dict内存的2倍 123456789101112131415from collections import OrderedDictdef ordered_dict():d = OrderedDict()d[&apos;foo&apos;] = 1d[&apos;bar&apos;] = 2d[&apos;spam&apos;] = 3d[&apos;grok&apos;] = 4# Outputs &quot;foo 1&quot;, &quot;bar 2&quot;, &quot;spam 3&quot;, &quot;grok 4&quot;for key in d:print(key, d[key])# 输出有序的json编码&gt;&gt;&gt; import json&gt;&gt;&gt; json.dumps(d)&apos;&#123;&quot;foo&quot;: 1, &quot;bar&quot;: 2, &quot;spam&quot;: 3, &quot;grok&quot;: 4&#125;&apos;&gt;&gt;&gt; 字典运算查找最大值和最小值，zip函数，将dict的值和value转置，直接使用max，min处理dict，计算的值是dict的key，得到的结果也是key。也可以配合sort 1234567891011121314151617181920prices = &#123;&apos;ACME&apos;: 45.23,&apos;AAPL&apos;: 612.78,&apos;IBM&apos;: 205.55,&apos;HPQ&apos;: 37.20,&apos;FB&apos;: 10.75, &apos;qw&apos;: 10.75&#125;b = zip(prices.values(), prices.keys())for i in b: print(i)//b是一个只能迭代一次的对象(205.55, &apos;IBM&apos;)(10.75, &apos;qw&apos;)(10.75, &apos;FB&apos;)(45.23, &apos;ACME&apos;)(612.78, &apos;AAPL&apos;)(37.2, &apos;HPQ&apos;)b = zip(prices.values(), prices.keys())print(min(b)) //如果有多个相同值的key存在，会再根据key的大小返回结果(10.75, &apos;FB&apos;) 寻找2个字典的相同之处 集合操作，&amp; - + 12345678910111213141516171819202122a = &#123;&apos;x&apos; : 1,&apos;y&apos; : 2,&apos;z&apos; : 3&#125;b = &#123;&apos;w&apos; : 10,&apos;x&apos; : 11,&apos;y&apos; : 2&#125;print(a.keys() &amp; b.keys())&#123;&apos;y&apos;, &apos;x&apos;&#125;print(a.items() &amp; b.items())&#123;(&apos;y&apos;, 2)&#125;print(a.items() - b.items())&#123;(&apos;z&apos;, 3), (&apos;x&apos;, 1)&#125;print(a.items() | b.items())&#123;(&apos;y&apos;, 2), (&apos;w&apos;, 10), (&apos;z&apos;, 3), (&apos;x&apos;, 11), (&apos;x&apos;, 1)&#125;# dict.values()不支持集合操作，因为不能保证值无重复，可以先转为set。# 排除字典里某些元素，构建新的字典c = &#123;key:a[key] for key in a.keys() - &#123;&apos;z&apos;, &apos;w&apos;&#125;&#125; 保持序列原有的序列顺序，同时删除重复的值 12345678910111213# 利用集合或者生成器，直接使用set会打乱原有顺序def dedupe(items): seen = set() for item in items: if item not in seen: yield item seen.add(item)a = [1, 5, 2, 1, 9, 1, 5, 10]print(list(deque(a)))# 用来去除文件里重复的行也可以with open(&apos;a.txt&apos;) as f: for line in deque(f): print(line) 命名切片 使用可读的变量名定义slice对象，增加代码可维护可读性 12345record = &apos;....................100 .......513.25 ..........&apos;# s.start, s.stop, s.step 分片对象属性，s.indices(len(record)),根据record长度重新映射分片的边界PRICE = slice(31,37)NUMBERS = slice(20,23)cost = int(record[NUMBERS]) * float(record[PRICE]) 统计序列中元素出现的次数 常规方法我们会用字典计数实现，key保存元素，value用来计数，count[&#39;s&#39;] += 1 。collections.Counter类更方便，还有most_common方法,更重要的是两个counter对象实例可以进行数学运算+，- 1234567891011121314words = [&apos;look&apos;, &apos;into&apos;, &apos;my&apos;, &apos;eyes&apos;, &apos;look&apos;, &apos;into&apos;, &apos;my&apos;, &apos;eyes&apos;,&apos;the&apos;, &apos;eyes&apos;, &apos;the&apos;, &apos;eyes&apos;, &apos;the&apos;, &apos;eyes&apos;, &apos;not&apos;, &apos;around&apos;, &apos;the&apos;,&apos;eyes&apos;, &quot;don&apos;t&quot;, &apos;look&apos;, &apos;around&apos;, &apos;the&apos;, &apos;eyes&apos;, &apos;look&apos;, &apos;into&apos;,&apos;my&apos;, &apos;eyes&apos;, &quot;you&apos;re&quot;, &apos;under&apos;]from collections import Counterword_counts = Counter(words)# 出现频率最高的3 个单词top_three = word_counts.most_common(3)print(top_three)# Outputs [(&apos;eyes&apos;, 8), (&apos;the&apos;, 5), (&apos;look&apos;, 4)]print(word_counts)Counter(&#123;&apos;eyes&apos;: 8, &apos;the&apos;: 5, &apos;look&apos;: 4, &apos;my&apos;: 3, &apos;into&apos;: 3, &apos;around&apos;: 2, &quot;don&apos;t&quot;: 1, &apos;not&apos;: 1, &apos;under&apos;: 1, &quot;you&apos;re&quot;: 1&#125;) 通过某几个字典关键字段排序字典列表 operator的itemgetter，支持多个key 123456789from operator import itemgetterrows_by_uid = sorted(rows, key=itemgetter(&apos;uid&apos;))# output[&#123;&apos;uid&apos;: 1001, &apos;lname&apos;: &apos;Cleese&apos;, &apos;fname&apos;: &apos;John&apos;&#125;, &#123;&apos;uid&apos;: 1002, &apos;lname&apos;: &apos;Beazley&apos;, &apos;fname&apos;: &apos;David&apos;&#125;, &#123;&apos;uid&apos;: 1003, &apos;lname&apos;: &apos;Jones&apos;, &apos;fname&apos;: &apos;Brian&apos;&#125;, &#123;&apos;uid&apos;: 1004, &apos;lname&apos;: &apos;Jones&apos;, &apos;fname&apos;: &apos;Big&apos;&#125;]rows_by_lfname = sorted(rows, key=itemgetter(&apos;lname&apos;,&apos;fname&apos;))# itemgetter()有时候也可以用lambda表达式代替，lambda比较慢，比如：rows_by_fname = sorted(rows, key=lambda r: r[&apos;fname&apos;])rows_by_lfname = sorted(rows, key=lambda r: (r[&apos;lname&apos;],r[&apos;fname&apos;]))# itemgetter同样适用于min、max的key关键字参数 排序不支持比较的对象 sorted方法的key参数接收一个callable对象，该对象作用于每个排序的个体，根据其返回的值作为排序比较的依据，同理max，min函数。加入比较的是一类对象，key的接收函数可以定义为lambda函数，也可以使用operator.attrgetter(‘user_id’） 12345sorted(users, key=lambda u: u.user_id) ### 二者均可，但是attrgetter运行速度更快，而且支持多个变量&gt;&gt;&gt; from operator import attrgetter&gt;&gt;&gt; sorted(users, key=attrgetter(&apos;user_id&apos;))[User(3), User(23), User(99)] 根据某个字段将数据分组 先将数据排序，然后使用itertools.groupby(&#39;key_field&#39;)，或者使用 collections.defaultdict（list）, dict_a[row[&#39;key_field&#39;]].append(row) 1234567891011121314151617181920212223242526272829303132from operator import itemgetterfrom itertools import groupbyrows = [&#123;&apos;address&apos;: &apos;5412 N CLARK&apos;, &apos;date&apos;: &apos;07/01/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;5148 N CLARK&apos;, &apos;date&apos;: &apos;07/04/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;5800 E 58TH&apos;, &apos;date&apos;: &apos;07/02/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;2122 N CLARK&apos;, &apos;date&apos;: &apos;07/03/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;5645 N RAVENSWOOD&apos;, &apos;date&apos;: &apos;07/02/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;1060 W ADDISON&apos;, &apos;date&apos;: &apos;07/02/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;4801 N BROADWAY&apos;, &apos;date&apos;: &apos;07/01/2012&apos;&#125;,]l = groupby(rows, key=itemgetter(&apos;date&apos;)) //l值能被迭代一次for i, j in l: print(i) for z in j: print(z)# 输出结果07/01/2012&#123;&apos;date&apos;: &apos;07/01/2012&apos;, &apos;address&apos;: &apos;5412 N CLARK&apos;&#125;&#123;&apos;date&apos;: &apos;07/01/2012&apos;, &apos;address&apos;: &apos;4801 N BROADWAY&apos;&#125;07/02/2012&#123;&apos;date&apos;: &apos;07/02/2012&apos;, &apos;address&apos;: &apos;5800 E 58TH&apos;&#125;&#123;&apos;date&apos;: &apos;07/02/2012&apos;, &apos;address&apos;: &apos;5645 N RAVENSWOOD&apos;&#125;&#123;&apos;date&apos;: &apos;07/02/2012&apos;, &apos;address&apos;: &apos;1060 W ADDISON&apos;&#125;07/03/2012&#123;&apos;date&apos;: &apos;07/03/2012&apos;, &apos;address&apos;: &apos;2122 N CLARK&apos;&#125;07/04/2012&#123;&apos;date&apos;: &apos;07/04/2012&apos;, &apos;address&apos;: &apos;5148 N CLARK&apos;&#125;### 使用defaultdictfrom collections import defaultdictrows_by_date = defaultdict(list)for row in rows: rows_by_date[row[&apos;date&apos;]].append(row) 过滤序列元素 最简单的方法，当过滤规则简单时，使用列表推导式或生成器表达式；当过滤规则负责时，编写一个过滤规则函数(对每个元素返回True或False)，然后使用内建的filter()函数，filter(func, list_a)得到的是生成器，想要得到列表，可以使用list()函数。 1234567891011121314151617mylist = [1, 4, -5, 10, -7, 2, 3, -1]l = [n for n in mylist if n &gt; 0]# 上述的列表推导式会占用大量的内存，使用生成器表达式迭代产生要过滤的元素pos = (n for n in mylist if n &gt; 0)for i in pos: print(i)def is_int(i): try: x = int(i) return True except ValueError: return Falsel = list(filter(is_int, &apos;123wertsdfg&apos;))print(l)# 输出 [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;] 使用列表推导式，可以同时转换(处理)符合条件的数据；也可以替换(处理)不符合条件的结果，而不是直接丢弃。 123import mathl1 = [math.sqrt(n) for n in mylist if n &gt; 0]l2 = [n if n &gt; 0 else 0 for n in mylist] 另外一个值得关注的过滤工具就是itertools.compress()，它以一个iterable对象和一个相对应的Boolean选择器序列作为输入参数。然后输出iterable对象中对应选择器为True的元素,是个生成器。当你需要用另外一个相关联的序列来过滤某个序列的时候，这个函数是非常有用的. 12345678910# 关键点在于先创建一个Boolean序列，指示哪些元素复合条件。然后compress()函数根据这个序列去选择输出对应位置为True的元素。from itertools import compresss = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;]count = [99, 20, 16, 71, 56]big50 = [n &gt; 50 for n in count]l = list(compress(s, big50))print(l)# 输出 [&apos;a&apos;, &apos;d&apos;, &apos;e&apos;] 从字典中提取子集 首先考虑字典推导,dict()函数传入元组的推导式也可以实现，但比字典推到速度慢1倍： 1234567891011121314prices = &#123;&apos;ACME&apos;: 45.23,&apos;AAPL&apos;: 612.78,&apos;IBM&apos;: 205.55,&apos;HPQ&apos;: 37.20,&apos;FB&apos;: 10.75&#125;# Make a dictionary of all prices over 200p1 = &#123;key: value for key, value in prices.items() if value &gt; 200&#125;# Make a dictionary of tech stockstech_names = &#123;&apos;AAPL&apos;, &apos;IBM&apos;, &apos;HPQ&apos;, &apos;MSFT&apos;&#125;p2 = &#123;key: value for key, value in prices.items() if key in tech_names&#125;# dict()方式慢p1 = dict((key, value) for key, value in prices.items() if value &gt; 200) 映射名称到序列， 命名元组替代不可读的下标，或作为有大量字典元素的替代，节省空间，但命名元组的元素是不可修改的，（可以_replace()方法更新，如果需要大量更新，则不适合使用nametuple） 12345678910111213141516171819202122232425from collections import namedtuplenamedtuple(typename,field_names,verbose=False,rename=False)# 返回一个类，但该类支持元组操作，是元组类的一个子类Subscriber = namedtuple(&apos;Subscriber&apos;, [&apos;addr&apos;, &apos;joined&apos;])sub = Subscriber(&apos;jonesy@example.com&apos;, &apos;2012-10-19&apos;)print(sub.addr, sub.joined)a, b = sublen(sub)# 使用命名元组增加可读性的一个例子from collections import namedtupleStock = namedtuple(&apos;Stock&apos;, [&apos;name&apos;, &apos;shares&apos;, &apos;price&apos;])def compute_cost(records): //records [[price, nums],] total = 0.0 for rec in records: s = Stock(*rec) total += s.shares * s.price return total## 如果非要改变命名元组的属性，可以使用_replace()方法。该方法常用于填充数据，返回一个新的元组实例# 比如我们命名了一个具有缺省值的默认元组，用该方法创建实例stock_prototype = Stock(&apos;&apos;, 0, 0.0, None, None)def dict_to_stock(s): return stock_prototype._replace(**s) 转换并同时处理数据 处理函数与生成器参数结合 123456s = sum(x*x for x in nums)import os if any(name.endswith(&apos;.py&apos;) for name in os.listdir(&apos;.&apos;)): print(1)s = (&apos;ACME&apos;, 50, 123.45)print(&apos;,&apos;.join(str(x) for x in s)) 合并多个字典或映射使用collections.ChainMap()或dict的update方法，update会改变原有的字典结构，或创建新的字典对象，（当原字典更新时，update无法反应这种改变） 12345678910a = &#123;&apos;x&apos;: 1, &apos;z&apos;: 3 &#125;b = &#123;&apos;y&apos;: 2, &apos;z&apos;: 4 &#125;# 现在假设你必须在两个字典中执行查找操作(比如先从a中找，如果找不到再在b中找)from collections import ChainMapc = ChainMap(a,b) //字典的列表容器，仍支持大部分字典的方法print(c[&apos;x&apos;]) # (from a)print(c[&apos;y&apos;]) # (from b)print(c[&apos;z&apos;]) # (from a)，永远只输出第一个匹配字典的结果# 对于字典的更新或删除操作总是影响的是列表中第一个字典。del c[&apos;z&apos;] 字符串和文本日期和时间迭代器和生成器文件和IO数据编码和处理函数类和对象元编程模块和包网络和web编程并发编程脚本和系统管理异常、调试、测试]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix部署]]></title>
    <url>%2F2016%2F10%2F30%2F%E6%8A%80%E6%9C%AF%2Fzabbix%2F</url>
    <content type="text"><![CDATA[zabbix appliance 部署 登陆 appliance：zabbix 修改中文支持修改你的 locales . inc . php 这个文件，/usr/share/zabbix/include/ 下‘zhCN’ =&gt; [‘name’ =&gt; (‘Chinese (zh_CN)’), ‘display’ =&gt; true], #也就是把false改为true，在web页面点击用户信息头像，选择语言。 解决图形字体乱码修改你的defines.inc.php 这个文件 1234#修改第93行define(&apos;ZBX_FONT_NAME&apos;, &apos;msyh&apos;); #修改第45行改为 define(&apos;ZBX_GRAPH_FONT_NAME&apos;, &apos;msyh&apos;) 然后下载微软雅黑字体，传入zabbix/fonts下，架设服务器，python -m SimpleHTTPServer 8080 配置snmp server]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Git服务器]]></title>
    <url>%2F2016%2F09%2F22%2F%E6%8A%80%E6%9C%AF%2Fgit-server-on-centos%2F</url>
    <content type="text"><![CDATA[cent os 搭建git server + gitolite 爬了很久的坑，终于爬出来了，真累。说说走的弯路，最初打算配置gitlab（有和github一样漂亮的web界面），然后下源码编译，失败。后来发现gitlab有中文站，提供二进制包，可能是centos32位的原因，为了少浪费生命，果断放弃，转而采用git-core + gitolite。 1、安装git1.1、创建git用户12345678# yum update# yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel perl-devel# mkdir -p /home/git //创建git用户的目录# groupadd git //创建git用户组# useradd -g git -d /home/git -s /bin/bash git // 添加linux用户git# passwd git //设置git用户密码# passwd -d git //在所有工作完成后禁用git用户的密码，这样git无法ssh远程登录主机 1.2、更改git目录权限12# chown -R git:git /home/git# chmod -R 2755 /home/git 1.3、源码安装git先从官网 http://git-scm.com/download 上下载 git源码，然后解压，tar -zxvf git-xxx.xxx.tar.gz 1234567891011121314系统默认版本为1.8.3，版本太老，删除并通过源代码安装删除安装的gityum -y remove git安装git依赖包$ sudo yum install curl-devel expat-devel gettext-devel openssl-devel perl-devel zlib-devel下载源码并编译安装 mkdir /tmp/git &amp;&amp; cd /tmp/git curl --progress https://www.kernel.org/pub/software/scm/git/git-2.8.2.tar.gz | tar xz cd git-2.8.2/ ./configure make make prefix=/usr/local install为确保$PATH环境变量生效，需要重新连接后执行git --versionctrl + d 关闭ssh会话，然后重新ssh登录。 2、安装gitolite 参照官方的quick install，只需3行命令 12345678910111213# su – git //切换到git用户下安装$ pwd /home/git$ mkdir bin// 获取源码$ git clone https://github.com/sitaramc/gitolite.git或者git clone git://github.com/ossxp-com/gitolite.git $ ls bin gitolite// 安装$ ./gitolite/install --to /home/git/bin/$ ls bin/commands gitolite gitolite-shell lib syntactic-sugar triggers VERSION VREF 3、配置gitolite管理员首先生成rsa key,然后用管理员的public key初始化gitolite 1234567891011121314// Windows安装git for windows2.6.2，安装完后打开Git Bash$ ssh-keygen –f admin –C 20151030@qq.com$ scp admin.pub git@serverIP:/tmp/admin.pub// 切换到git用户，为gitolite配置管理员$ su - git$ bin/gitolite setup -pk /tmp/admin.pub Initialized empty Git repository in /home/git/repositories/gitolite-admin.git/Initialized empty Git repository in /home/git/repositories/testing.git/WARNING: /home/git/.ssh missing; creating a new one (this is normal on a brand new install)WARNING: /home/git/.ssh/authorized_keys missing; creating a new one (this is normal on a brand new install)$ ls bin gitolite projects.list repositories 4、ssh配置服务端 123456789su -// vim /etc/ssh/sshd_config，将下面几句前面的#号去掉RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keysAuthorizedKeysCommand noneAuthorizedKeysCommandRunAs nobody// 重启sshservice sshd restart 客户端的git bash， 1234在 ~/.ssh/config 添加以下内容，以便连接到服务器Host 192.168.1.254Compression yesIdentityFile ~/.ssh/id_rsa 5、gitolite添加用户、库，设置用户管理权限12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455561. windows安装TortoiseGit2. 新建git文件夹，进入，右键Git Clone…URL: git@serverIP:gitolite-admin.git 勾选Load Putty Key，选择你的私钥（此处需使用ppk格式私钥，可以使用puttygen将私钥admin转换成admin.ppk） 点击OK即可clone服务器上的gitolite-admin文件夹到本地3. 添加用户管理员将其他用户的公钥(如dong.pub)复制到gitolite-admin/keydir/下4. 添加库进入gitolite-admin/conf/，右键Git Bash Here，$ vim gitolite.conf (此文件用于添加库和配置用户对库的权限)repo gitolite-admin RW+ = adminrepo testing RW+ = @all repo git-dong #新建库git-dong RW+ = admin dong #设置用户admin dong有读写权限 将修改push到服务器，即可添加库和用户。:wq$ cd ..$ git status #查看git库状态$ git add keydir/dong.pub conf/gitolite.conf 或者git add .$ git commit –m “add repo git-admin;add user dong”进入gitolite-admin文件夹，右键TortoiseGitPush点击OK即可。5. 用户权限管理在客户端clone gitolite-admin.git,编辑gitolite-admin/conf/gitolite.conf配置各用户权限。1 @team1 = zc2 @team2 = aws zc3 repo gitolite-admin4 RW+ = admin56 repo ossxp/.+7 C = admin8 RW = @all910 repo testing11 RW+ = admin12 RW master = junio13 RW+ pu = junio14 RW cogito$ = pasky15 RW bw/ = linus16 - = somebody17 RW tmp/ = @all18 RW refs/tags/v[0-9] = junio在上面的示例中，我们演示了很多授权指令。• 第1行，定义了用户组 @admin，包含两个用户 jiangxin 和 wangsheng。• 第3-4行，定义了版本库 gitolite-admin。并指定只有用户 jiangxin 才能够访问，并拥有读(R)写(W)和强制更新(+)的权限。• 第6行，通过正则表达式定义了一组版本库，即在 ossxp/ 目录下的所有版本库。• 第7行，用户组 @admin 中的用户，可以在 ossxp/ 目录下创建版本库。创建版本库的用户，具有对版本库操作的所有权限。• 第8行，所有用户都可以读写 ossxp 目录下的版本库，但不能强制更新。• 第9行开始，定义的 testing 版本库授权使用了引用授权语法。• 第11行，用户组 @admin 对所有的分支和里程碑拥有读写、重置、添加和删除的授权。• 第12行，用户 junio 可以读写 master 分支。（还包括名字以 master 开头的其他分支，如果有的话）。• 第13行，用户 junio 可以读写、强制更新、创建以及删除 pu 开头的分支。第14行，用户 pasky 可以读写 cogito 分支。 (仅此分支，精确匹配）。 以下是通过修改gitolite-admin库，管理git用户和权限。添加新用户wuxiaohui的公钥，赋予其读写testing.git库的权限。 123456789## 对权限文件配置gitolite.conf的修改repo gitolite-admin RW+ = adminrepo testing RW+ = @allrepo new-source RW+ = wuxiaohui Chenchuneng maning 修改完成后push到git server。 12345678910111213141516171819202122232425262728293031shuaiyy@shuaiyy-PC MINGW64 /g/git/gitolite-admin (master)$ git statusOn branch masterYour branch is up-to-date with &apos;origin/master&apos;.Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: conf/gitolite.confUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) keydir/wuxiaohui.pubno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)shuaiyy@shuaiyy-PC MINGW64 /g/git/gitolite-admin (master)$ git add keydir/wuxiaohui.pub conf/gitolite.confwarning: LF will be replaced by CRLF in keydir/wuxiaohui.pub.The file will have its original line endings in your working directory.shuaiyy@shuaiyy-PC MINGW64 /g/git/gitolite-admin (master)shuaiyy@shuaiyy-PC MINGW64 /g/git/gitolite-admin (master)$ git commit -m &quot;add repo new-source;add user wuxiaohui&quot;[master bf3f6fe] add repo new-source;add user wuxiaohuiwarning: LF will be replaced by CRLF in keydir/wuxiaohui.pub.The file will have its original line endings in your working directory. 2 files changed, 4 insertions(+) create mode 100644 keydir/wuxiaohui.pub 6、git基础教程6.1、基本操作 git init要想使用git进行版本管理，必须先初试化仓库。新建一个文件夹，在该文件夹下执行 git init 命令，就可以将该文件夹初始化为git仓库，其下会生成.git的文件夹，记录git的版本追踪管理，千万不要删除或更改。 git status显示当前仓库的状态，，如 处于master分支，没有修改内容。 git add我们创建的文件不会被自动加入git仓库的管理文件中，必须使用git add filename或git add .使其加入本地的Index暂存区。然后git status 查看改变。 git commit -m “First Commit !”将git add的内容提交，保存到版本管理的历史记录中。git status 查看状态。 git log查看日志，其中 commit ID是版本号，如果想要改变到某个版本，需要指定版本号。git log file/dir 查看指定文件或文件夹的日志 git diff 查看当前状态和最新提交，历史版本的不同之处。其中+代表新增，-代表删除； 6.2、分支操作 git branch 显示分支一览表 git checkout -b 创建、切换分支 git merge 合并分支 git log –graph 以图表形式查看分支 6.3、更改提交 git reset 回溯历史版本 git commit –amend 修改提交信息 git rebase -i 压缩历史 6.4、推送至远程仓库 git remote add 添加远程仓库 git push 推送至远程仓库 6.5、从远程仓库获取 git clone 获取远程仓库到本地 git pull 获取最新的远程仓库分支 6.6、在线git命令练习 LearnGitBranching 提供在线的git命令学习与实践 Try Git 6.7、git命令备忘录master: 默认开发分支 origin: 默认远程版本库 Head: 默认开发分支 Head^: Head的父提交 创建版本库12$ git clone &lt;url&gt; #克隆远程版本库$ git init #初始化本地版本库 修改和提交123456789$ git status #查看状态$ git diff #查看变更内容$ git add . #跟踪所有改动过的文件$ git add &lt;file&gt; #跟踪指定的文件$ git mv &lt;old&gt;&lt;new&gt; #文件改名$ git rm&lt;file&gt; #删除文件$ git rm --cached&lt;file&gt; #停止跟踪文件但不删除$ git commit -m &quot;commit messages&quot; #提交所有更新过的文件$ git commit --amend #修改最后一次改动 查看提交历史123$ git log #查看提交历史$ git log -p &lt;file&gt; #查看指定文件的提交历史$ git blame &lt;file&gt; #以列表方式查看指定文件的提交历史 撤销1234$ git reset --hard HEAD #撤销工作目录中所有未提交文件的修改内容$ git checkout HEAD &lt;file&gt; #撤销指定的未提交文件的修改内容$ git revert &lt;commit&gt; #撤销指定的提交$ git log --before=&quot;1 days&quot; #退回到之前1天的版本 分支与标签1234567$ git branch #显示所有本地分支$ git checkout &lt;branch/tag&gt; #切换到指定分支和标签$ git branch &lt;new-branch&gt; #创建新分支$ git branch -d &lt;branch&gt; #删除本地分支$ git tag #列出所有本地标签$ git tag &lt;tagname&gt; #基于最新提交创建标签$ git tag -d &lt;tagname&gt; #删除标签 合并与衍合12$ git merge &lt;branch&gt; #合并指定分支到当前分支$ git rebase &lt;branch&gt; #衍合指定分支到当前分支 远程操作12345678$ git remote -v #查看远程版本库信息$ git remote show &lt;remote&gt; #查看指定远程版本库信息$ git remote add &lt;remote&gt; &lt;url&gt; #添加远程版本库$ git fetch &lt;remote&gt; #从远程库获取代码$ git pull &lt;remote&gt; &lt;branch&gt; #下载代码及快速合并$ git push &lt;remote&gt; &lt;branch&gt; #上传代码及快速合并$ git push &lt;remote&gt; :&lt;branch/tag-name&gt; #删除远程分支或标签$ git push --tags #上传所有标签 7、githubGitHub 除了 Git 代码仓库托管及基本的 Web 管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能。 Github的三个精彩功能 fork，star，watch 1.想拷贝别人项目到自己帐号下就fork一下。2.持续关注别人项目更新就star一下3.watch是设置接收邮件提醒的。 7.1、开始 创建账户 完善个人信息 设置Key 添加公钥 7.2、使用 创建仓库 连接仓库 提交开源代码 Github Desktop 7.3、GitHUb Flow ————以部署为中心的开发模式GitHub Flow 保持master分支始终是可部署使用的 新的开发要从master分支创建新的分支，分支名要有描述性意义 在本地仓库的新建分支提交 PUSH本地分支作业到github服务器端的同名分支 审查测试push的新分支，确认无误后与master合并 合并后的master分支可以立即投入使用，如果有bug，和回退到提交状态之前 8、Git server 使用指导8.1、将自己的代码上传到远程仓库我在 project 目录下创建了4个用户对应的仓库 Chenchuneng.git Maning.git Wuxiaohui.git Wangba.git因此对应的git仓库地址为： git@121.41.15.6:project/Wangba.git使用git clone 克隆到本地： 假如我的workspace目录下有2个项目源码，project1，project2，想上传project1到远程仓库 Wangba.git。 在workspace下打开git bash here，执行 git init 初试化本地仓库 git add project1,添加需要git log的文件，git add .可以添加所有变更的文件，省事。 git commit -m &quot; 备注&quot;，提交变化到本地仓库 git remote add mywork git@121.41.15.6:project/Yangshuai.git,为本地仓库添加远程仓库，命令中的mywork 是我们为远程仓库起的别名（可以自己任取），然后push的时候使用别名 mywork 就可以少打很多字了。 将本地仓库push到服务器上的远程仓库，右键workspace，选择Tortogit小乌龟里的push命令： 以后修改添加的代码想要上传至git服务器，重复 2 3 5步骤，即 add commit push。其他目录下的代码想要上传的话，重复步骤 1-6 。 注意：git clone 和git push 我们都是用的程序而不是命令行，是因为在git bash 下如何配置git服务器认证所需的ssh-key暂时没找到解决方法，而GUI工具可以”load ssh key”。]]></content>
      <categories>
        <category>技术</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 模板语法]]></title>
    <url>%2F2016%2F08%2F19%2F%E6%8A%80%E6%9C%AF%2FDjango%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Table of Contents generated with DocToc Django模板语法 模板变量 条件分支 循环 过滤器 filter Django模板语法模板变量 {{var_name}} ,view视图传入参数可以为 Context({'name':'vale',}) {{dict.key}},view视图传入参数为dict字典 {{object_name.attribute 或 method}} ,view视图传入一个对象object。 {{list_name.N}},view视图出入一个list，可以取出list的第N个元素。 条件分支 {% if user.age < 18 %} 未成年{% else %} XXX {% endif %} 条件中的逻辑关系词 and or not，其中and与or不能同时使用。 循环 for 123456789101112&#123;% raw %&#125;&#123;% for book in booklist reversed %&#125; &#123;&#123;book&#125;&#125;&#123;&#123;% empty %&#125;&#125; &lt;li&gt;&#123;&#123;forloop.counter&#125;&#125;&lt;/li&gt; &lt;li&gt;&#123;&#123;forloop.revcounter&#125;&#125;&lt;/li&gt; &lt;li&gt;&#123;&#123;foorloop.counter0&#125;&#125;&lt;/li&gt; &lt;li&gt;&#123;&#123;forloop.revcounter0&#125;&#125;&lt;/li&gt;&#123;% endfor %&#125;&#123;% for key in dict %&#125; &#123;&#123;key&#125;&#125; &#123;% endfor %&#125;&#123;% endraw %&#125; reversed 是逆序输出,{{% empty %}}判断循环体是否为空，for 没有continue和break； forloop变量forloop.counter 从1计数的当前循环次数，forloop.revcounter，剩余的循环次数forloop.counter0，从0开始计数的循序。forloop.first和forloop.last,是否为第一次和最后一次循环，True 或False。 过滤器 filter |{{book | upper | lower | capfisrt}},将变量book先变大写在变小写在变首字母大写|过滤器标识，类似于linux的管道，{{today | date:"Y-m-d"}},today变量为datetime.datetime.now()。 自定义过滤器模板自定义的过滤器位于 templatetags文件夹下，这是一个python包，里面要有一个空的__init__文件，一般一个过滤器函数为一个单独的py文件 12345from django import templateregister = template.Library()def percent(value): return value + '%'register.filter('percent',percent) 过滤器必须用register对象注册，有2个参数，name和func，也可以在func处使用装饰器，如下： 1234567from django import templateregister = template.Library()@register.filter(name='percent')def percent(value): return value + '%' #register.filter('percent',percent) 使用自定义过滤器时必须在模板中先load py文件，{% load percent %} 需要重启服务器使过滤器生效]]></content>
      <categories>
        <category>技术</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 学习笔记(二)]]></title>
    <url>%2F2016%2F08%2F16%2F%E6%8A%80%E6%9C%AF%2FDjango%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%2F</url>
    <content type="text"><![CDATA[Table of Contents generated with DocToc Django进阶 1、用户信息扩展 1.1、使用profile扩展 1.2、继承AbstractUser 2、自定义认证 2.1、自定义认证方式 2.2、session保持 3、权限设计和使用 3.1、用户注册 4、自定义模板库 5、自定义标签 6、自定义过滤器 7、model的定义和同步，增删改 8、django单表查询和多表查询 9、聚合查询 10、ORM无法满足查询需求时，直接使用SQL语句查询 10.1、raw() 10.2、cursor() 11、querysets和惰性机制 12、自定义manager管理器 新增manager方法 改变原有的querySet方法 13、Form表单 django开发实践技巧： 1、从setting.py中读取配置信息为全局使用 2、数据库模型设计技巧 模板设计 导航数据获取 Django进阶1、用户信息扩展1.1、使用profile扩展在models定义一个profile表，使用外键关联 1.2、继承AbstractUser在models创建继承AbstractUser的User类，为其增加新的字段，比如QQ号，手机号。在setting里设置字段 AUTH_USER_MODEL = &#39;blog.User&#39;在admin注册自己的User。 2、自定义认证2.1、自定义认证方式2.2、session保持3、权限设计和使用3.1、用户注册4、自定义模板库base文件和base.html 123456\&#123;\% block head_css \%\&#125; &lt;继承扩展此处部分&gt;\&#123;\% endblock \%\&#125;\&#123;\% block head_css \%\&#125; &lt;继承扩展此处部分&gt;\&#123;\% endblock \%\&#125; 模板文件中引用base.html,重载里面需要定制的block，其他的继承base里默认的。 1234\&#123;\% extend "base/base.html" \%\&#125;\&#123;\% block container \%\&#125;实现部分\&#123;\% endblock \%\&#125; 5、自定义标签在app目录下建立templatetags文件夹， 12345678910111213141516from django import templateregister = template.Library()class UpperNode(template.Node): def __init__(self,nodelist): self.nodelist = nodelist def render(self,context): content = self.nodelist.render(context) return content.lower()@register.tagdef upper(parse,token): nodelist = parse.parse("endupper") parse.delete_first_token() return UpperNode(nodelist) 在模板中加载使用自定义标签： 1234\&#123;\% load upper \%\&#125; \&#123;\% upper \%\&#125; upper标签将作用于此块的内容 \&#123;\% endupper \%\&#125; 6、自定义过滤器在templatetags下创建 assets.py,实现区分开发环境的assets资源定位 123456789from django import templateregister = template.Library()from blog.settings import is_Debug@register.filterdef assets(value): if is_debug: return '/static/'+value return '/static/assets/' +value 在模板中使用过滤器： 1234&#123;% raw %&#125;&#123;% load assets %&#125; &lt;link src="&#123;&#123;'1.css' | assets&#125;&#125;"/&gt;&#123;% endraw %&#125; 7、model的定义和同步，增删改在models.py中导入django.db 的models,数据类定义：在models.py中定义一个评论类123456789101112class review(models.Model): user = models.ForeignKey(User) new = models. content = models.TextField creat_time = models.DateTimeField() is_dele = models.BoolenField(default = 0) def __str__(self): return self.content class Meta: permission = () 同步数据库: 增加数据：在视图views中导入数据类，比如user类，然后创建user对象的实例，然后调用save方法。修改：先get实例，然后赋值修改，最后save 删除： 1、修改is_dele标志字段 2、删除数据实例，先获取实例。然后调用delete()方法 8、django单表查询和多表查询ORM查询 多表查询用到外键 9、聚合查询Q协助查询，实现与或非。12from django.db.models import Qres_list = news.objects.filter(Q(title = 'today')|Q(body = 'new')) 10、ORM无法满足查询需求时，直接使用SQL语句查询10.1、raw() XXX.objects.raw(‘SQL 语句’)，但raw的SQL语句里，必须包含主键；对复杂语句支持不好 12res_list = user.objects.raw('select * from blog_user where id = 1 or sex = "女" ') 10.2、cursor()12345from django.db import connection,transactioncursor = connection.cursor()sql = 'select * from user'cursor.execute(sql)res_list = cursor.fetchall() #每条记录是一个元组 11、querysets和惰性机制querysets查询结果集，可以遍历，也可以继续执行查询筛选，res_list = news.objects.filter(Q(title = &#39;today&#39;)|Q(body = &#39;new&#39;)) 惰性机制：只有querysets被使用到，其数据查询才被执行，并返回结果。 1234res_list = news.objects.filter(Q(title = 'today')|Q(body = 'new')) res_list.order_by('age')res_dict = res_list.value('id','name') 12、自定义manager管理器当ORM的方法无法满足我们的需求时， 新增manager方法类似article.objects.all()就是一个manager，用来获取数据的方法，我们可以定义自己的manager，在models.py中新建类，继承models.Manager， 12345678910111213141516class PollManager(models.Manager): def with_counts(self): from django.db import connection with connection.cursor() as cursor: cursor.execute(""" SELECT p.id, p.question,p.poll_date, COUNT(*) FROM polls_opinionpoll p, polls_response r WHERE p.id = r.poll_id GROUP BY p.id, p.question, p.poll_date ORDER BY p.poll_date DESC""") result_list = [] for row in cursor.fetchall(): p = self.model(id=row[0], question=row[1],poll_date=row[2]) p.num_responses = row[3] result_list.append(p) return result_listclass OpinionPoll(models.Model): question = models.CharField(max_length=200) poll_date = models.DateField() objects = PollManager() 然后就可以使用OpinionPoll.objects.with_count()方法 改变原有的querySet方法覆盖重写原有方法。 13、Form表单在views.py中定义form类：1234567891011from django import formsclass LoginForm(forms.Form) email = forms.CharField(label = 'email',max_length =100) pwd = forms.CharField(label ='password',widget = forms.PasswordInput)#添加验证 def clean_email(self): pass def clean(self): if len(self.cleaned_data["email"].split('@'))&lt;2: raise forms.ValidationError('email is wrong!') return email django开发实践技巧：1、从setting.py中读取配置信息为全局使用settings中： 123456789101112131415161718192021222324252627#网站的基本信息配置SITE_URL = 'http://localhost:8000/'SITE_NAME = '个人博客'SITE_DESC = '专注Python开发，欢迎和大家交流'WEIBO = 'http://weibo.com/holdhiitfitness/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo&amp;is_all=1'WEIBO_TENCENT = 'http://weibo.com/holdhiitfitness/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo&amp;is_all=1'PRO_RSS = 'http://www.abroadrecommend.com'PRO_EMAIL = 'johnson_hugh@163.com'# 在templates中配置处理器TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [ os.path.join(BASE_DIR,'templates'), ], 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', 'blog.views.global_setting' # 注意此处 ], &#125;, &#125;,] 在 views中定义templates处理器的方法12345from django.conf import settingsdef globe_setting(request): return &#123;'SITE_URL':settings.SITE_URL, 'SITE_NAME':settings.SITE_NAME &#125; 在模板文件中，可直接引用变量{{SITE_URL}} 2、数据库模型设计技巧使用工具 PowerDesign、ERWin、Visio、Navicat Data MOdeler 分析可能存在的数据库表 分析可能会有的数据列，以及对应的数据类型和约束。 设计数据模型图 在modles.py中定义模型： 1234567891011class Tag(models.Model): name = models.CharField(max_length =30,verbose_name ='标签名称') class Meta: # 在admin管理界面可以看到 verbose_name = '标签' verbose_name_plural = verbose_name def __unicode__(self): #print 对象时的输出 return self.name# 自定义User modelclass User(AbstactUser): avatar = models.ImageField(upload_to ='url',default ='avatar/default.png',max_length =200,) QQ = models.CharField() 自定义user model需要在setting中添加参数：AUTH_USER_MODEL= &#39;blog.User&#39;，可以继承abstactUser，或关联外键扩展。 mysql配置 settings: 123456789101112DATABASES = &#123; 'default': &#123; #配置别忘了用逗号，否则不被识别为元组 'ENGINE': 'django.db.backends.mysql', 'NAME': 'blogdb', #开发环境可用，生产环境不要用 'USER': 'root', 'PASSWORD': '123456', 'HOST': '', 'PORT': '',#默认3306 &#125;&#125; 在mysql中创建blogdb数据库，utf-8编码。 富文本编辑框：使用kindeditor，下载kindeditor，解压文件放到/static/js/kindeditor 下面。定义媒体文件 1234class Media： js = ('/static/js/kindeditor-4.10/kindeditor-min.js', '/static/js/kindeditor-4.10/lang/zh-CN.js', '/static/js/kindeditor-4.10/config.js',) 注意新建配置文件config.js，参考官方文档：K.create(‘#id’),#id 是对应html页面中需要富文本编辑器的网页元素 12345678KindEditor.ready(function(K) &#123; K.create('textarea[name=content]',&#123; width:'800px', height:'200px', //配置上传地址，这个地址在url.py中已经配置好了，要和它对应 uploadJson: '/admin/upload/kindeditor', &#125;); &#125;); 需要自己定义文件上传接口。配置: 根目录下创建uploads，在settings中配置MEDIA_URL和MEDIA_ROOT 配置文件上传 123#上传图片设置MEDIA_URL = '/uploads/'MEDIA_ROOT = os.path.join(BASE_DIR,'uploads') 路由设置： 123456#配置用于处理图片上传的url映射url(r"^uploads/(?P&lt;path&gt;.*)$", \ #django.views.static.serve专门用于处理静态文件 "django.views.static.serve", \ #这里用到了settings中配置好的路径MEDIA_ROOT &#123;"document_root": settings.MEDIA_ROOT,&#125;), 配置富文本编辑器的文件上传 12345from blog.upload import upload_image#用于映射富文本编辑器的图片上传url(r'^admin/upload/(?P&lt;dir_name&gt;[^/]+)$', upload_image,\ name='upload_image'), 自己实现上传处理函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# -*- coding: utf-8 -*-from django.http import HttpResponsefrom django.conf import settingsfrom django.views.decorators.csrf import csrf_exemptimport osimport uuidimport jsonimport datetime as dt#这个装饰器用于不再进行表单验证提交@csrf_exemptdef upload_image(request, dir_name): ################## # 这是kindeditor想要的格式 # kindeditor图片上传返回数据格式说明： # &#123;"error": 1, "message": "出错信息"&#125; # &#123;"error": 0, "url": "图片地址"&#125; ################## result = &#123;"error": 1, "message": "上传出错"&#125; #imgFile来自于富文本编辑器查看源码之后找到的它定义的文件名字 files = request.FILES.get("imgFile", None) if files: result =image_upload(files, dir_name) return HttpResponse(json.dumps(result), content_type="application/json")#目录创建def upload_generation_dir(dir_name): today = dt.datetime.today() dir_name = dir_name + '/%d/%d/' %(today.year,today.month) if not os.path.exists(settings.MEDIA_ROOT + dir_name): os.makedirs(settings.MEDIA_ROOT + dir_name) return dir_name# 图片上传def image_upload(files, dir_name): #允许上传文件类型 allow_suffix =['jpg', 'png', 'jpeg', 'gif', 'bmp'] file_suffix = files.name.split(".")[-1] if file_suffix not in allow_suffix: return &#123;"error": 1, "message": "图片格式不正确"&#125; relative_path_file = upload_generation_dir(dir_name) path=os.path.join(settings.MEDIA_ROOT, relative_path_file) if not os.path.exists(path): #如果目录不存在创建目录 os.makedirs(path) file_name=str(uuid.uuid1())+"."+file_suffix path_file=os.path.join(path, file_name) file_url = settings.MEDIA_URL + relative_path_file + file_name #写入操作，二进制形式，最终完成上传，真正保存图片 open(path_file, 'wb').write(files.file.read()) return &#123;"error": 0, "url": file_url&#125; 配置好路由，在kindeditor的config js中配置上传路径： 12345678KindEditor.ready(function(K) &#123; K.create('textarea[name=content]',&#123; width:'800px', height:'200px', //配置上传地址，这个地址在url.py中已经配置好了，要和它对应 uploadJson: '/admin/upload/kindeditor', &#125;); &#125;); ​ 3、模板设计base.html保留不变的内容，变化的部分以\{\% block XXX \%\} \{\% endblock \%\}代替，子模板继承 base.html,\{\% extends &#39;base.html&#39;},然后重写\{\% block XXX \%\} \{\% endblock \%\}，在{\% block XXX \%} 中间添加变化的内容 {\% endblock \%}。也可将变化的部分单独放入一个文件中，使用 \{\% include &#39;XXX.html&#39; \%\} 4 、导航栏数据获取假如以category模型设计导航栏，先获取对象列表，传递变量给模板，然后在模板中展示123456# views 中from models import *class index(request)： ca_list = category.objects.all() return render(request,'base.html',&#123;'ca_list':ca_list&#125;) 在模板中 12345&#123;% raw %&#125;&#123;% for ca in ca_list %&#125;&lt;a herf =''&gt;&#123;&#123;ca.name&#125;&#125; &lt;/a&gt;&#123;% endfor %&#125;&#123;% endraw %&#125; 5、其他 locals() 将所有变量封装传递进模板，render(&#39;index.html&#39;,locals()) 代码重构：使用模板处理器，上面的global_setting的例子，将每个视图都用到的模板变量数据定义到模板处理器中。 聚合查询 annotate 异常捕获 1234try ： get_article()except Artical.NotExist: xxx 标签过滤器 {{update_date|date:'Y-m-d'}}safe 不转义变量文本里的html便签，确定变量安全的可以用该过滤器。 csrf表单验证 模板页面表单处需要 \{\% csrf_token \%\},防止跨站提交的安全手段。 不使用csrf(不推荐)，post方法使用csrf_exmp装饰器。]]></content>
      <categories>
        <category>技术</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 学习笔记(一)]]></title>
    <url>%2F2016%2F08%2F13%2F%E6%8A%80%E6%9C%AF%2Fdjango%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%2F</url>
    <content type="text"><![CDATA[Table of Contents generated with DocToc Django 学习笔记 1、快速开始 1.1、创建项目 1.2、创建应用 1.3、 Django Admin 2、记录踩过的坑 Django 学习笔记1 基于python2.7和django1.10 django工作流程： 1、快速开始1.1、创建项目 django-admin.py startproject website manage.py startapp blog 修改webisite下的setting.py urls.py 1234567891011# setting 修改# installed_apps 添加你的app#To include the app in our project, we need to add a reference to its configuration class in the INSTALLED_APPS setting. The PollsConfigclass is in the polls/apps.pyfile, so its dotted pathis ’blog.apps.PollsConfig’. Edit the mysite/settings.pyfile and add that dotted path to the INSTALLED_APPSsetting.INSTALLED_APPS = [...XXX..., ’blog.apps.PollsConfig’, ]language zh-HANStime-zone zh/Shanghai# 数据库连接配置 1234567# website/urls.py修改from django.conf.urls import include, url # 注意导入includefrom django.contrib import adminurlpatterns = [ url(r'^blog/', include('blog.urls')), # include app的urls规则 url(r'^admin/', admin.site.urls), # 只有admin例外，可以不用include] ​ 1.2、创建应用 manage.py startapp blog 创建视图 1234# 编辑文件 blog/views.pyfrom django.http import HttpResponsedef index(request):return HttpResponse("Hello, world. You're at the polls index.") views可以获取url中的参数作为变量使用：url(r&#39;^(?P&lt;question_id&gt;[0-9]+)/vote/$&#39;, views.vote, name=&#39;vote&#39;),通过urls中的正则表达式匹配的命名变量: %question_id view视图模板 模板文件存放在 app/templates/app/下。在view文件中加载渲染的模板： 12345678910from django.http import HttpResponsefrom django.template import loaderfrom .models import Questiondef index(request):latest_question_list = Question.objects.order_by('-pub_date')[:5]template = loader.get_template('polls/index.html')context = &#123;'latest_question_list': latest_question_list,&#125;return HttpResponse(template.render(context, request)) 或直接使用render()函数渲染,这样就不用Httpresponse和loader了。 123456from django.shortcuts import renderfrom .models import Questiondef index(request):latest_question_list = Question.objects.order_by('-pub_date')[:5]context = &#123;'latest_question_list': latest_question_list&#125;return render(request, 'blog/index.html', context) 模板中的url软编码在urls.py中每个url匹配规则可以指定一个name参数。在模板中使用{% url 'name' %}引用URL， e.g. {{ question.question_text }} 这样，当url映射改变时，无需修改模板文件。当app很多时，在urls.py中指定url命名空间，即app_name=’XXX’,然后模板引用url时，使用{% url 'blog:detail' question_id %} 模板中的表单和view中对表单数据的处理 404视图使用try catch或get_XXX_or_404方法，推荐后者 123456789try: question = Question.objects.get(pk = question_id) except Question.DoesNotExist: raise Http404("Questin does not exist!") return render(request, 'blog/detail.html', context=&#123;'question':question&#125;) ### 或者 no use try ,we can do this : question = get_object_or_404(Question, pk=question_id) return render(request, 'polls/detail.html', &#123;'question': question&#125;) 使用generic views 12345678from django.views import genericclass IndexView(generic.ListView): template_name = 'blog/index.html' context_object_name = 'latest_question_list' def get_queryset(self): """Return the last five published questions.""" return Question.objects.order_by('-pub_date')[:5] url映射规则 12345678#编辑 blog/urls.pyfrom django.conf.urls import urlfrom . import viewsurlpatterns = [url(r'^$', views.index, name='index'),]# name参数可以在模板文件中以&#123;% raw %&#125;&#123;% url name %&#125;&#123;% endraw %&#125; 引用。# 然后将其include到website下的urls.py内 数据库设置 123#修改settings.py，配置数据库连接# 然后执行命令创建数据库manage.py migrate 创建模式 编辑 blog/models.py 123456789from django.db import modelsclass Question(models.Model): question_text = models.CharField(max_length=200) pub_date = models.DateTimeField('date published')class Choice(models.Model): question = models.ForeignKey(Question,on_delete=models.CASCADE) choice_text = models.CharField(max_length=200) votes = models.IntegerField(default=0) 激活models，执行manage.py makemigrations blog，最后应用到数据库，manage migrate。 总结：• Change your models (in models.py).• Run python manage.py makemigrations to create migrations for those changes• Run python manage.py migrate to apply those changes to the database. views中引用数据库中的数据 获得全部对象： 12345678910from .models import Questionquestion_list = Question.object.all()l1=Question.objects.filter(question_text = u'what 你好') # 对象数据过滤器，等于Question.objects.filter( age__gt = 16) # 属性age &gt; 16Question.objects.filter( age__gte = 16) # 属性age &gt;= 16Question.objects.filter( name__contains = '张') # 属性name中含有张。# 批量更新,无需调用saveQuestion.objects.filter( age__gt = 16).update(name = 'zhang')# 删除调用.delete()方法 filter过滤表达式 123456ques = Question.object.get(id = 2)ques.name = 'a'ques.date = todayques.save() #同步修改# 删除调用.delete()方法ques.delete() 获得单个对象 添加一个新对象数据 12newQ = Question(name = 'new',age =16)newQ.save() 实现实体对应关系一对一和多对多 待补充 1.3、 Django Admin create admin user 1234python manage.py createsuperusername:adminmail:password:**** ​启动server,访问 http://127.0.0.1:8000/admin/ 在admin界面注册app 的object 打开的admin管理界面默认是没有blog app数据的编辑 blog/admin.py 1234from django.contrib import admin# Register your models here.from .models import Questionadmin.site.register(Question) 个性化 app static文件夹，blog\static\blog\ 下存放css文件，模板文件中引用css和图片使用相对路径。 blog\static\blog\images\下存放图片文件，css中和模板中引用图片使用相对路径。 django favicon.ico 处理：https://pypi.python.org/pypi/django-favicon 从url传值 get 12id = request.GET['id']passwd = request.POST['password'] /argv1/argv2/… 123456views.pydef wanda(request,wanda): return HttpResponse(r' `&lt;h4&gt; %s &lt;/h4&gt;`' % wanda)urls.pyurl(r'wanda/(?P&lt;wanda&gt;\d&#123;4&#125;)/$',views.wanda), 其中变量wanda是在url正则表达式中定义的分组变量名称。如果不定义?P&lt;name&gt;,则在view视图函数中可以传入任意变量名，按对应顺序从URL匹配中取值。 2、记录踩过的坑 object has no attribute ‘_state’： django不使用init方法，参考文档creating-objects部分。 IOError: No translation files found for default language zh-CN. 经确认是新版本的django包版本中只有zh_Hans目录，没有zh_CN,把zh_Hans目录复制一个zh_CN就Ok了或者在settings里面直接改成zh-Hans，这样就不用升级完Django，还去改目录了。 debug=False后，网站无法访问静态资源图片等 需要在启动参数中添加insecure manage runserver 0.0.0.0:8080 --insecure]]></content>
      <categories>
        <category>技术</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PhantomJS + Selenium实现登陆网站签到]]></title>
    <url>%2F2016%2F08%2F10%2F%E6%8A%80%E6%9C%AF%2FPhantomJS%20%2B%20Selenium%E5%AE%9E%E7%8E%B0%E7%99%BB%E9%99%86%E7%BD%91%E7%AB%99%E7%AD%BE%E5%88%B0%2F</url>
    <content type="text"><![CDATA[PhantomJS + Selenium实现登陆网站签到 js脚本是由浏览器前端加载执行的，因此爬虫要想渲染js，就要实现js引擎，或者我们分析出js发送的请求后，自己构造(很难，复杂)。 另外一种，就是使用selenium调用浏览器自动化处理。phantomjs是一个无头浏览器，没有界面，因此运行速度要比chrome快一点，没有弹窗干扰。 requests，urllib之类的http库，只能将http资源请求下来，其中的img link js标签都不会自动加载。而浏览器会请求所有的资源。 目标网站分析 网站yrw.com,登陆后签到页面是一个js脚本控制的插件,ipinyou.com估计是显示签到效果的 1234567891011121314151617181920 &lt;script type="text/javascript"&gt; var _py = _py || []; var _userId = "0"; var _source = "0"; var _pv = "0"; _py.push(['a', 'qJ..OIEiS_boFsG_SD2lEUB5nX']); _py.push(['domain', 'stats.ipinyou.com']); _py.push(['e', "&#123;\"userId\":\"" + _userId + "\",\"source\":\"" + _source + "\"&#125;"]); _py.push(['pv', _pv]); -function (d) &#123; var s = d.createElement('script'), e = document.body.getElementsByTagName('script')[0]; e.parentNode.insertBefore(s, e), f = 'https:' == location.protocol; s.src = (f ? 'https' : 'http') + '://' + (f ? 'fm.ipinyou.com' : 'fm.p0y.cn') + '/j/adv.js'; &#125;(document);&lt;/script&gt;&lt;noscript&gt; &lt;img src="//stats.ipinyou.com/adv.gif?a=qJ..OIEiS_boFsG_SD2lEUB5nX&amp;e=" style="display:none;"/&gt;&lt;/noscript&gt; 最初用fiddler抓包，发现请求的链接很复杂，不好构造，于是决定用selenium操作浏览器。后来用Chrome的XHR发现了请求链接，囧。 1234567891011121314151617Request-Headers:GET /member/check/?_=1482648421735 HTTP/1.1Host: www.yrw.comConnection: keep-aliveAccept: application/json, text/javascript, */*; q=0.01X-Requested-With: XMLHttpRequestUser-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36Referer: https://www.yrw.com/member/homeAccept-Encoding: gzip, deflate, sdch, brAccept-Language: zh-CN,zh;q=0.8Cookie: JSESSIONID=99ED09E3560F9F944EEAE855532F8684; CNZZDATA1253600367=1638558785-1463053615-%7C1482644670Request-url:https://www.yrw.com/member/check/?_=1482648421735Response-json:&#123;&quot;error&quot;:false,&quot;page&quot;:null,&quot;result&quot;:&#123;&quot;checkDate&quot;:1482648441429,&quot;checkSource&quot;:0,&quot;createTime&quot;:null,&quot;gainPopularity&quot;:2,&quot;id&quot;:null,&quot;memberId&quot;:110850411330,&quot;popularityDouble&quot;:1&#125;,&quot;resultCode&quot;:null,&quot;resultCodeEum&quot;:null,&quot;resultCodeList&quot;:[],&quot;resultList&quot;:null,&quot;success&quot;:true&#125; 签到脚本12345678910111213141516171819202122232425# coding:utf-8from selenium import webdriverimport time # 要注意等待浏览器加载页面完成browser = webdriver.PhantomJS(executable_path=r"E:\C\python\py\pantomjs_selenuim\bin\phantomjs.exe")url = r'http://www.yrw.com/'a = browser.get(url)login_url = r'http://www.yrw.com/security/login/'browser.get(login_url)username= browser.find_elements_by_id('j-cpn2') # find_elements方法返回的列表， find_element返回找到的第一个passwd=browser.find_elements_by_name('password')submit = browser.find_elements_by_id('j-login-submit')username[0].send_keys('username') # 填用户名passwd[0].send_keys('password') # 填密码submit[0].click() # 点击登陆按钮time.sleep(5)page1 = browser.page_sourcebrowser.save_screenshot('login.jpg')browser.find_element_by_id("j-checkin-btn").click()time.sleep(5)page2 = browser.page_source # 打印网页源码browser.save_screenshot('sign.jpg')browser.quit() # 关闭phantom浏览器print 1 使用requests模拟签到 找到了js请求的url：https://www.yrw.com/member/check/?_=1482648421735 12345678910111213141516171819202122232425# 主要是想验证签到的url是否正确，登陆部分直接用的cookie，模拟登陆也很简单，post登陆表单即可，无验证码import requestssign_url = r'https://www.yrw.com/member/check/?_=1482648421735' # 数字含义应该是时间戳，不影响url访问cookie_string = r'JSESSIONID=6F98E86EBE571D3233EE14B986320E86; CNZZDATA1253600367=1638558785-1463053615-%7C1482717076'headers = &#123; 'Host': 'www.yrw.com', 'Connection': 'keep-alive', 'Accept': 'application/json, text/javascript, */*; q=0.01', 'X-Requested-With': 'XMLHttpRequest', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36', 'Referer': 'https://www.yrw.com/member/home', 'Accept-Encoding': 'gzip, deflate, sdch, br', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Cookie': cookie_string&#125;s = requests.Session()s.headers = headersresponse = s.get(sign_url)q = response.json()if q['success']: print u'签到成功,获得积分%d点' % q['result']['gainPopularity']else: print u'签到失败']]></content>
      <categories>
        <category>技术</category>
        <category>python 爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Selenium</tag>
        <tag>Phantomjs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫笔记(基于python2)]]></title>
    <url>%2F2016%2F08%2F09%2F%E6%8A%80%E6%9C%AF%2Fpython2%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[基于python2.7 urllib2库的用法基础用法 urlopen方法打开网页 传入3个参数，data和timeout不是必须的；返回网页的源码可用read方法读出。 123response = urlopen(url,data,timeout)content = response.read() urlopen()本质上接收一个request对象，返回response对象。构建request对象：request = urllib2.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False) 12345import urllib2request = urllib2.Request(&quot;http://www.baidu.com&quot;)response = urllib2.urlopen(request)print response.read() post方式请求数据 post提交的数据不会在网址中显示 123456789import urllibimport urllib2values = &#123;&quot;username&quot;:&quot;xxxx@qq.com&quot;,&quot;password&quot;:&quot;XXXX&quot;&#125;data = urllib.urlencode(values) url = &quot;https://passport.csdn.net/account/login?from=http://my.csdn.net/my/mycsdn&quot;request = urllib2.Request(url,data)response = urllib2.urlopen(request)print response.read() get方式请求数据 get方式提高的数据直接包含在网址当中 12345678910111213import urllibimport urllib2values=&#123;&#125;values[&apos;username&apos;] = &quot;1016903103@qq.com&quot;values[&apos;password&apos;]=&quot;XXXX&quot;data = urllib.urlencode(values) url = &quot;http://passport.csdn.net/account/login&quot; ### 数据拼接到url中geturl = url + &quot;?&quot;+data request = urllib2.Request(geturl)response = urllib2.urlopen(request)print response.read() 高级用法 下面来说一说urllib2中的两个重要概念：Openers和Handlers。1.Openers：当你获取一个URL你使用一个opener(一个urllib2.OpenerDirector的实例)。正常情况下，我们使用默认opener：通过urlopen。但你能够创建个性的openers。2.Handles：Openers使用处理器handlers，所有的“繁重”工作由handlers处理。每个handlers知道如何通过特定协议打开URLs，或者如何处理URL打开时的各个方面。例如HTTP重定向或者HTTP cookies headers属性模拟浏览器身份 User-Agent : 通常会通过该值来判断是否是浏览器发出的请求Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用application/json ： 在 JSON RPC 调用时使用application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务referer：有些网站会检测该值是否为自身，防盗链。 在构建request对象时设置headers 12345headers = &#123; &apos;User-Agent&apos; : &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos; , &apos;Referer&apos;:&apos;http://www.zhihu.com/articles&apos; &#125;request = urllib2.Request(url, data, headers) response = urllib2.urlopen(request) page = response.read() 用自建的opener()中addheaders属性加入headers参数： 1234567891011121314user_agents = [ &apos;Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11&apos;, &apos;Opera/9.25 (Windows NT 5.1; U; en)&apos;, &apos;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)&apos;, &apos;Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)&apos;, &apos;Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12&apos;, &apos;Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9&apos;, &quot;Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7&quot;, &quot;Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0 &quot;, ] agent = random.choice(user_agents) opener.addheaders = [(&quot;User-agent&quot;,agent),(&quot;Accept&quot;,&quot;*/*&quot;),(&apos;Referer&apos;,&apos;http://www.google.com&apos;)] Proxy代理设置 在创建opner时传入Proxy handler ，urllib2.build_opener(proxy_handler)或在一个opener实例中调用opener.add_handler(proxy_handler)方法传入 创建proxy handler对象 1proxy_handler = urllib2.ProxyHandler(&#123;&quot;http&quot; : &apos;http://some-proxy.com:8080&apos;&#125;) 超时设置timeout 可以设置等待多少秒无响应即为超时，在urlopen(url,data,timeout=10)中设置 http的put和delete方法 http协议有6种请求数据的方法，除了最常用get和post，还有head，put，delete，options；PUT：这个方法比较少见。HTML表单也不支持这个。本质上来讲， PUT和POST极为相似，都是向服务器发送数据，但它们之间有一个重要区别，PUT通常指定了资源的存放位置，而POST则没有，POST的数据存放位置由服务器自己决定。DELETE：删除某一个资源。基本上这个也很少见，不过还是有一些地方比如amazon的S3云服务里面就用的这个方法来删除资源。 123# 在创建request对象时指定put或delete方法request = urllib2.Request(url,data,headers=&#123;&#125;)request.get_method = lambda: &apos;PUT&apos; # or &apos;DELETE&apos; 使用DebugLog 该功能可以将收发包的内容打印出来，不常用。 123httpHandler = urllib2.HTTPHandler(debuglevel=1)httpsHandler = urllib2.HTTPSHandler(debuglevel=1)opener = urllib2.build_opener(httpHandler, httpsHandler) cookie保持登陆在opener中绑定处理cookie对象的handler，即可捕获cookie并在后续的请求中使用。 cookielib模块 cookielib模块提供可存储的cookie对象，配合urllib2使用。cookielib提供的主要对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。 保存cookie到变量中，使用cookiejar()对象 12345678910import urllib2import cookielibcookie = cookielib.CookieJar()cookie_handler = urllib2.HTTPCookieProcessor(cookiejar=cookie)opener = urllib2.build_opener(cookie_handler)opener.open(url) # ...for item in cookie : print item.name + item.value 保存cookie到文件中， 使用FileCookieJar()对象及其子类 MozillaCookieJar和LWPCookiejar。 123456789import urllib2import cookielibcookie_file = &apos;cookie.txt&apos;cookie_jar=cookielib.MozillaCookieJar(filename=cookie_file)cookie_handler = urllib2.HTTPCookieProcessor(cookiejar=cookie_jar)opener = urllib2.build_opener(cookie_handler)opener.open(&apos;http:\\www.baidu.com&apos;)cookie_jar.save(ignore_discard=True, ignore_expires=True) # 即使将废弃的cookie也保存，覆盖cookie文件内容 从文件中加载cookie cookiejar对象的load方法 1234567891011import urllib2import cookielibcookie_file = &apos;cookie.txt&apos;cookie_jar=cookielib.MozillaCookieJar()### load()方法cookie_jar.load(filename=cookie_file, ignore_discard=True, ignore_expires=True)cookie_handler = urllib2.HTTPCookieProcessor(cookiejar=cookie_jar)opener = urllib2.build_opener(cookie_handler)resp=opener.open(&apos;http:\\www.baidu.com&apos;)print resp.read() 实例 登陆小说网站 166zw.com 12345678910111213141516171819202122232425262728293031323334353637383940import urllib2import urllibimport cookielib########################################################################class Browser(object): &quot;&quot;&quot; 创建一个有cookie和headers的opener对象,带有异常处理 &quot;&quot;&quot; #---------------------------------------------------------------------- def __init__(self): &quot;&quot;&quot;Constructor&quot;&quot;&quot; cookie_handler = urllib2.HTTPCookieProcessor(cookiejar=cookielib.CookieJar( )) self.opener = urllib2.build_opener(cookie_handler) self.opener.addheaders = [(&quot;User-agent&quot;, &apos;Opera/9.25 (Windows NT 5.1; U; en)&apos;,),(&quot;Accept&quot;,&quot;*/*&quot;),(&apos;Referer&apos;,&apos;http://www.google.com&apos;)] def openurl(self,url,data=None,timeout=10): try: response = self.opener.open(url,data,timeout) except urllib2.URLError, e: print e.code,&apos;\n&apos;,e.reason return response postData=urllib.urlencode(&#123;&apos;username&apos;:&apos;wocaonima&apos;,\ &apos;password&apos;:&apos;******&apos;,\ &apos;usecookie&apos;:&apos;1&apos;,\ &apos;submit.x&apos;:&apos;25&apos;,\ &apos;submit.y&apos;:&apos;5&apos;,\ &apos;action&apos;:&apos;login&apos;&#125;)loginUrl=r&apos;http://www.166zw.com/loginframe.php&apos;html=Browser().openurl(loginUrl,postData)print html.code,html.msg,html.infocontent= html.read()print content ### 打印的页面含有用户名信息，表明登陆成功 URLError异常处理 http协议状态码 服务器返回的响应请求，包含一个状态码。urllib2.HTTPError可以捕获 100：继续 客户端应当继续发送请求。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。101： 转换协议 在发送完这个响应最后的空行后，服务器将会切换到在Upgrade 消息头中定义的那些协议。只有在切换新的协议更有好处的时候才应该采取类似措施。102：继续处理 由WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。200：请求成功 处理方式：获得响应的内容，进行处理201：请求完成，结果是创建了新资源。新创建资源的URI可在响应的实体中得到 处理方式：爬虫中不会遇到202：请求被接受，但处理尚未完成 处理方式：阻塞等待204：服务器端已经实现了请求，但是没有返回新的信 息。如果客户是用户代理，则无须为此更新自身的文档视图。 处理方式：丢弃300：该状态码不被HTTP/1.0的应用程序直接使用， 只是作为3XX类型回应的默认解释。存在多个可用的被请求资源。 处理方式：若程序中能够处理，则进行进一步处理，如果程序中不能处理，则丢弃301：请求到的资源都会分配一个永久的URL，这样就可以在将来通过该URL来访问此资源 处理方式：重定向到分配的URL302：请求到的资源在一个不同的URL处临时保存 处理方式：重定向到临时的URL304：请求的资源未更新 处理方式：丢弃400：非法请求 处理方式：丢弃401：未授权 处理方式：丢弃403：禁止 处理方式：丢弃404：没有找到 处理方式：丢弃500：服务器内部错误 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器端的源代码出现错误时出现。501：服务器无法识别 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。502：错误网关 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。503：服务出错 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。 URLError HTTPError是URLError的子类,所以try… except…时应先捕获子类，子类捕获不到再捕获父类错误。 1234567891011import urllib2req = urllib2.Request(&apos;http://blog.csdn.net/cqcre&apos;)try: urllib2.urlopen(req)except urllib2.HTTPError, e: print e.codeexcept urllib2.URLError, e: print e.reasonelse: print &quot;OK&quot; 直接捕获一个URLError，如果含有code和reason属性，则说明是一个HTTPError。 123456789101112import urllib2req = urllib2.Request(&apos;http://blog.csdn.net/cqcre&apos;)try: urllib2.urlopen(req)except urllib2.URLError, e: if hasattr(e,&quot;code&quot;): print e.code,e.reason else: print eelse: print ok 正则表达式 和 bs4正则表达式 正则表达式用来匹配符合特定规则的字符串，类似于数学表达式是一种逻辑公式，实现对字符串的过滤匹配。 正则表达式的语法规则 贪婪模式和转义字符 python默认使用贪婪模式匹配查找字符串，即总是尝试匹配尽可能多的字符，非贪婪模式则相反。举例：模式ab 在abbbc字符串中将匹配到3个b，即abbb;非贪婪模式ab? 在abbbc字符串中将匹配到0个b，即a; 转义字符为\，很多编程语言也用\做转义字符，那么编程语言里的正则表达式想要匹配“\”就得使用4个‘\’,即“\\”。先在编程语言环境中转义得到“\”,然后提供给正则表达式。好在python有原生字符串的表示方法，即不转义任何’\’,而将其作为字符’\’,比如 print r’\s\%\‘ 执行后得到的结果就是 \s\%\ 。 re模块 re模块提供正则表达式引擎 pattern 是re 匹配模式的对象，由正则表达式字符串预编译得到。 12import repattern = re.compile(r&apos;\d&apos;) re主要的方法如下： 12345678910#返回pattern对象,该对象包含match，findall等方法p = re.compile(string[,flag]) #以下为匹配所用函数，也可传入pattern字符串，re会先执行compile编译正则表达式字符串生成pattern对象re.match(pattern, string[, flags])re.search(pattern, string[, flags])re.split(pattern, string[, maxsplit])re.findall(pattern, string[, flags])re.finditer(pattern, string[, flags])re.sub(pattern, repl, string[, count])re.subn(pattern, repl, string[, count]) flag参数可选值如下： 123456• re.I(全拼：IGNORECASE): 忽略大小写（括号内是完整写法，下同）• re.M(全拼：MULTILINE): 多行模式，改变&apos;^&apos;和&apos;$&apos;的行为（参见上图）• re.S(全拼：DOTALL): 点任意匹配模式，改变&apos;.&apos;的行为• re.L(全拼：LOCALE): 使预定字符类 \w \W \b \B \s \S 取决于当前区域设定• re.U(全拼：UNICODE): 使预定字符类 \w \W \b \B \s \S \d \D 取决于unicode定义的字符属性• re.X(全拼：VERBOSE): 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。 re方法 re.match(pattern,string,flag)match返回第一次匹配成功的结果（match对象）或None。注意match是从字符串开头的第一个字符开始匹配。hello模式match 字符串say hello就会失败返回None。 1234567891011121314151617 import re# 匹配如下内容：单词+空格+单词+任意字符m = re.match(r&apos;(\w+) (\w+)(?P&lt;sign&gt;.*)&apos;, &apos;hello world!&apos;)print &quot;m.string:&quot;, m.stringprint &quot;m.re:&quot;, m.reprint &quot;m.pos:&quot;, m.posprint &quot;m.endpos:&quot;, m.endposprint &quot;m.lastindex:&quot;, m.lastindexprint &quot;m.lastgroup:&quot;, m.lastgroupprint &quot;m.group():&quot;, m.group() # 默认返回group(0),即整个匹配结果，group(n)可以输出第n个元组匹配的结果。print &quot;m.group(1,2):&quot;, m.group(1, 2)print &quot;m.groups():&quot;, m.groups()print &quot;m.groupdict():&quot;, m.groupdict()print &quot;m.start(2):&quot;, m.start(2)print &quot;m.end(2):&quot;, m.end(2)print &quot;m.span(2):&quot;, m.span(2)print r&quot;m.expand(r&apos;\g \g\g&apos;):&quot;, m.expand(r&apos;\2 \1\3&apos;) re.search(pattern, string[, flags])search()同match()方法相似，区别是match()仅从字符串的开头匹配，如果0位置失败，则匹配以失败结束。search同match有相同的属性和方法。 123456789101112 #导入re模块 import re# 将正则表达式编译成Pattern对象pattern = re.compile(r&apos;world&apos;)# 使用search()查找匹配的子串，不存在能匹配的子串时将返回None# 这个例子中使用match()无法成功匹配res = re.search(pattern,&apos;hello world!&apos;)if res: # 使用Match获得分组信息 print res.group()### 输出 #### world re.split(pattern, string[, maxsplit]) 按照能够匹配的子串将string分割后返回列表。maxsplit用于指定最大分割次数，不指定将全部分割。 123456 import re pattern = re.compile(r&apos;\d+&apos;) print re.split(pattern,&apos;one1two2three3four4&apos;) ### 输出 #### [&apos;one&apos;, &apos;two&apos;, &apos;three&apos;, &apos;four&apos;, &apos;&apos;] re.findall(pattern, string[, flags]) 搜索string，以列表形式返回全部能匹配的子串 re.finditer(pattern, string[, flags]) 搜索string，返回一个顺序访问每一个匹配结果（Match对象）的迭代器.迭代器使用for进行遍历。 re.sub(pattern, repl, string[, count]) 使用repl替换string中每一个匹配的子串后返回替换后的字符串。当repl是一个字符串时，可以使用\id或\g、\g引用分组，但不能使用编号0。当repl是一个方法时，这个方法应当只接受一个参数（Match对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。count用于指定最多替换次数，不指定时全部替换。 123456789101112131415import repattern = re.compile(r&apos;(\w+) (\w+)&apos;)s = &apos;i say, hello world!&apos;print re.sub(pattern,r&apos;\2 \1&apos;, s)def func(m): return m.group(1).title() + &apos; &apos; + m.group(2).title()print re.sub(pattern,func, s)### output #### say i, world hello!# I Say, Hello World! re.subn(pattern, repl, string[, count]) 返回 (sub(repl, string[, count]), 替换次数)。 使用pattern对象调用上述方法则不必传入pattern对象 常用匹配 ip 文件后缀 r&#39;http://img\.tupianzj\.com/uploads/allimg/\d+/.+\.(?:gif|jpg|png|bmp)&#39; BeautifulSoup BeautifulSoup是python的一个第三方库，用于解析网页并从中提取数据。 安装 pip install beautifulsoup4,或去官网下载安装包。 BeautifulSoup使用Python标准库中默认的HTML解析器，想要使用其他可选的第三方html解析器需要提前安装好。 lxml或html5lib。lxml解析器更强大速度更快。html5lib是纯python实现的，解析方式与浏览器相同。 pip install lxml 和 pip install html5lib windows下直接pip安装lxml失败，可以先pip install wheel ,然后去Python Extension Packages for Windows - Christoph Gohlke上下载对应系统的安装包，然后pip install c:\lxml-3.6.4-cp27-cp27m-win_amd64.whl其他pip无法直接安装的模块也可用这种方法。 官方文档 Beautiful Soup 4.4.0 文档 — beautifulsoup 4.4.0 文档 使用 1234567from bs4 import BeautifulSoupimport urllib2URL = r&quot;http://www.baidu.com&quot;content = urllib2.urlopen(URL).read()bsobj = BeautifulSoup(content) #传入html格式的字符串print bsobj.prettify() #打印出html文档树，格式化输出 bs将html文档转换为树形结构，每个节点都是一个对象，对象分为4种：Tag，NavigableString，BeautifulSoup，Comment。 Tag 即HTML中的标签，由&lt;&gt; &lt;/&gt;闭合。比如 News Today 获取标签方法如下，找不到返回None，这种方法只能查找符合条件的第一个标签。tag常用的属性有name，attrs，string。 12345678910111213print bsobj.title# &lt;title&gt;百度一下，你就知道&lt;/title&gt;print bsobj.title.string# 百度一下，你就知道print bsobj.div.attrs# &#123;&apos;id&apos;: &apos;wrapper&apos;&#125; 输出的是标签的属性print bsobj.div.name# div 标签的名字为对象本身的名字，bsobj对象的名字为`[document]`#获取某个属性的值print bsobj.div[&apos;id&apos;]wrapperprint bsobj.div.get(&apos;id&apos;)wrapper NavgableString 可以遍历的字符串，即标签闭合的内容：print bsobj.title.string #输出 百度一下，你就知道 BeautifulSoup BeautifulSoup对象表示一个文档的全部内容，也可以当做tag对象，只不过是包含很多子tag的特殊的tag Comment Comment 对象是一个特殊类型的 NavigableString 对象，但是使用ob.string输出的内容仍然不包括注释符号。 有必要时可进行类型判断 12 if type(soup.a.string)==bs4.element.Comment:print soup.a.string 遍历文档树 tag.contens 属性 bsobj.tag.contents 可以将tag的子节点以列表的形式输出。 tag.children 属性 bsobj.tag.children 是一个子节点的list生成器，可以用for循环遍历 tag.descendants 属性 bsobj.tag.descendants 是tag的子孙节点的list生成器，用for遍历。而contents和children只包含孩子节点（一级子节点），不包含孩子的孩子及后续孙子节点。 tag.string 属性，节点内容 如果tag只含有唯一的一个子tag，那么tag.string输出的是子tag的内容，不含子tag，则直接输出tag的内容；如果有多个子tag，则输出none。 tag.strings 和 tag.stripped_strings 获取多个内容 都需要用for循环遍历，.stripped_strings去除了多余空白内容。 tag.parent 属性 父节点 tag.parents 属性 全部父辈节点 通过递归得到tag全部的父辈节点，用for循环遍历 tag.next_sibling 和 tag.previous_sibling 兄弟节点 获取与tag同一级的节点，.next_sibling 属性获取了该节点的下一个兄弟节点，.previous_sibling 则与之相反，如果节点不存在，则返回 None注意：实际文档中的tag的 .next_sibling 和 .previous_sibling 属性通常是字符串或空白，因为空白或者换行也可以被视作一个节点，所以得到的结果可能是空白或者换行 .next_siblings .previous_siblings 属性 全部兄弟节点 .next_element .previous_element 属性 tag节点的前后节点，与兄弟节点不同，前后节点没有层级关系，可以是父子也可以是兄弟节点。 .next_elements .previous_elements 属性 搜索文档树 bsobj.find_all(name=None, attrs={}, recursive=True, text=None, limit=None)搜索当前tag的所有子tag，返回符合条件的结果 name 参数 查找所有名字满足name条件的tag。 1&gt;如果传入字符串，则搜索完全匹配该字符串的tag : 标签; 2&gt; 传正则表达式，搜索名字匹配正则的tag标签 3&gt; 传入name列表，返回所有与列表元素匹配的tag 4&gt; 传True，返回全部的任意tag，除字符串节点。 5&gt; 传方法，该方法只接收一个参数，根据判断条件返回True 或False。find_all()返回的是满足True的全部节点 attrs 和 keyword 参数 attrs={‘id’:’123’,’color’:’red’},attrs接收一个字典，用来搜索含有指定属性的tag。也可以指定关键字参数，如 id=’today’,class_=’nostyle’,注意class是python保留字，所以bs使用class来做区别。注意 html5中的类似 data-*的参数不能用作关键字搜索，会报错。 text参数 可以匹配文档中的字符串内容，与name参数的可选值一样，接收 字符串，正则表达式，列表，True limit参数 limit = n ，限制返回的数量为n，即找到n个节点就停止。 recursive参数 递归参数默认为true，即搜索子孙节点。如果只搜索子节点怎改为false。 find( name , attrs , recursive , text , **kwargs )它与 find_all() 方法唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果 find_parents() 和 find_parent()find_all() 和 find() 只搜索当前节点的所有子节点,孙子节点等. find_parents() 和 find_parent() 用来搜索当前节点的父辈节点,搜索方法与普通tag的搜索方法相同,搜索文档搜索文档包含的内容 find_next_siblings() find_next_sibling()迭代对象为tag.next_siblings ，对tag 的所有后面解析的兄弟 tag 节点进行迭代, find_next_siblings() 方法返回所有符合条件的后面的兄弟节点,find_next_sibling() 只返回符合条件的后面的第一个tag节点 find_previous_siblings() 和 find_previous_sibling()迭代对象为.previous_siblings ，对当前 tag 的前面解析的兄弟 tag 节点进行迭代, find_previous_siblings() 方法返回所有符合条件的前面的兄弟节点, find_previous_sibling() 方法返回第一个符合条件的前面的兄弟节点 find_all_next() find_next()迭代对象为tag.next_elements，find_all_next() 方法返回所有符合条件的节点, find_next() 方法返回第一个符合条件的节点 find_all_previous() 和 find_previous()迭代对象为.previous_elements,find_all_previous() 方法返回所有符合条件的节点, find_previous()方法返回第一个符合条件的节点 Requests库的用法 Requests是第三方库，比urllib库更高级、抽象。也就是说使用更方便。 文档 Requests: HTTP for Humans — Requests 2.11.1 documentation 安装 pip install requests 初次相见html= requests.get(url),返回的是对象，具有以下属性或方法：’apparent_encoding’, ‘close’, ‘connection’, ‘content’, ‘cookies’, ‘elapsed’, ‘encoding’, ‘headers’, ‘history’, ‘is_permanent_redirect’, ‘is_redirect’, ‘iter_content’, ‘iter_lines’,‘json()’：解析json格式内容,‘links’, ‘ok’, ‘raise_for_status’,‘raw’：获得原始套接字,要在初始的请求中设置stream=True‘reason’, ‘request’, ‘status_code’ ：http响应状态码,‘text’:直接输出内容,‘url’ 基本http请求 requests实现了http的6种请求，get、post、delete、put、options、head。 GET请求方式requests.get(url, params=None),参数传入字典，比如{‘k1’:’v1’ , ‘k2’:’v2’}，则请求url自动编码为url?k1=v1&amp;k2=v2在请求中添加headers， 123456import requestspayload = &#123;&apos;key1&apos;: &apos;value1&apos;, &apos;key2&apos;: &apos;value2&apos;&#125;headers = &#123;&apos;content-type&apos;: &apos;application/json&apos;&#125;r = requests.get(&quot;http://httpbin.org/get&quot;, params=payload, headers=headers)print r.url post请求 12345678910import jsonpost_data =&#123;'key1': 'value1', 'key2': 'value2'&#125; res = requests.post(url, data=post_data, json=None)# 如果提交的信息不是表单形式，而是json格式的数据，可用json.dumps()把表单数据序列化,或使用json参数res = requests.post(url, data=json.dumps(post_data), json=None)res = requests.post(url, data=None, json=post_data)# 以上2者等效# 支持流式上传数据，只需传入一个file-like对象,不用file.read()加载内容至内存with open('test.file','r') as f : res = requests.post(url, data=f) Cookies 如果服务器返回的响应包含cookie，则我们得到的response对象就会保存该cookie，可以利用.cookies属性查看。使用cookie发送请求时只需在get/post方法中指定cookies参数要想使用cookie保持登陆，则需要一个session会话对象 123456789# 查看cookiesimport requestsurl1 = 'http://xxx.com'r = requests.get(url1)print r.cookies# 构造cookies并用来发送请求url2 = 'http://ooo.com'cookies = &#123;'id':'1234'&#125;r = requests.get(url2，cookies=cookies) 超时设置 timeout在get/post方法中传入timeout参数，仅对连接建立有效，与返回response全部数据的时间无关。 会话对象 session 每次直接调用requests.get/post方法都相当与建立了一个新的请求会话，相当于用不同的浏览器发起请求。因而无法使用cookie保持登陆状态。 123456s = requests.session()s.headers.update(&#123;(&quot;User-agent&quot;,&quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11&quot;),(&quot;Accept&quot;,&quot;*/*&quot;),(&apos;Referer&apos;,&apos;http://www.google.com &apos;)&#125;) # 此处设置的headers是全局变量，post/get方法传入的headers参数的同名变量会覆盖此处headers的变量，不使用某个参数可将其值设为None。# 不同名的参数会一块加入到请求的headers中生效.res = s.get(url)# 通过会话发起请求 SSL证书验证Requests可以为HTTPS请求验证SSL证书，就像web浏览器一样。要想检查某个主机的SSL证书，你可以在get/post方法中使用 verify 参数，默认为True，是检查的。如直接requests.get(‘https://www.12306.cn&#39;),就会出现证书无效的错误：requests.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)。使用requests.get(url=&apos;https://www.12306.cn&apos;,verify=False)取消证书检查。 代理在post/get方法中传入参数proxies： 1234proxies = &#123; &quot;https&quot;: &quot;http://41.118.132.69:4433&quot;&#125;r = requests.post(&quot;http://httpbin.org/post&quot;, proxies=proxies) 也可以设置环境变量HTTP_PROXY和HTTPS_PROXY来配置代理。 12export HTTP_PROXY=&quot;http://10.10.1.10:3128&quot;export HTTPS_PROXY=&quot;http://10.10.1.10:1080&quot; 参考：官方API文档 xpath语法与lxml库 lxml解析库使用xpath语法，beautifulSoup的内部实现是RExpath语法参考lxml官方文档 - Processing XML and HTML with Python xpath语法 XPath 是一门在 XML 文档中查找信息的语言。XPath 可用来在 XML 文档中对元素和属性进行遍历。XPath 是 W3C XSLT 标准的主要元素，并且 XQuery 和 XPointer 都构建于 XPath 表达之上。 xpath使用路径表达在xml中选取节点。节点的逻辑关系有：父节点Parent、先辈节点Ancestor、子节点Children、后代节点Descendant、兄弟/同胞节点sibling。 路径表达式：nodename: 选择nodename节点的所有子节点,/:从根路径选取,绝对路径，//:从当前路径下选取全部,不考虑位置，相对路径，.:选取当前节点,..:选取当前节点的父节点,@:选取属性. 谓语：用来查找某个特定的节点或者包含某个特定的值的节点。谓语嵌在方括号[]中。 | 路径表达式 | 结果 || /bookstore/book[1] | 选取属于 bookstore 子元素的第一个 book 元素。 || /bookstore/book[last()] | 选取属于 bookstore 子元素的最后一个 book 元素。 || /bookstore/book[last()-1] | 选取属于 bookstore 子元素的倒数第二个 book 元素。 || /bookstore/book[position()35.00] | 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。 || /bookstore/book[price&gt;35.00]/title | 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 | 选取未知节点： *:匹配任何节点; @*:匹配任何属性的节点; node():匹配任何类型的节点; 选取多个路径：|运算符，//book/title | //book/price xpath表达式运算符|:找到满足2个路径表达式之一的节点集合；+:加法-:减法*:乘法div:除法，取整 ; mod:取余数=:等于，比较，相等返回true，否则返回false！=:不等于，比较&lt;:小于 ； &lt;=:小于或等于&gt;:大于 ； &gt;=:大于或等于or: 或，连接2个布尔表达式，有一个为真返回trueand: 与，都为真才返回true lxml用法PhantomJS的用法Selenium的用法PyQuery的用法 PySpider框架]]></content>
      <categories>
        <category>技术</category>
        <category>python 爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程技巧]]></title>
    <url>%2F2016%2F08%2F07%2F%E6%8A%80%E6%9C%AF%2Fpython%E7%BC%96%E7%A8%8B%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[python编程技巧 x的n次方求解 ： x ** n 22]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 字符串技巧]]></title>
    <url>%2F2016%2F07%2F29%2F%E6%8A%80%E6%9C%AF%2Fpython-str%2F</url>
    <content type="text"><![CDATA[将 \u3232\u6674转换成unicode字符 运用str对象的decode方法 1234567#任何字符，都可以在：#http://unicodelookup.com/#中，查找到对应的unicode的值s=&apos;\u6210\u529f&apos;print s.decode(&apos;unicode-escape&apos;)...成功 写入unicode报错‘ascii’ codec can’t encode character in position 0-3。py2如果开头指定了utf-8，Python的解释器根据 “#encoding:utf-8” 来按utf-8的编码规则来理解这些字符码•当你把字符串写到文件中去时，因为你的字符串是 unicode，所以Python会（自动）先调用encode方法来编码unicode字符串，然后再写入文件•当调用encode方法时，因为没有指定编码格式，所以采用默认值 ascii，ascii并不能解释定义的utf-8中翻译的字符码，所以报错 今天遇到一个错误： UnicodeEncodeError: ‘ascii’ codec can’t encode characters in position 0-3 搜索网上找到一个解决办法（转载自 http://blog.sina.com.cn/s/blog_727b603701019pyl.html） 异常: ‘ascii’ codec can’t encode characters 字符集的问题，在文件前加两句话： reload(sys) sys.setdefaultencoding( “utf-8” ) 完美解决，]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python字典键值转置]]></title>
    <url>%2F2016%2F07%2F29%2F%E6%8A%80%E6%9C%AF%2Fpython-dict-reverse%2F</url>
    <content type="text"><![CDATA[dict的key与value转置 如果value1值唯一，则直接交换key1与value1的位置 1234from collections import OrderedDict as od_dict #生成有序的dictd= od_dict((v,k) for k,v in OID_FS_lst.items()) print d 如果value1不唯一，直接交换，则作为key2插入的value1值，会被相同的key2覆盖。也就说会丢失部分值。]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python创建项目]]></title>
    <url>%2F2016%2F07%2F28%2F%E6%8A%80%E6%9C%AF%2Fsomething-with-python-project%2F</url>
    <content type="text"><![CDATA[使用Cookiecutter模板创建Python项目样式 首先安装cookiecutter pip install -U cookiecutter 在github上找一个模板 在某个目录下执行 cookiecutter https://github.com/audreyr/cookiecutter-pypackage.git，使用找到的模板创建项目，输入相关参数 是否开源发布看个人 import自己的模块 首先.py文件和主文件在一个目录下 import name ,不要加.py,否则报错 Fatal error in launcher: Unable to create process using ‘“‘Windows新装的python3自带了easy_install和pip，但执行时会出错Fatal error in launcher: Unable to create process using &#39;&quot;&#39; ,1、确保环境变量中没空格。2、如果不是环境变量问题，则重装\升级easy_install和pip，python3 -m pip install -U pip,python3 -m pip install -U setuptools。问题解决。 pip安装模块失败的解决办法windows下直接pip安装lxml失败，可以先pip install wheel ,然后去Python Extension Packages for Windows - Christoph Gohlke上下载对应系统的安装包，然后pip install c:\lxml-3.6.4-cp27-cp27m-win_amd64.whl其他pip无法直接安装的模块也可用这种方法。 python2 Unicode和普通字符串之间转换问题描述：You need to deal with data that doesn’t fit inthe ASCII character set.解决： 将Unicode转换成普通的Python字符串:”编码(encode)” 12u_string = u&quot;你好&quot;utf8_s = u_string.encode(&apos;utf-8&apos;) ascii_s ,iso_s,utf16_s 对应的编码集分别为”ascii” “ISO-8859-1” “utf-16”将普通的Python字符串转换成Unicode: “解码(decode)” 1234plainstring1 = unicode(utf8string, &quot;utf-8&quot;) plainstring2 = unicode(asciistring, &quot;ascii&quot;) plainstring3 = unicode(isostring, &quot;ISO-8859-1&quot;)plainstring4 = unicode(utf16string, &quot;utf-16&quot;)]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统服务配置]]></title>
    <url>%2F2016%2F07%2F26%2F%E6%8A%80%E6%9C%AF%2Fconfig-of-Linux%2F</url>
    <content type="text"><![CDATA[查看内核版本1uname -r 稳定版本的偶数版，如2.6.x，适合于商业与家用环境使用；开发中版本，如2.5.x，适合开发特殊功能的环境。 安装内核文件：apt-get install linux-headers-$(uname -r) 修改源vim /etc/apt/sources.list deb http://http.kali.org/kali kali-rolling main contrib non-free ssh远程登陆设置允许root登陆ssh新装的系统可能需要要修改root密码gedit /etc/ssh/sshd_config修改 PermitRootLogin yes PermitRootLogin yesPermitEmptyPasswords no注意重启ssh service 设置系统开机启动sshd服务chkconfig sshd on 即可but debian8不行， 如果是debian 8，直接使用systemd，比如：sudo systemctl enable xxx.servicesudo systemctl disable xxx.service Mac：（若以root身份登陆，将username改为root） 登陆 1ssh root@IPaddress Linux：(若以root身份登陆，将username删掉）Linux默认root的提示符为#,而一般身份用户的提示符为$。 1su - username 使用ssh上传下载文件 很多系统初始不带这工具，但感觉比起ftp，sftp传输文件，还是方便很多的。 首先安装sz，rz命令行工具，apt-get install lrzsz。然后使用xshell或secureRT登陆服务器，putty好像不行。 上传 rz -E输入rz -E会自动弹出打开文件窗口，选择要上传的文件。 下载 sz file-path输入sz file，会弹出窗口选择file文件下载存放的位置。 FTP 服务配置参考链接:proftp 安装配置FTP服务器 切换root 安装proftpd，在确认安装中选Y，并选择《Standalone》安装apt-get install proftpd 安装完以后将实现先停掉，以方便改配置/etc/init.d/proftpd stop 或service proftpd stop 编辑proftpd的配置文件nano /etc/proftpd/proftpd.conf更改FTP根目录, 默认为: DefaultRoot ~，比如说改为:DefaultRoot /var/www允许匿名用户访问,找到配置文件中的 ““ 和 ““ 之间的部分，将注释移除即可。 保存配置文件 “/etc/proftpd.conf”,重启proftpd服务/etc/init.d/proftpd restart 为FTP服务器添加用户名、密码和读写权限 FTP服务的用户名密码其实就是拥有特定目录权限的linux用户及其密码，所以添加一个FTP用户并设置密码，用户信息即可adduser tester -home /var/www 还要为此用户添加FTP共享目录的读写权限，[直接改变所有权]chown tester /var/www [可选] 当用户非常多时，可以添加一个用户组，统一配置权限addgroup ftpuser并将tester添加进ftpuser组adduser tester ftpuser注* 删除用户和用户组deluser testerdelgroup tester安装VMware tools 修改防火墙端口 开启防火墙 UDP161端口 for snmp 查看防火墙配置：iptables –L –n 添加一条规则记录：iptables -I INPUT -p udp --dport 161 -j ACCEPT 保存规则使防火墙生效iptables-save(debian linux),ubuntu下的保存iptables命令iptables -save。 常用端口 21是指FTP默认端口、80是指web服务器端口、3306是指MySQL数据库链接端口，22是指SSH远程管理端口，9000到9045是FTP被动模式端口范围 修改DNS永久修改网卡DNSsudo –icd /etc/resolvconf/resolv.conf.dvim base添加如下内容nameserver 8.8.8.8nameserver 8.8.4.4 Linux下python2 与3共存 安装Python3后，建立ln，使用Python（Python2），Python3 来区分两个版本，使用sudo apt-get install python3-setuptools 安装Easy_install，再使用sudo easy_install3 pip 安装Pip，Pip 对应Python2，Pip3 对应Python3。Easy_Install 对应Python2，Easy_Install3 对应Python3. python使用virtualenv创建隔离环境 注销Linux注销Linux并不意味着关机，只是用户离开系统。 1exit 基础命令的操作1command [-options] parameter1 parameter2 ··· 123456echo $LANG #显示目前支持的语言LANG=en_US #将语言改为英文系date #显示日期与实践cal 10 2014 #显示日历bc #计算器quit #退出 重要的热键[tab]：连按两次，具有“命令补全”和“文件补齐”的作用。[control]+c：中断目前程序。[control]+d：键盘输入结束；直接离开文字界面（相当于`exit）。 在线求助 man page1man command #command是要查询的命令名称 进入man命令后，可按空格往下翻页，按q键离开。在man page中，可以在任何时候输入/keyword来查询关键字，比如/date. 正确的关机方法123who #查看目前有谁在线netstat -a #查看网络的联机状态ps -aux #查看后台执行的程序 数据同步写入磁盘：为了防止不正常关机导致的内存数据没有来得及写入磁盘，在文字界面输入 1sync 惯用的关机命令： 123456shutdown -h now #立刻关机shutdown -h 20:25 #晚上8点25分关机shutdown -h +10 #过十分钟后关机shutdown -r now #立刻重启shutdown -r +30 ‘The system will be reboot’ #再过30分钟关机，并显示后面的消息给所有在线用户shutdown -k now ‘The system will be reboot’ #仅发出警告，系统并不会真正关机 重启、关机：reboot，halt，poweroff。务必用man去查询一下。 压缩解压unrar x a.rar ./a/ 使用screen12345678910111213141516171819screen -S myjobSessionscreen -r sessionname/ID 恢复sessionscreen -lsexit 退出screen session，终止任务，ctrl+a+d 暂时离开/后台运行session，先按住ctrl，然后按一下a，按一下d远程演示首先演示者先在服务器上执行 screen -S test 创建一个screen会话，观众可以链接到远程服务器上执行screen -x test 观众屏幕上就会出现和演示者同步其它命令Ctrl + a，d #暂离当前会话Ctrl + a，c #在当前screen会话中创建一个子会话 Ctrl + a，w #子会话列表 Ctrl + a，p #上一个子会话 Ctrl + a，n #下一个子会话 Ctrl + a，0-9 #在第0窗口至第9子会话间切换有时在恢复screen时会出现There is no screen to be resumed matching ****，遇到这种情况咋办呢？输入命令screen -d ****]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 目录 文件 权限]]></title>
    <url>%2F2016%2F07%2F25%2F%E6%8A%80%E6%9C%AF%2Ffile-and-dir-of-Linux%2F</url>
    <content type="text"><![CDATA[Linux目录操作12345. # 代表此层目录.. # 代表上一层目录- # 代表前一个工作目录~ # 代表『目前使用者身份』所在的家目录~account # 代表 account 这个使用者的家目录(account是个帐号名称) 在所有目录底下都会存在的两个目录，分别是.与.. 根目录的上一层(..)与根目录自己(.)是同一个目录 几个常见的处理目录的命令 cd：变换目录，cd是Change Directory的缩写 pwd：显示目前的目录，pwd是Print Working Directory的缩写 mkdir：创建一个新的目录 rmdir：删除一个空的目录 mv：移动文件 1234pwd -P # -P：代表显示正确的完整路径，而不是连接路径mkdir -m xxx # -m：直接配置文件的权限mkdir -p test1/test2 # -p：直接将所需要的目录(包含上一级目录)递回创建起来！PATH=&quot;$PATH&quot;:/root # 将/root路径加入PATH环境变量中 文件与目录管理 文件与目录的检视： ls 复制、删除与移动： cp, rm, mv 1234567cp -a # 将文件的所有特性都一起复制过来cp -p # 连同文件的属性一起复制过去，而非使用默认属性(备份常用)cp -r # 可以复制目录，但是，文件与目录的权限可能会被改变rm -i # 互动模式，在删除前会询问使用者是否动作rm -r # 连目录下的东西一起删掉，并且不会询问，慎用mv -f # force强制移动，如果目标文件已经存在，不会询问而直接覆盖mv -i # 若目标文件 (destination) 已经存在时，就会询问是否覆盖 文件内容查询 直接检视文件内容： cat, tac, nl （常用） 可翻页检视： more, less （常用） 数据撷取： head, tail 非纯文字档： od 修改文件时间与建置新档： touch 12345678cat [-AbEnTv] filename # 由第一行开始显示文件内容。-b列出非空白行行号；-n列出所有行号。tac # 从最后一行开始显示文件内容，tac就是cat倒着写！nl # 显示文件内容，顺便输出行号more # 一页一页地显示文件内容less # 与more类似，但可以往前翻页head [-n number] # 只看文件头几行，默认是10行，number是自定义行数tail # 只看文件尾几行，文件很大的时候常用od # 以二进制方式读取文件内容 文件与目录的默认权限与隐藏权限 文件默认权限：umask 文件隐藏属性： chattr, lsattr 文件特殊权限：SUID, SGID, SBIT, 权限配置 观察文件类型：file 123umask # 后三位数是被拿走的权限分数，比如0022，u没有被拿走权限，g和o被拿走了w权限umask -S # 以符号类型来显示权限umask number # 配置自己需要的权限 在默认的情况中，root的umask会拿掉比较多的属性，root的umask默认是022， 这是基於安全的考量啦～至於一般身份使用者，通常他们的 umask 为002 ，亦即保留同群组的写入权力。 特殊权限s和t Set UID，简称SUID，当s标志在文件拥有者的x项目为SUID，对目录无效 Set GID，简称SGID，当s标志在群组的x项目为SGID，对目录有效 Sticky Bit, 简称SBIT，目前只针对目录有效，对於文件已经没有效果了 配置SUID,SGID,SBIT权限在原有的权限数字前面加上需要配置的权限数字。比如755-&gt;4755 ，就意味着-rwxr-xr-x变为了-rwsr-xr-x。 4 为 SUID 2 为 SGID 1 为 SBIT 123chmod 4755 filenamechmod u=rwxs,go=x test; ls -l test # 配置权限为-rws--x--x的模样chmod g+s,o+t test; ls -l test # 配置权限为-rws--s--t，即加入SGID,SBIT权限 ###命令与文件的搜寻 命令档名的搜寻：which 文件档名的搜寻：whereis, locate, find 权限与命令的关系一、让使用者能进入某目录成为『可工作目录』的基本权限为何： 可使用的命令：例如 cd 等变换工作目录的命令； 目录所需权限：使用者对这个目录至少需要具有 x 的权限 额外需求：如果使用者想要在这个目录内利用 ls 查阅档名，则使用者对此目录还需要 r 的权限。 二、使用者在某个目录内读取一个文件的基本权限为何？ 可使用的命令：例如本章谈到的 cat, more, less等等 目录所需权限：使用者对这个目录至少需要具有 x 权限； 文件所需权限：使用者对文件至少需要具有 r 的权限才行！ 三、让使用者可以修改一个文件的基本权限为何？ 可使用的命令：例如 nano 或未来要介绍的 vi 编辑器等； 目录所需权限：使用者在该文件所在的目录至少要有 x 权限； 文件所需权限：使用者对该文件至少要有 r, w 权限 四、让一个使用者可以创建一个文件的基本权限为何？ 目录所需权限：使用者在该目录要具有 w,x 的权限，重点在 w 啦！ 五、让使用者进入某目录并运行该目录下的某个命令之基本权限为何？ 目录所需权限：使用者在该目录至少要有 x 的权限； 文件所需权限：使用者在该文件至少需要有 x 的权限]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-磁盘与文件系统管理]]></title>
    <url>%2F2016%2F07%2F24%2F%E6%8A%80%E6%9C%AF%2Fdisk-and-fileSystem-management-of-Linux%2F</url>
    <content type="text"><![CDATA[###认识EXT2文件系统每种操作系统能够使用的文件系统并不相同。 举例来说，windows 98以前的微软操作系统主要利用的文件系统是FAT(或FAT16)，windows 2000以后的版本有所谓的NTFS文件系统，至于Linux的正统文件系统则为Ext2(Linux second extended file system, ext2fs)这一个。此外，在默认的情况下，windows操作系统是不会认识Linux的Ext2的。 那么文件系统是如何运行的呢？这与操作系统的文件数据有关。较新的操作系统的文件数据除了文件实际内容外，通常含有非常多的属性，例如Linux操作系统的文件权限(rwx)与文件属性(拥有者、群组、时间参数等)。 文件系统通常会将这两部份的数据分别存放在不同的区块，权限与属性放置到inode中，至于实际数据则放置到data block区块中。 另外，还有一个超级区块(superblock)会记录整个文件系统的整体信息，包括inode与block的总量、使用量、剩余量等。 ###文件系统的简单操作 ###磁盘的分割、格式化、检验与挂载 磁盘分区： fdisk, partprobe 磁盘格式化： mkfs, mke2fs 磁盘检验： fsck, badblocks 磁盘挂载与卸除： mount, umount 磁盘参数修订： mknod, e2label, tune2fs, hdparm ###配置启动挂载 启动挂载 /etc/fstab 及 /etc/mtab 特殊装置 loop 挂载(映象档不刻录就挂载使用) ###内存置换空间(swap)之建置 使用实体分割槽建置swap 使用文件建置swap swap使用上的限制 ###文件系统的特殊观察与操作 boot sector 与 superblock 的关系 磁盘空间之浪费问题 利用 GNU 的 parted 进行分割行为 ###重点回顾 基本上 Linux 的正统文件系统为 Ext2 ，该文件系统内的信息主要有： superblock：记录此 filesystem 的整体信息，包括inode/block的总量、使用量、剩余量， 以及文件系统的格式与相关信息等； inode：记录文件的属性，一个文件占用一个inode，同时记录此文件的数据所在的 block 号码； block：实际记录文件的内容，若文件太大时，会占用多个 block 。 Ext2 文件系统的数据存取为索引式文件系统(indexed allocation) 需要碎片整理的原因就是文件写入的 block 太过于离散了，此时文件读取的效能将会变的很差所致。 这个时候可以透过碎片整理将同一个文件所属的 blocks 汇整在一起。 Ext2文件系统主要有：boot sector, superblock, inode bitmap, block bitmap, inode table, data block 等六大部分。 data block 是用来放置文件内容数据地方，在 Ext2 文件系统中所支持的 block 大小有 1K, 2K 及 4K 三种而已 inode 记录文件的属性/权限等数据，其他重要项目为： 每个 inode 大小均固定为 128 bytes； 每个文件都仅会占用一个 inode 而已； 因此文件系统能够创建的文件数量与 inode 的数量有关； 文件的 block 在记录文件的实际数据，目录的 block 则在记录该目录底下文件名与其 inode 号码的对照表； 日志式文件系统 (journal) 会多出一块记录区，随时记载文件系统的主要活动，可加快系统复原时间； Linux 文件系统为添加效能，会让主存储器作为大量的磁盘高速缓存； 实体链接只是多了一个文件名对该 inode 号码的链接而已； 符号链接就类似Windows的快捷方式功能。 磁盘的使用必需要经过：分割、格式化与挂载，分别惯用的命令为：fdisk, mkfs, mount三个命令 启动自动挂载可参考/etc/fstab之配置，配置完毕务必使用 mount -a 测试语法正确否； ###参考资料 鸟哥的Linux私房菜 第八章]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 下配置sublime text]]></title>
    <url>%2F2016%2F07%2F11%2F%E6%8A%80%E6%9C%AF%2FSublime-Text3-On-Linux%2F</url>
    <content type="text"><![CDATA[Linux下配置sublime text Sublime Text是个跨平台的编辑器，支持Windows、Linux、Mac系统平台，支持各种语言的代码编辑，配合上对应的插件，话上点时间学习，你将会对它爱不释手，大大的提高你的编码效率。本文将讲解在Ubuntu 14.04系统中安装SublimeText 3，并配置SublimeClang插件来配置C/C++开发环境。 1. Sublime Text 3的下载安装 到官方网站上http://www.sublimetext.com/3下载64位（系统位64位）的.deb安装包（http://c758482.r82.cf2.rackcdn.com/sublime-text_build-3059_amd64.deb），下载后双击安装即可。安装好之后，通过命令subl即可打开程序，此时已经可以编写代码了。在开始之前建议先记下一些常用的快捷键，可参考：http://blog.csdn.net/cywosp/article/details/31791881 2. 安装Package ControlPackage Control是一个用于管理插件的好工具，可以用于安装、删除、禁用相应的插件，常用的插件都能在上面找到。其源码地址在https://github.com/wbond/package_control_channel上，安装非常方便，使用git将该代码先克隆下来即可，然后拷贝到~/.config/sublime-text-3/Packages/目录下并命名为Package Control即可。（也可以直接在github上打包下载，然后解压复制到~/.config/sublime-text-3/Packages/目录下并命名为Package Control）。 cd ~/.config/sublime-text-3/Packages/git clone https://github.com/wbond/package_control_channel.git Package\ Control 或者打开sublime_text然后按快捷键ctrl+`(Esc下面那个键)，在弹出的命令输入窗口输入下面信息回车即可： 12[python] view plain copyimport urllib.request,os,hashlib; h = &apos;2915d1851351e5ee549c20394736b442&apos; + &apos;8bc59f460fa1548d1514676163dafc88&apos;; pf = &apos;Package Control.sublime-package&apos;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &apos;http://packagecontrol.io/&apos; + pf.replace(&apos; &apos;, &apos;%20&apos;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&apos;Error validating download (got %s instead of %s), please try manual install&apos; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &apos;wb&apos; ).write(by) 重新启动SublimeText 3，然后使用快捷键Ctrl + Shift + p，在弹出的输入框中输入Package Control则可以看到Install Package的选项，选择它后一会儿（看左下角的状态）会弹出插件查询及安装窗口，输入想用的插件，选中回车即可。如果用于C/C++开发建议安装C++ snipptes，ConvertToUTF8，SublimeAStyleFormatter插件，具体代表什么意思baidu一下就清楚了。 3. 安装强大的SublimeClang插件 SublimeClang是Sublime Text中唯一的C/C++自动补全插件，功能强大，自带语法检查功能，不过最近作者已经停止更新了，目前只能在Sublime Text 2的Package Control中可以找到并自动安装，在SublimeText 3中只能手动通过源码安装，其代码线在https://github.com/quarnster/SublimeClang中。具体安装步骤如下：安装相关软件 12345678910sudo apt-get install cmake build-essential clang gitcd ~/.config/sublime-text-3/Packagesgit clone --recursive https://github.com/quarnster/SublimeClang SublimeClangcd SublimeClangcp /usr/lib/x86_64-linux-gnu/libclang-3.4.so.1 internals/libclang.so #这一步很重要，如果你的clang库不是3.4版本的话，请将对应版本的库拷贝到internals中cd srcmkdir buildcd buildcmake ..make 一切成功的话将会在SublimeClang/internals目录中生成libcache.so库文件。重启Sublime Text，然后按快捷键Ctrl + ~ (Esc下面那个键)打开自带的控制输出，看看有没有错误，如果没有错误就说明一切OK了。接下来就是配置自己的文件了，按下ctrl + shift + p快捷键，在弹出的输入框中输入 sublimeclang settings ，然后选择带User那一行，在打开的文件中输入如下信息： 123456789101112131415161718&#123; &quot;show_output_panel&quot;: false, &quot;dont_prepend_clang_includes&quot;: true, &quot;inhibit_sublime_completions&quot;: false, &quot;options&quot;: [ &quot;-std=gnu++11&quot;, &quot;-isystem&quot;, &quot;/usr/include&quot;, &quot;-isystem&quot;, &quot;/usr/include/c++/*&quot;, &quot;-isystem&quot;, &quot;/usr/include/c++/4.8&quot;, &quot;-isystem&quot;, &quot;/usr/include/c++/4.8/*&quot;, &quot;-isystem&quot;, &quot;/usr/include/boost&quot;, &quot;-isystem&quot;, &quot;/usr/include/boost/**&quot;, &quot;-isystem&quot;, &quot;/usr/lib/gcc/x86_64-linux-gnu/4.8/include&quot;, &quot;-isystem&quot;, &quot;/usr/lib/gcc/x86_64-linux-gnu/4.8/include/*&quot; ]&#125; 注释：我的gcc版本为4.8，如果你的不是请替换对应的版本，在#include相应的头文件后保存当前文件，在接下来的操作中将更快的提示所包含在头文件的函数或者变量。 4. 工程实例通过菜单栏中的Project -&gt; Add Folder To Project…把你已有的原代码目录加入到Sublime Text中，然后通过Project -&gt; Save Project As…来保存你的项目，这样就创建好了项目。例如我的机器在/media/WinE/WorkStation/Swift中有个C++项目，代码分别放在了Swift下的swift/base和swift/disruptor两个目录下，现在想要把这两个目录中的内容在写代码时能够自动提示则需要相应的配置修改。Project -&gt; Edit Project，在所打开的配置文件中我更改如下： 12345678910111213141516171819&#123; &quot;folders&quot;: [ &#123; &quot;follow_symlinks&quot;: true, &quot;path&quot;: &quot;/media/WinE/WorkStation/Swift&quot; &#125; ], &quot;settings&quot;: &#123; &quot;sublimeclang_options&quot;: [ &quot;-I/media/WinE/WorkStation/Swift&quot;, &quot;-I/media/WinE/WorkStation/Swift/swift/base&quot;, &quot;-I/media/WinE/WorkStation/Swift/swift/disruptor&quot;, ] &#125;&#125;]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-3]]></title>
    <url>%2F2016%2F07%2F06%2F%E6%8A%80%E6%9C%AF%2Fstudy-of-python-3%2F</url>
    <content type="text"><![CDATA[正则表达式 通过正则表达式匹配字符串，\w+\@\w+.\w+ 匹配一个邮箱地址，\w可以匹配一个数字或字母，\d \D \s \S,[a-z,A-Z,0-9,_] 取值范围；[P|p]ython 匹配Python或python；^\d 必须以数字开头； \d$ 必须以数字结尾； re模块 判断字符串是否匹配re.match() 12345test = &apos;用户输入的字符串&apos;if re.match(r&apos;正则表达式&apos;, test): print &apos;ok&apos;else: print &apos;failed&apos; 切分字符串，用正则表达式试试： 12&gt;&gt;&gt; re.split(r&apos;\s+\,\;&apos;, &apos;a b c&apos;)[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;] 分组正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group） 1234567891011121314&gt;&gt;&gt; m = re.match(r&apos;^(\d&#123;3&#125;)-(\d&#123;3,8&#125;)$&apos;, &apos;010-12345&apos;)&gt;&gt;&gt; m&lt;_sre.SRE_Match object at 0x1026fb3e8&gt;&gt;&gt;&gt; m.group(0) # `group(0)`永远是原始字符串&apos;010-12345&apos;&gt;&gt;&gt; m.group(1)&apos;010&apos;&gt;&gt;&gt; m.group(2)&apos;12345&apos;&gt;&gt;&gt; t = &apos;19:05:30&apos;&gt;&gt;&gt; m = re.match(r&apos;^(0[0-9]|1[0-9]|2[0-3]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])$&apos;, t)&gt;&gt;&gt; m.groups()(&apos;19&apos;, &apos;05&apos;, &apos;30&apos;) 预编译一个匹配模式被多次用到，可以先预编译，re.compile(‘string’) 贪婪模式 常用内建模块collections namedtuple 12345678&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Point = namedtuple(&apos;Point&apos;, [&apos;x&apos;, &apos;y&apos;]) #定义一个点&gt;&gt;&gt; Circle = namedtuple(&apos;Circle&apos;, [&apos;x&apos;, &apos;y&apos;, &apos;r&apos;])#定义一个圆&gt;&gt;&gt; p = Point(1, 2)&gt;&gt;&gt; p.x #按属性访问&gt;&gt;&gt; 1&gt;&gt;&gt; p.y&gt;&gt;&gt; 2 deque 双向列表，deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈 123456&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])&gt;&gt;&gt; q.append(&apos;x&apos;)&gt;&gt;&gt; q.appendleft(&apos;y&apos;)&gt;&gt;&gt; q&gt;&gt;&gt; deque([&apos;y&apos;, &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;x&apos;]) deque除了实现list的append()和pop()外，还支持appendleft()和popleft()，这样就可以非常高效地往头部添加或删除元素。 defaultdict 使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict： 1234567&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(lambda: &apos;N/A&apos;)&gt;&gt;&gt; dd[&apos;key1&apos;] = &apos;abc&apos;&gt;&gt;&gt; dd[&apos;key1&apos;] # key1存在&gt;&gt;&gt; &apos;abc&apos;&gt;&gt;&gt; dd[&apos;key2&apos;] # key2不存在，返回默认值&gt;&gt;&gt; &apos;N/A&apos; 注意默认值是调用函数返回的，而函数在创建defaultdict对象时传入。 除了在Key不存在时返回默认值，defaultdict的其他行为跟dict是完全一样的。 OrderedDict 使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。如果要保持Key的顺序，可以用OrderedDict： 1234567&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)])&gt;&gt;&gt; d # dict的Key是无序的&gt;&gt;&gt; &#123;&apos;a&apos;: 1, &apos;c&apos;: 3, &apos;b&apos;: 2&#125;&gt;&gt;&gt; od = OrderedDict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)])&gt;&gt;&gt; od # OrderedDict的Key是有序的&gt;&gt;&gt; OrderedDict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)]) 注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序： 123456&gt;&gt;&gt; od = OrderedDict()&gt;&gt;&gt; od[&apos;z&apos;] = 1&gt;&gt;&gt; od[&apos;y&apos;] = 2&gt;&gt;&gt; od[&apos;x&apos;] = 3&gt;&gt;&gt; od.keys() # 按照插入的Key的顺序返回&gt;&gt;&gt; [&apos;z&apos;, &apos;y&apos;, &apos;x&apos;] OrderedDict可以实现一个FIFO（先进先出）的dict，当容量超出限制时，先删除最早添加的Key： 12345678910111213141516171819from collections import OrderedDictclass LastUpdatedOrderedDict(OrderedDict): def __init__(self, capacity): super(LastUpdatedOrderedDict, self).__init__() self._capacity = capacity def __setitem__(self, key, value): containsKey = 1 if key in self else 0 if len(self) - containsKey &gt;= self._capacity: last = self.popitem(last=False) print 'remove:', last if containsKey: del self[key] print 'set:', (key, value) else: print 'add:', (key, value) OrderedDict.__setitem__(self, key, value) Counter Counter是一个简单的计数器，例如，统计字符出现的个数： 1234567&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in &apos;programming&apos;:&gt;&gt;&gt; ... c[ch] = c[ch] + 1&gt;&gt;&gt; ...&gt;&gt;&gt; c&gt;&gt;&gt; Counter(&#123;&apos;g&apos;: 2, &apos;m&apos;: 2, &apos;r&apos;: 2, &apos;a&apos;: 1, &apos;i&apos;: 1, &apos;o&apos;: 1, &apos;n&apos;: 1, &apos;p&apos;: 1&#125;) Counter实际上也是dict的一个子类，统计的元素作为key，出现次数是key的值。 base64对二进制数据进行处理，每3个字节一组，一共是3x8=24bit，划为4组，每组正好6个bit，对应编码规则里定义的64个字符。二进制字节不是3的倍数，Base64用\x00字节在末尾补足后，再在编码的末尾加上1个或2个=号。解码的=。urlsafe编码，把字符+和/分别变成-和_。字符+和/，在URL中就不能直接作为参数。 123456&gt;&gt;&gt; base64.b64encode('i\xb7\x1d\xfb\xef\xff')'abcd++//'&gt;&gt;&gt; base64.urlsafe_b64encode('i\xb7\x1d\xfb\xef\xff')'abcd--__'&gt;&gt;&gt; base64.urlsafe_b64decode('abcd--__')'i\xb7\x1d\xfb\xef\xff' struct Python提供了一个struct模块来解决str和其他二进制数据类型的转换。 struct的pack函数把任意数据类型变成字符串： 123&gt;&gt;&gt; struct.unpack(&apos;&gt;IH&apos;, &apos;\xf0\xf0\xf0\xf0\x80\x80&apos;)(4042322160, 32896)### &gt;IH的说明，后面的str依次变为I：4字节无符号整数和H：2字节无符号整数,&gt;表示大端存储方式 struct模块定义的数据类型可以参考： Python官方文档 hashlib用户名如果不可变可以作为salt与passwd一起hash 12345678910import hashlibmd5 = hashlib.md5()md5.update('how to use md5 in python hashlib?')print md5.hexdigest() import hashlibsha1 = hashlib.sha1()sha1.update('how to use sha1 in ')sha1.update('python hashlib?') #多次update和一次做完结果一样print sha1.hexdigest() itertools count() cycle() repeat() chain() groupby() imap() imap()可以作用于无穷序列，并且，如果两个序列的长度不一致，以短的那个为准。 12for x in itertools.imap(lambda x, y: x * y, [10, 20, 30], itertools.count(1)): print x 注意imap()返回一个迭代对象，而map()返回list。当你调用map()时，已经计算完毕.当你调用imap()时，并没有进行任何计算,必须用for循环迭代，每次计算出一个元素。 ifliter() XMLHTMLParser###第三方模块 PILNumpy图形界面网络编程TCP/IPTCP编程UDP编程电子邮件SMTP发送邮件pop3接收邮件访问数据库使用SQLite使用MySQL使用SQLAlchemyWeb开发HTTP协议HTMLWSGI接口使用Web框架使用模板协程12]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础学习笔记]]></title>
    <url>%2F2016%2F07%2F05%2F%E6%8A%80%E6%9C%AF%2Flinux%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Table of Contents generated with DocToc Linux简介 系统安装与grub引导 登陆系统 常用命令 用户管理 用户和用户组 账号管理和用户切换 文件管理 文件和目录管理和权限 文件查找 文件压缩解压 文件系统 磁盘分区，文件系统的挂载和卸载 linux逻辑卷 硬链接和软链接 字符处理 管道和grep sort排序和uniq去重 cut、tr、paste、split 网络管理 网络配置和dns配置 网络测试 进程管理 查看进程 杀死进程 查询进程打开的文件 进程优先级调整：nice、renice 编译、安装软件 安装包管理器 编译安装 执行定时任务 正则表达式与sed、awk vi和vim shell编程 Linux简介系统安装与grub引导 系统运行级别6级，0：关机；1：单用户模式，进行系统维护；2：多用户，无网络连接；3：完全多用户模式，默认不用图形化；4：保留；5：窗口模式，多用户；6：重启。 安装linux grub系统引导 登陆系统有图形登陆、终端登陆、远程终端登陆。 linux默认提供6个终端，使用ctrl + alt + F1~F6切换。 使用ctrl + alt + F7切回桌面图形登陆。 在字符终端中使用startx可以启动图形桌面。 终端用户退出登陆exit. 常用命令 ls pwd cd cat ps ifconfig netstat -ano查看命令帮助文档 man 和 info在查看结果中 使用 /keyword进行关键字搜索，n向下查找，N向上查找。空格向下翻页，pageup/pagedown翻页，q退出查看。 123$ man -f reboot man -f 查看命令位于哪些man文件里$ man 2 reboot 查看2号文档里的reboot命令$ info ls 获取ls命令的说明文档 用户管理用户和用户组 UIDUID为0的是root超级用户，1~500是系统用户，500以后是普通用户。 GID一个用户可以属于多个组 12345$ id 查看自己的uid$ group 查看自己的gid$ whoami 查看用户自己的信息$ who 查看当前已登录的用户信息$ w 查看更详细的用户信息 /etc/passwd /etc/shadow两个文件记录了用户的用户名和密码。 账号管理和用户切换 增加删除用户 1234# useradd -u 555 - g group1 -d /home/john -s /bin/bash john 指定uid，加入的组，家目录，shell路径，组必须是已经存在的# useradd -G group1,group2 john -G加入一系列的组# userdel john 删除了john用户，从shadow中清除记录，用户文件还在# userdel -r john 删除john的home目录和邮件 修改密码和删除密码 1234567# passwd 不加参数则是修改当前用户的密码# passwd user1 修改user1的密码，新建的用户必须修改密码后才能登陆# passwd -d user1 删除密码，即密码为空# passwd -l user1 禁用user1的密码，实际在shadow文件的记录前加了`!`# passwd -u user1 解锁账号# usermod -L user1 # usermod -U user1 原理同上 修改用户# usermod 增加删除组 12# groupadd group1 新增group1# groupdel group1 删除用户组，如果组内有用户，则删除失败 切换用户 123456$ su 直接切换到root用户，当前用户环境不变，工作目录，shell等# exit 退出root，切回普通用户$ su - 切换到root用户，同时使用root用户环境# su - john 切换为john用户，root切换为任何user无需密码， 普通user相互切换要知道user密码$ sudo command 普通用户以root身份执行某条命令，输入自己的密码即可 用户能否使用sudo命令，在/etc/sudoers中配置。使用visudo命令进行配置。123456789# User privilege specificationroot ALL=(ALL:ALL) ALLjohn ALL=(ALL:ALL) ALLluna ALL=(ALL:ALL) NOPASSWD:/sbin/shutdown# 在此处添加，允许john 从任何地方登陆后，执行任何人的，任何命令,对应3个all# 允许luna 不用输密码即可执行 sudo shutdown 命令。# Members of the admin group may gain root privileges%admin ALL=(ALL) ALL# 允许admin组的所有成员，......执行sudo 文件管理文件和目录管理和权限 目录结构 123456789101112131415161718/├── bin 常见用户指令├── boot 内核和启动文件├── cdrom 光驱挂载位置├── dev 设备文件├── etc 系统和服务配置├── home 用户主目录├── lib 系统函数库├── lib64├── lost+found ext3 文件系统用于磁盘检查├── media 挂载u盘等临时文件系统├── mnt 系统加载文件系统的挂载点├── opt 第三方软件安装目录├── root root用户主目录├── sbin 系统管理命令├── tmp 临时文件存放目录├── usr 存放和用户直接相关的文件├── var 路径和特殊目录绝对路径，从/根路径开始，比如/root/abc;当前路径，pwd命令可以查看。shell命令执行默认基于当前路径；特殊目录.和..，分别表示当前目录和当前目录的父目录。相对路径，从当前路径开始计算,通常配合.和..使用。 文件操作 1234567891011121314151617181920212223### 文件创建$ touch a.txt touch一个已存在的文件，会更新其时间戳属性$ vi a.txt$ echo &apos;XXX&apos; &gt; a.txt### 删除文件$ rm -f a.txt 直接删除，不用确认### 复制、移动文件$ cp a.txt /home/test/$ cp a.txt /path/to/b.txt 可以实现重命名$ mv a.txt /path/to/filename$ mv a.txt /path/to/### 文件查看$ cat readme.txt$ cat -n readme.txt 查看文件时显示行号$ head readme.txt 默认显示前10行内容$ head -n 20 readme.txt 查看前20行内容$ tail readme.txt 查看最后10行内容，-n 指定显示的行数目$ tail -f error.log 文件不断写入时，可以动态查看文件末尾的内容### 改变属主# chown john a.txt 改变a的所有者为john用户# chown :john a.txt 改变a的所有组为john组# chown -R john:john dir_a 递归改变dir_a下所有文件的所有组为john，所有者为john# chgrp -R john dir_b 递归改变所有组 目录操作 123$ cd /home/ 进入到指定目录$ mkdir dir_a 创建目录$ rm -rf dir_a 删除目录及其包含的全部内容，无需确认 权限和属性 1234567891011# ls -al总用量 1712drwx------ 27 root root 4096 10月 31 12:54 .-rw------- 1 root root 16213 10月 30 22:25 .bash_history#第一个字符的含义：d目录，-普通文件，l链接文件，b块文件， c字符文件，s socket文件，p管道文件#接下来的3组 X 3个字符，表示文件所有者，文件所有组和其他用户对该文件的权限# rwx为可读、可写、可执行，-表示不拥有该位置的权限 # 第二列的数字为连接数，文件为1，目录为其包含的目录数（包括特殊目录.和..）# 第三四列 为文件的所有人和所有组# 第五列文件大写，第六列文件最近修改时间 文件隐藏属性a和i123456# lsattr run_dedicated_servers.sh -------------e-- run_dedicated_servers.sh## 有13个短横，# chattr +a a.ttx append,a属性的文件即使root也不能删除，可以追加的方式写文件# chattr +i b.txt i属性确保文件无法删除写入和改名，常用于关键配置文件# man chattr 查看更多 文件特殊属性SUID/SGID/Sticky1234567891011# ll /usr/bin/passwd -rwsr-xr-x 1 root root 54256 3月 29 2016 /usr/bin/passwd*### 原本的执行权限x变成了s，表示其他用户可以以文件的所有者身份来执行该文件### SGID与SUID类似，都只能用于可执行文件# chmod u+s ‘可执行文件’ 设置SUID# chmod g+s ‘可执行文件’ 设置SGID，用的较少### Sticky属性用于目录# ll -d /tmpdrwxrwxrwt 26 root root 4096 10月 31 14:24 /tmp/### 最后一个t表示，任何人都能在此创建修改文件，但只有owner和root可以删除文件。# chmod o+t ‘目录’ 添加t属性 默认权限和umask，用户或系统创建的文件有默认的权限设置，root和普通用户创建文件，权限为644，root的目录为755，普通用户的为775. 文件查找 findfind PATH -name FILENAME locatelocate查找依赖一个数据库，因此使用之前一般先执行updatedb。 which/whereis 查找可执行文件which command可以找到命令所在位置，which在path路径中查找。whereis同时给出相关man文件。 文件压缩解压1234567# gzip a.txt 压缩成gz格式压缩包# gunzip a.txt.gz 解压# tar -zcvf a.tar.gz /home/a 创建压缩文件### 参数z，使用gzip压缩；参数c，创建压缩文件；参数f，使用文件名；参数v，详情模式；# tar -zxvf a.tar.gz -C /tmp -C 指定解压到的目录# bzip2 -z a 压缩得到bz2格式的压缩包# bzip2 -d a.bz2 解压文件 文件系统磁盘分区，文件系统的挂载和卸载1234# fdisk -l 查看磁盘设备和分区# mount DEVICE MOUNT_POINT 将设备挂载到指定位置### 设备自动挂载，在/etc/fstab中配置# df -h 查看磁盘使用情况 linux逻辑卷硬链接和软链接硬链接：多个文件名指向同一个文件的inode索引节点，使得一个文件拥有多个合法的路径。删除一个硬链接不会影响其他的硬链接和文件本身。目录无法创建硬链接，不同文件系统/分区之间不能建立硬链接。ln &#39;源文件&#39; &#39;新建硬链接文件&#39;软连接：包含指向另一个文件的路径，类似windows的快捷方式。ln -s &#39;源文件&#39; &#39;新建软连接&#39; 字符处理管道和grep管道连接符command1 | command2，将command1的输出作为command2的输入。grep文本搜索123$ cat a.txt | grep &apos;name&apos; 打印含有name的行$ grep [-ivnc] &apos;search_str&apos; CONTENT/FILE### i忽略大小写，v反向选择，n同时输出行号，c统计匹配的行数 sort排序和uniq去重1234567891011### sort参数，-t指定分割符，k排序的列，n以数字排序，默认是字符排序，r反向排序，即降序# sort -t &apos;:&apos; -k 3 -n /etc/passwd daemon:*:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:*:2:2:bin:/bin:/usr/sbin/nologinsys:*:3:3:sys:/dev:/usr/sbin/nologinsync:*:4:65534:sync:/bin:/bin/syncgames:*:5:60:games:/usr/games:/usr/sbin/nologinman:*:6:12:man:/var/cache/man:/usr/sbin/nologinlp:*:7:7:lp:/var/spool/lpd:/usr/sbin/nologin### uniq只能去除连续重复的行，因此一般配合sort使用# uniq -ic CONTENT 忽略大小写，输出该行重复的次数 cut、tr、paste、split网络管理网络配置和dns配置1234567891011121314151617181920$ ifconfig 查看网卡信息eth0 Link encap:以太网 硬件地址 f4:8e:38:b8:1e:fd inet 地址:172.16.27.118 广播:172.16.27.255 掩码:255.255.252.0 inet6 地址: fe80::ec5d:eb79:5e03:112/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 跃点数:1 接收数据包:1002 错误:0 丢弃:3 过载:0 帧数:0 发送数据包:147 错误:0 丢弃:0 过载:0 载波:0 碰撞:0 发送队列长度:1000 接收字节:107932 (107.9 KB) 发送字节:18302 (18.3 KB)eth1 ***lo ***wlan ***$ ifconfig eth0 查看指定网卡### ifconfig动态修改网卡的配置，重启后失效# ifconfig eth0 192.168.159.130 netmask 255.255.255.0# ifconfig eth0 192.168.159.130/24 指定eth0的ip和子网# ifconfig eth0 down 停用网卡# ifconfig eth0 up 启用网卡# ifdown/ifup eth0 效果同上# route add/del default gw 192.168.8.1 修改网关 修改网卡配置文件1234567891011121314/etc/network/interfaces文件默认的内容如下： auto lo iface lo inet loopback 在后面添加内容 1、获取动态配置： auto eth0 iface eth0 inet dhcp 2、获取静态配置： auto eth0 iface eth0 inet static address 192.168.0.1 netmask 255.255.255.0 gateway 192.168.0.1 重启networking服务 修改DNS配置1234567永久修改网卡DNSsudo –icd /etc/resolvconf/resolv.conf.dvim base添加如下内容nameserver 8.8.8.8nameserver 8.8.4.4 网络测试 ping发icmp echo请求，判断主机是否可达 hosthost命令查询DNS记录 traceroute追溯数据包所经过的路由 网络故障排查1、ping 127.0.0.1 判断网卡工作是否正常，tcp/ip协议栈出问题2、ping 本机ip 判断本地设备驱动/物理端口3、ping 同网段其他主机 看交换机是否正常4、ping 网关ip5、ping 公网ip ，本地路由，nat6、ping 公网域名 dns设置是否正确 进程管理查看进程1234567# ps aux 显示所有包含其他使用者的有效进程USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.1 119732 5812 ? Ss 16:49 0:01 /sbin/init splashroot 2 0.0 0.0 0 0 ? S 16:49 0:00 [kthreadd]# top 查看结果是实时动态变化的# 按下O，大写的，进入排序过滤器，然后... 杀死进程12345678910#### 先查看要杀死进程的PID# ps -ef | grep &apos;ssh&apos;root 975 1 0 16:49 ? 00:00:00 /usr/sbin/sshd -Droot 3291 975 0 16:50 ? 00:00:00 sshd: root@pts/8root 4873 3782 0 20:05 pts/8 00:00:00 grep --color=auto ssh# pidof sshd3291 975# kill 3291 停止pid = 3291的进程# kill -1 pid 1重启，9强制退出，15正常退出（默认）# killall sshd 停止进程，跟进程名 查询进程打开的文件1234567891011# lsof -i:22 查询打开22端口的进程COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 975 root 3u IPv4 19938 0t0 TCP *:ssh (LISTEN)sshd 975 root 4u IPv6 19940 0t0 TCP *:ssh (LISTEN)# lsof -c sshd 显示COMMAND中包含指定字符串的进程所打开的全部文件COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 975 root cwd DIR 8,2 4096 2 /sshd 975 root rtd DIR 8,2 4096 2 /sshd 975 root txt REG 8,2 799216 12328642 /usr/sbin/# lsof FILENAME 显示所有打开FILENAME文件的进程 进程优先级调整：nice、renice使用top时，可以看到进程的NI、PR字段。NI表示进程优先级，取值-20~19，默认为0。普通用户可以设置进程NI值0~19.PR是动态优先级，系统进程调用采取的’动态优先级‘调度算法。最终程序的优先级是NI+PR，定义和修改进程优先级nice和renice12345# topPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 119732 5812 3904 S 0.0 0.1 0:01.58 systemd# nice -n -12 ./job.sh 设定程序运行时的优先级为-12# renice -10 -p 5555 将优先级调整为-10 编译、安装软件安装包管理器编译安装执行定时任务 at crontab screen 正则表达式与sed、awkvi和vim单独讲 shell编程单独讲]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-2]]></title>
    <url>%2F2016%2F07%2F04%2F%E6%8A%80%E6%9C%AF%2Fstudy-of-python-2%2F</url>
    <content type="text"><![CDATA[错误、调试、测试 try机制 12345678910 try: print 'try...' r = 10 / 0 #此处出现异常直接跳转到except print 'result:', r except ZeroDivisionError, e: print 'except:', e finally: # 若没有错误发生，except语句块不会被执行，但是finally如果有，则一定会被执行（可以没有finally语句）。#此外，如果没有错误发生，可以在except语句块后面加一个else，当没有错误发生时，会自动执行else语句： print 'finally...'print 'END' 错误类型都继承自BaseException，所以在使用except时需要注意的是，它不但捕获该类型的错误，还把其子类也“一网打尽”。比如ValueError是StandardError的子类，如果有except先捕获了StandardError，则第二个except就捕获不到ValueError。 记录错误 Python内置的logging模块可以非常容易地记录错误信息。logging.exception(e) 抛出错误如果要抛出错误，首先根据需要，可以定义一个错误的class，选择好继承关系，然后，用raise语句抛出一个错误的实例 12345678class FooError(StandardError): pass# 只有在必要的时候才定义我们自己的错误类型。如果可以选择Python已有的内置的错误类型（比如ValueError，TypeError），尽量使用Python内置的错误类型。def foo(s): n = int(s) if n==0: raise FooError('invalid value: %s' % s) return 10 / n 一种常见的抛出用法 12345678910def foo(s): n = int(s) return 10 / n def bar(s): try: return foo(s) * 2 except StandardError, e: #此处捕获只是为了记录错误，如果无法处理，可以上抛 print 'Error!' raise 调试 简单粗暴的print将可能出错的变量打印出来，print太多的话看着不爽 断言凡是用print来辅助查看的地方，都可以用断言（assert）来替代，assert n!=0,’n is zero’ ,如果断言失败，assert语句本身就会抛出AssertionError：n is zero运行时加参数-O可以关闭断言。 logging 12345678import logginglogging.basicConfig(level=logging.INFO) #它允许你指定记录信息的级别，有debug，info，warning，error等几个级别，#当我们指定level=INFO时，logging.debug就不起作用了。#同理，指定level=WARNING后，debug和info就不起作用了s = '0'n = int(s)logging.info('n = %d' % n)print 10 / n IDE 断点据说最好的IDE是pycharm，我用的是wing IDE，有个debug probe窗口，可以程序断点暂停后进行python命令交互，print之类的。编辑器推荐sublime text。 单元测试“测试驱动开发”（TDD：Test-Driven Development）；编写单元测试时，我们需要编写一个测试类，从unittest.TestCase继承。以test开头的方法就是测试方法，不以test开头的方法不被认为是测试方法，测试的时候不会被执行。 对每一类测试都需要编写一个test_xxx()方法。 由于unittest.TestCase提供了很多内置的条件判断，我们只需要调用这些方法就可以断言输出是否是我们所期望的。最常用的断言就是assertEquals()： 另一种重要的断言assertRaises()就是期待抛出指定类型的Error，比如通过d[&#39;empty&#39;]访问不存在的key时，断言会抛出KeyError：运行测试单元if __name__ == &#39;__main__&#39;: unittest.main()可以在单元测试中编写两个特殊的setUp()和tearDown()方法。这两个方法会分别在每调用一个测试方法的前后分别被执行 文档测试Python内置的“文档测试”（doctest）模块可以直接提取注释中的代码并执行测试。 IO 文件读写 123456789try: f = open('/path/to/file', 'r') print f.read()finally: if f: f.close()# 或者 使用with，Python会自动调用close方法with open('/path/to/file', 'r') as f: print f.read() 调用read()会一次性读取文件的全部内容，如果文件有10G，内存就爆了，所以，要保险起见，可以反复调用read(size)方法，每次最多读取size个字节的内容。另外，调用readline()可以每次读取一行内容，调用readlines()一次读取所有内容并按行返回list。因此，要根据需要决定怎么调用。 如果文件很小，read()一次性读取最方便；如果不能确定文件大小，反复调用read(size)比较保险；如果是配置文件，调用readlines()最方便 1234for line in f.readlines(): print line.strip() #strip()删除字符串末尾的回车等字符。os.mknod("new.txt")#创建空白新文件 fp=open('a.txt','w')#打开一个文件，如果不存在则创建文件 文件操作 fp.read([size]) #size为读取的长度，以byte为单位 fp.readline([size]) #读一行，如果定义了size，有可能返回的只是一行的一部分 fp.readlines([size]) #把文件每一行作为一个list的一个成员，并返回这个list。其实它的内部是通过循环调用readline()来实现的。如果提供size参数，size是表示读取内容的总长，也就是说可能只读到文件的一部分。 fp.write(str) #把str写到文件中，write()并不会在str后加上一个换行符 fp.writelines(seq) #把seq的内容全部写到文件中(多行一次性写入)。这个函数也只是忠实地写入，不会在每行后面加上任何东西。 fp.close() #关闭文件。python会在一个文件不用后自动关闭文件，不过这一功能没有保证，最好还是养成自己关闭的习惯。 如果一个文件在关闭后还对其进行操作会产生ValueError fp.flush() #把缓冲区的内容写入硬盘 fp.fileno() #返回一个长整型的”文件标签“ fp.isatty() #文件是否是一个终端设备文件（unix系统中的） fp.tell() #返回文件操作标记的当前位置，以文件的开头为原点 fp.next() #返回下一行，并将文件操作标记位移到下一行。把一个file用于for … in file这样的语句时，就是调用next()函数来实现遍历的。 fp.seek(offset[,whence]) #将文件打操作标记移到offset的位置。这个offset一般是相对于文件的开头来计算的，一般为正数。但如果提供了whence参数就不一定了，whence可以为0表示从头开始计算，1表示以当前位置为原点计算。2表示以文件末尾为原点进行计算。需要注意，如果文件以a或a+的模式打开，每次进行写操作时，文件操作标记会自动返回到文件末尾。 fp.truncate([size]) #把文件裁成规定的大小，默认的是裁到当前文件操作标记的位置。如果size比文件的大小还要大，依据系统的不同可能是不改变文件，也可能是用0把文件补到相应的大小，也可能是以一些随机的内容加上去。 open模式 w 以写方式打开，a 以追加模式打开 (从 EOF 开始, 必要时创建新文件)r+ 以读写模式打开w+ 以读写模式打开 (参见 w )a+ 以读写模式打开 (参见 a )rb 以二进制读模式打开wb 以二进制写模式打开 (参见 w )ab 以二进制追加模式打开 (参见 a )rb+ 以二进制读写模式打开 (参见 r+ )wb+ 以二进制读写模式打开 (参见 w+ )ab+ 以二进制读写模式打开 (参见 a+ ) 目录操作 os.mkdir(“file”) 创建目录复制文件：shutil.copyfile(“oldfile”,”newfile”) oldfile和newfile都只能是文件shutil.copy(“oldfile”,”newfile”) oldfile只能是文件夹，newfile可以是文件，也可以是目标目录复制文件夹：shutil.copytree(“olddir”,”newdir”) olddir和newdir都只能是目录，且newdir必须不存在重命名文件（目录）os.rename(“oldname”,”newname”) 文件或目录都是使用这条命令移动文件（目录）shutil.move(“oldpos”,”newpos”) 删除文件os.remove(“file”)删除目录os.rmdir(“dir”)只能删除空目录shutil.rmtree(“dir”) 空目录、有内容的目录都可以删转换目录os.chdir(“path”) 换路径 文件或文件夹操作python中对文件、文件夹 （文件操作函数） 的操作需要涉及到os模块和shutil模块。 得到当前工作目录，即当前Python脚本工作的目录路径: os.getcwd() 返回指定目录下的所有文件和目录名:os.listdir() 函数用来删除一个文件:os.remove() 删除多个目录：os.removedirs（r“c：\python”） 检验给出的路径是否是一个文件：os.path.isfile() 检验给出的路径是否是一个目录：os.path.isdir() 判断是否是绝对路径：os.path.isabs() 检验给出的路径是否真地存:os.path.exists() 返回一个路径的目录名和文件名:os.path.split() eg os.path.split(‘/home/swaroop/byte/code/poem.txt’) 结果：(‘/home/swaroop/byte/code’, ‘poem.txt’) 分离扩展名：os.path.splitext() 获取路径名：os.path.dirname() 获取文件名：os.path.basename() 运行shell命令: os.system() 读取和设置环境变量:os.getenv() 与os.putenv() 给出当前平台使用的行终止符:os.linesep Windows使用’\r\n’，Linux使用’\n’而Mac使用’\r’ 指示你正在使用的平台：os.name 对于Windows，它是’nt’，而对于Linux/Unix用户，它是’posix’os.uname() 系统详细版本，Windows上无 重命名：os.rename（old， new） 创建多级目录：os.makedirs（r“c：\python\test”） 创建单个目录：os.mkdir（“test”） 获取文件属性：os.stat（file） 修改文件权限与时间戳：os.chmod（file） 终止当前进程：os.exit（） 获取文件大小：os.path.getsize（filename） 比如我们要列出当前目录下的所有目录，只需要一行代码： 12&gt;&gt;&gt; [x for x in os.listdir(&apos;.&apos;) if os.path.isdir(x)][&apos;.lein&apos;, &apos;.local&apos;, &apos;.m2&apos;, &apos;.npm&apos;, &apos;.ssh&apos;, &apos;.Trash&apos;, &apos;.vim&apos;, &apos;Adlm&apos;, &apos;Applications&apos;, &apos;Desktop&apos;, ...] 要列出所有的.py文件，也只需一行代码： 12&gt;&gt;&gt; [x for x in os.listdir(&apos;.&apos;) if os.path.isfile(x) and os.path.splitext(x)[1]==&apos;.py&apos;][&apos;apis.py&apos;, &apos;config.py&apos;, &apos;models.py&apos;, &apos;pymonitor.py&apos;, &apos;test_db.py&apos;, &apos;urls.py&apos;, &apos;wsgiapp.py&apos;] 序列化 我们把变量从内存中变成可存储或传输的过程称之为序列化，在Python中叫pickling，在其他语言中也被称之为serialization，marshalling，flattening等等。序列化之后，就可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上。反过来，把变量内容从序列化的对象重新读到内存里称之为反序列化，即unpickling。 Python提供两个模块来实现序列化：cPickle和pickle。这两个模块功能是一样的，区别在于cPickle是C语言写的，速度快，pickle是纯Python写的，速度慢，跟cStringIO和StringIO一个道理。用的时候，先尝试导入cPickle，如果失败，再导入pickle： 123456789101112131415161718try: import cPickle as pickleexcept ImportError: import pickle&gt;&gt;&gt; d = dict(name=&apos;Bob&apos;, age=20, score=88)&gt;&gt;&gt; pickle.dumps(d) #序列化为str&quot;(dp0\nS&apos;age&apos;\np1\nI20\nsS&apos;score&apos;\np2\nI88\nsS&apos;name&apos;\np3\nS&apos;Bob&apos;\np4\ns.&quot;&gt;&gt;&gt; f = open(&apos;dump.txt&apos;, &apos;wb&apos;)&gt;&gt;&gt; pickle.dump(d, f) #pickle.dump()#写入文件&gt;&gt;&gt; f.close()#当我们要把对象从磁盘读到内存时，可以先把内容读到一个str，然后用pickle.loads()方法反序列化出对象，也可以直接用pickle.load()方法从一个file-like Object中直接反序列化出对象。&gt;&gt;&gt; f = open(&apos;dump.txt&apos;, &apos;rb&apos;)&gt;&gt;&gt; d = pickle.load(f)&gt;&gt;&gt; f.close()&gt;&gt;&gt; d&#123;&apos;age&apos;: 20, &apos;score&apos;: 88, &apos;name&apos;: &apos;Bob&apos;&#125; JSON 标准化对象序列格式Python内置的json模块提供了非常完善的Python对象到JSON格式的转换。我们先看看如何把Python对象变成一个JSON： 1234&gt;&gt;&gt; import json&gt;&gt;&gt; d = dict(name=&apos;Bob&apos;, age=20, score=88)&gt;&gt;&gt; json.dumps(d)&apos;&#123;&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;&#125;&apos; dumps()方法返回一个str，内容就是标准的JSON。类似的，dump()方法可以直接把JSON写入一个file-like Object。 要把JSON反序列化为Python对象，用loads()或者对应的load()方法，前者把JSON的字符串反序列化，后者从file-like Object中读取字符串并反序列化： 123&gt;&gt;&gt; json_str = &apos;&#123;&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;&#125;&apos;&gt;&gt;&gt; json.loads(json_str)&#123;u&apos;age&apos;: 20, u&apos;score&apos;: 88, u&apos;name&apos;: u&apos;Bob&apos;&#125; 有一点需要注意，就是反序列化得到的所有字符串对象默认都是unicode而不是str。 JSON进阶JSON序列化一个类对象，json.dumps(class_a,defaul=func_x) ,func_x函数，负责将对象变成可序列化的json对象。 1print(json.dumps(s, default=lambda obj: obj.__dict__)) JSON反序列化得到类对象：json.loads(json_str,object_hook=dict_2_student) 12345def dict2student(d): return Student(d['name'], d['age'], d['score'])json_str = '&#123;"age": 20, "score": 88, "name": "Bob"&#125;'print(json.loads(json_str, object_hook=dict2student)) 进程和线程windows下没有os.fork（）调用,multiprocessing模块就是跨平台版本的多进程模块。 process 1234567891011121314from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print 'Run child process %s (%s)...' % (name, os.getpid())if __name__=='__main__': print 'Parent process %s.' % os.getpid() p = Process(target=run_proc, args=('test',)) ###创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动，这样创建进程比fork()还要简单 print 'Process will start.' p.start() p.join() #join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。 print 'Process end.' pool 12345678910111213141516171819from multiprocessing import Pool #使用进程池创建大量子进程import os, time, randomdef long_time_task(name): print 'Run task %s (%s)...' % (name, os.getpid()) start = time.time() time.sleep(random.random() * 3) end = time.time() print 'Task %s runs %0.2f seconds.' % (name, (end - start))if __name__=='__main__': print 'Parent process %s.' % os.getpid() p = Pool() for i in range(5): p.apply_async(long_time_task, args=(i,)) print 'Waiting for all subprocesses done...' p.close() p.join() print 'All subprocesses done.' 进程间通信操作系统提供了很多机制来实现进程间的通信。Python的multiprocessing模块包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据。 多线程Python的标准库提供了两个模块：thread和threading，thread是低级模块，threading是高级模块，对thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。启动一个线程就是把一个函数传入并创建Thread实例，然后调用start()开始执行： 123456789101112131415161718import time, threading# 新线程执行的代码:def loop(): print 'thread %s is running...' % threading.current_thread().name n = 0 while n &lt; 5: n = n + 1 print 'thread %s &gt;&gt;&gt; %s' % (threading.current_thread().name, n) time.sleep(1) print 'thread %s ended.' % threading.current_thread().nameprint 'thread %s is running...' % threading.current_thread().namet = threading.Thread(target=loop, name='LoopThread')t.start()t.join()print 'thread %s ended.' % threading.current_thread().name##由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的threading模块有个current_thread()函数，它永远返回当前线程的实例。主线程实例的名字叫MainThread，子线程的名字在创建时指定，我们用LoopThread命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为Thread-1，Thread-2… Lock多线程访问修改同一变量，互斥操作，创建一个锁就是通过threading.Lock()来实现。注意死锁问题。 12345678910111213balance = 0lock = threading.Lock()def run_thread(n): for i in range(100000): # 先要获取锁: lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() Python解释器由于设计时有GIL全局锁，导致了多线程无法利用多核。多线程的并发在Python中就是一个美丽的梦。 Threadlocal每个线程都有自己的局部变量，threading.local()相当于创建了一个全局的dict，每个属性都是线程的局部变量，多线程读写不会相互干扰。ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。 123456789101112131415161718import threading# 创建全局ThreadLocal对象:local_school = threading.local()def process_student(): print 'Hello, %s (in %s)' % (local_school.student, threading.current_thread().name)def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student()t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')t1.start()t2.start()t1.join()t2.join() 进程vs线程 多任务的一般模式master-worker。多进程模式最大的优点就是稳定性高，缺点是创建进程的代价大；多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。异步IO，对于Python语言，单进程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。 分布式进程Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-1]]></title>
    <url>%2F2016%2F07%2F02%2F%E6%8A%80%E6%9C%AF%2Fstudy-of-python%2F</url>
    <content type="text"><![CDATA[基础语法数据结构list len() 求长度 a[i] list a 中第i个元素， a[2:9] 第2到9个元素，a[-1] 倒数第一个元素。 a.append(elem) list尾部追加一个元素 a.insert(i,elem) list第i个位置插入一个元素 a.pop(i) list 删除第i个元素，不指定i则默认删除最后一个 list的元素可以类型不同，也可以是list tupletuple不可变的元素集合，只有一个元素是的定义 a=(1,) 注意,是为了避免数学运算()的歧义 dict dict：key-value对，查询dict[key]可以取出对应的value。添加新键值对或修改值 dict[key]=newvalue。判断key是否存在 key in dict 或 dict.get(key,int-flag) 如果key不存在则返回指定的数值，不指定则返回none，none在交互式命令中不显示。删除一个key-value，用dict.pop(key) dict 插入查找速度快，占用内存多，list查找、插入与元素数量有关，占用内存空间少 set set： 也是一组key的集合，但是没有value set 初始化传入参数由一个list提供，s=set([1,1,2,2,3,4,5]),输出结果s为set([1,2,3,4,5])，其中的[]表示集合，重复元素被剔除，而不是list。 添加元素s.add(key),删除元素 s.remove(key) s1 &amp; s2 求交集；s1 | s2 ,求并集。 判断和循环 if 123456if x : # x非0非None即True xxxelif y: # 可以多个if嵌套使用 oooelse: xxoo for 123# xxxx为可迭代对象，list，dict，set和任何实现__next_item__方法的对象for x in xxxx: print x while 12345678# 当条件为真，执行循环while a&gt;0: a += 1 if a == 100: continue # a为100时跳过本次，进入下一层循环 if a &gt; 1000: # a为1000时跳过本次，跳出循环 break print a 函数 help(func) 查看函数fanc的帮助信息；dir(func)查看函数的内置方法；isinstance（x，（int,float））,类型检查； 定义函数 def myfunc(para1,para2): pass ;pass占位符，不知道写啥时可以先用它占位。 return x 返回x，没有return也会返回none，return可以返回多个值，但其本质是返回一个tuple。 默认参数放到后面，def power(x,n=2): pass;调用函数时默认参数如果不按顺序赋值，则需指定参数名。默认参数的值要设为不可变的变量类型，而不能是list。 可变参数，即输入的可以是0或多个参数不固定。定义可变参数仅在参数前面加了一个*号。在函数内部，参数numbers接收到的是一个tuple，因此调用该函数时，可以传入任意个参数，包括0个参数。如果变量已经保存为list或tuple，则可以用 *list来传递参数，而不用list[0],list[1] … 那么麻烦； 1234def func(*numbers): for num in numbers : pass #numbers是一个tuple pass 关键字参数：允许传入0或多个带参数名的参数，在函数内部作为一个dict, 使用**para 表示关键字参数.调用函数时也可将dict转换为关键字参数传递进去，func（**dict） 12def student(name,age,**p_other): print name,age,p_other 组合参数：参数定义的顺序必须是：必选参数、默认参数、可变参数和关键字参数。在函数调用的时候，Python解释器自动按照参数位置和参数名把对应的参数传进去，因此，对于任意函数，都可以通过类似func(args, *kw)的形式调用它，无论它的参数是如何定义的。 递归函数 注意栈溢出问题 Python高级特性切片切片 L[0:3] 或 L[:3] 取L的前三个元素,不包括3；L[-2:]取倒数后2个元素；L[:10:2] 前10个元素，每2个取一个；tuple或字符串也可以切片操作。 迭代迭代 for … in 实现遍历list或tuple或其他可迭代对象。默认情况下，dict迭代的是key，for key in d : pass。如果要迭代value，可以用for value in d.itervalues()，如果要同时迭代key和value，可以用for k, v in d.iteritems()。字符串也可以迭代，for ch in ‘ABC’: print ch。 for循环中同时迭代索引和元素本身 12for i, value in enumerate(['A', 'B', 'C']): print i, value 列表生成式快速创建list，简单强大。 12345678range(1,11) ;[x * x for x in range(1,11)] =[1,4,9,...,100] ;[x * x for x in range(1,11) if x%2==0] = [4,16,36,64,100] ; # 两层循环 [x + y for x in 'ABC' for y in 'ZXD' ]; # 列出当前目录下的文件和目录：import os[d d in os.listdir('.')] 生成器像不生成完整的list，而采用一边循环一边计算的机制；创建方法1： 将列表生成式的[]改为() g=(x * x for x in range(10)) 如果要一个一个打印出来g的元素，可以通过generator的next()方法，但是通常用for迭代来遍历。 for n in g: print g 方法2：通过函数实现生成器 123456def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b # generator在执行过程中，遇到yield就中断，下次又继续执行。 a, b = b, a + b n = n + 1 要理解generator的工作原理，它是在for循环的过程中不断计算出下一个元素，并在适当的条件结束for循环。对于函数改成的generator来说，遇到return语句或者执行到函数体最后一行语句，就是结束generator的指令，for循环随之结束。 函数式编程高阶函数允许函数作为参数传入: 12def add(x, y, f): return f(x) + f(y) map/reduce map()函数接收两个参数，一个是函数，一个是序列，map将传入的函数依次作用到序列的每个元素，并把结果作为新的list返回。 123456def f(x): return x**2map(f, [1,2,3,4,5,6])# 输出 [1,4,9,16,25,36]map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9])#输出 ['1', '2', '3', '4', '5', '6', '7', '8', '9'] reduce()把一个函数作用在一个序列[x1, x2, x3…]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算,例如，实现‘1 3 5 7 9’序列变为整数13579 1234def fn(x, y): return x*10 + yreduce(fn, [1, 3, 5, 7, 9])# 输出 13579 filter filter()也接收一个函数和一个序列。filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 123def is_odd(n): return n/2==1filter(is_odd,[1,2,3,4,5,6]) #求奇数 sorted Python内置的sorted()函数就可以对list进行排序。sorted()函数也是一个高阶函数，它还可以接收一个比较函数来实现自定义的排序。比如实现倒序排序reversed_cmp函数 1234567891011sorted([36, 5, 12, 9, 21])# 输出 [5, 9, 12, 21, 36]def reversed_cmp(x, y): if x &gt; y: return -1 if x &lt; y: return 1 return 0 sorted([36, 5, 12, 9, 21], reversed_cmp)# 输出 [36, 21, 12, 9, 5] 返回函数匿名函数lambda当高阶函数的参数是传入函数时，可以不显示地定义函数，直接传入匿名函数： 12map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9])# 输出 [1, 4, 9, 16, 25, 36, 49, 64, 81] 关键字lambda表示匿名函数，冒号前面的x表示函数参数。 匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数. 装饰器假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义，这种在代码运行期间动态增加功能的方式，称之为“装饰器”（Decorator）。 123456789def log(func): def wrapper(*args, **kw): print 'call %s():' % func.__name__ return func(*args, **kw) return wrapper #定义如上，使用如下： @logdef now(): print '2013-12-25' 把@log放到now()函数的定义处，相当于执行了语句 now=log(now) 参考文章-python装饰器 偏函数functools模块的一个功能， 1int2 = functools.partial(int, base=2) int2函数就是int（n，base=2）导出的新函数，int()函数的默认base=10，但是int2也可以接收参数base=10；functools.partial的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。 模块package &gt; module &gt; func package:按目录结构来管理模块，每一个包目录下面都会有一个__init__.py的文件，这个文件是必须存在的，否则，Python就把这个目录当成普通目录，而不是一个包。__init__.py可以是空文件，也可以有Python代码，因为__init__.py本身就是一个模块，而它的模块名就是mycompany。 编写模块 1234567891011121314151617181920#!/usr/bin/env python# -*- coding: utf-8 -*-' a test module ' #任何模块代码的第一个字符串都被视为模块的文档注释；__author__ = 'Michael Liao'import sysdef test(): args = sys.argv #用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是该.py文件的名称，运行python hello.py Michael获得的sys.argv就是['hello.py', 'Michael]。 if len(args)==1: print 'Hello, world!' elif len(args)==2: print 'Hello, %s!' % args[1] else: print 'Too many arguments!'if __name__=='__main__': test() 导入模块 123456789try: import cStringIO as StringIO #使用别名except ImportError: # 导入失败会捕获到ImportError import StringIO# 常见用法try: import json # python &gt;= 2.6except ImportError: import simplejson as json # python &lt;= 2.5 模块中__xx__这样的变量或函数是有特殊用途的，比如__doc__,一般自己的不要这样定义。 模块中_XX或__xx是私有函数或变量，不能直接引用。外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public。 安装第三方模块 pip install module-namepip install wheel Scrapy pymongo requests celery -i http://pypi.douban.com/simple --trusted-host pypi.douban.com-i 指定源，—-trusted-host 添加域名信任 导入模块时，默认情况下，Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中 123import syssys.path# 输出 ['', '/Library/Python/2.7/site-packages/pycrypto-2.6.1-py2.7-macosx-10.9-intel.egg', '/Library/Python/2.7/site-packages/PIL-1.1.7-py2.7-macosx-10.9-intel.egg', ...] 如果我们要添加自己的搜索目录，有两种方法：一是运行时修改sys.path，添加要搜索的目录：sys.path.append(‘/user/module/path’) ，运行结束后失效。 第二种在系统中设置环境变量“PYTHONPATH” 面向对象编程类和实例面向对象最重要的概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，比如Student类，而实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但各自的数据可能不同,和静态语言不同，Python允许对实例变量绑定任何数据，也就是说，对于两个实例变量，虽然它们都是同一个类的不同实例，但拥有的变量名称都可能不同。 1234567class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print '%s: %s' % (self.__name, self.__score) (object)，表示该类是从哪个类继承下来的，通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。 类的初始化方法init,传进的第一个参数是self，表示实例自身。创建实例时必须传入与init一致的变量，self除外。 属性方法和访问限制class内的属性前加上__就会变成私有变量，外部无法访问,获取或修改私有变量可以创建方法get set等，在方法中可以对参数进行检查。 类的属性仅对当前类起作用，对继承的子类是不起作用的,除非在子类中也定义__slots__，这样，子类允许定义的属性就是自身的__slots__加上父类的__slots__。 12345678910111213141516171819202122232425class Student(object): passs=Student()s.name='abc' #为一个实例添加属性，对其他实例无效 def set_age(self,age): self.age=agefrom types import MethodTypes.set_age = MethodType(set_age, s, Student) # 给实例绑定一个方法s.set_age(25)print s.age #为对象绑定一个方法def set_score(self,score): self.score=scoreStudent.set_score=MethodType(set_score,None,Student)# 如果我们想要限制class的属性怎么办？比如，只允许对Student实例添加name和age属性。# 为了达到限制的目的，Python允许在定义class的时候，定义一个特殊的__slots__变量，来限制该class能添加的属性：class Student(object): __slots__ = ('name', 'age') # 用tuple定义允许绑定的属性名称#__slots__定义的属性仅对当前类起作用，对继承的子类是不起作用的,除非在子类中也定义__slots__，这样，子类允许定义的属性就是自身的__slots__加上父类的__slots__。 有没有既能检查参数，又可以用类似属性这样简单的方式来访问类的变量呢?Python内置的@property装饰器就是负责把一个方法变成属性调用的. 12345678910111213class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError('score must be an integer!') if value &lt; 0 or value &gt; 100: raise ValueError('score must between 0 ~ 100!') self._score = value 继承和多态继承可以把父类的所有功能都直接拿过来，这样就不必重零做起，子类只需要新增自己特有的方法，也可以把父类不适合的方法覆盖重写； 多态真正的威力：调用方只管调用，不管细节，而当我们新增一种Animal类的子类时，只要确保run()方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则：对扩展开放：允许新增Animal子类；对修改封闭：不需要修改依赖Animal类型的run_twice()等函数。 123def run_twice(animal): #animal是Animal类对象的一个实例 animal.run() animal.run() 获取对象信息获取对象类型用函数type(),模块types里保存了所有的type类型常量。 123import typesprint type('abc') == types.StringType# output: True 使用isinstance(x,y)判断的是x对象是否是y类型本身，或者位于y类型的父继承链上。 dir() 获取一个对象的所有方法和属性，返回一个list。 getattr()、setattr()访问设置对象属性，hasattr() 可以测试对象的属性是否存在。 1234def readImage(fp): if hasattr(fp, 'read'): return readData(fp) return None 多重继承class dog(Mammal, Runnable): pass ,使用多继承可以避免复杂庞大的继承链。 定制类__xxx__的变量或者函数名在Python中是有特殊用途的,__slots__是为了限制类的属性，__len__()方法是为了能让class作用于len()函数。__str__()方法，打印实例；如果一个类想被用于for … in循环，类似list或tuple那样，就必须实现一个__iter__()方法，该方法返回一个迭代对象，然后，Python的for循环就会不断调用该迭代对象的next()方法拿到循环的下一个值，直到遇到StopIteration错误时退出循环。要表现得像list那样按照下标取出元素，需要实现__getitem__()方法；__getattr__()方法，动态返回一个属性，如果找不到引用的属性，则用该方法动态返回 更多可定制属性参考python官方文档 元类参考博客]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python读写XLS和CSV]]></title>
    <url>%2F2016%2F07%2F01%2F%E6%8A%80%E6%9C%AF%2Fpython%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[python读写CSV 写入并生成csv文件代码： 123456789101112131415# coding: utf-8import csvcsvfile = file('csv_test.csv', 'wb')writer = csv.writer(csvfile)writer.writerow(['姓名', '年龄', '电话'])data = [ ('小河', '25', '1234567'), ('小芳', '18', '789456')]writer.writerows(data)csvfile.close() wb中的w表示写入模式，b是文件模式写入一行用writerow多行用writerows 读取csv文件代码： 123456789# coding: utf-8import csvcsvfile = file('csv_test.csv', 'rb')reader = csv.reader(csvfile)for line in reader: print linecsvfile.close() python读写Excel 1、导入模块 import xlrd 2、打开Excel文件读取数据 data = xlrd.open_workbook(&#39;excelFile.xls&#39;) 3、使用技巧 123456789101112131415161718192021222324252627282930313233343536# 获取一个工作表table = data.sheets()[0] #通过索引顺序获取table = data.sheet_by_index(0) #通过索引顺序获取table = data.sheet_by_name(u'Sheet1')#通过名称获取# 获取整行和整列的值（数组）table.row_values(i)table.col_values(i)# 获取行数和列数nrows = table.nrowsncols = table.ncols# 循环行列表数据for i in range(nrows ): print table.row_values(i)# 单元格cell_A1 = table.cell(0,0).valuecell_C4 = table.cell(2,3).value# 使用行列索引cell_A1 = table.row(0)[0].valuecell_A2 = table.col(1)[0].value#简单的写入row = 0col = 0# 类型 0 empty,1 string, 2 number, 3 date, 4 boolean, 5 errorctype = 1 value = '单元格的值' xf = 0 # 扩展的格式化table.put_cell(row, col, ctype, value, xf) table.cell(0,0) #查看单元格的值table.cell(0,0).value #单元格的值]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github问题汇总]]></title>
    <url>%2F2016%2F05%2F18%2F%E6%8A%80%E6%9C%AF%2Fgithub%2F</url>
    <content type="text"><![CDATA[https 转换为 ssh 查看当前地址版本 git remote -v 123$ git remote -vorigin git@github.com:shuaiyy/shuaiyy.github.io.git (fetch)origin git@github.com:shuaiyy/shuaiyy.github.io.git (push) ​ 设置为ssh地址 git remote set-url origin git@github:USERNAME/OTHERREPOSITROY.git Git冲突：commit your changes or stash them before you can merge. 当本地修改未提交，使用git pull更新时，会报错 解决方法： stash通常遇到这个问题，你可以直接commit你的修改；但我这次不想这样。看看git stash是如何做的。git stashgit pullgit stash pop接下来diff一下此文件看看自动合并的情况，并作出相应修改。git stash: 备份当前的工作区的内容，从最近的一次提交中读取相关内容，让工作区保证和上次提交的内容一致。同时，将当前的工作区内容保存到Git栈中。git stash pop: 从Git栈中读取最近一次保存的内容，恢复工作区的相关内容。由于可能存在多个Stash的内容，所以用栈来管理，pop会从最近的一个stash中读取内容并恢复。git stash list: 显示Git栈内的所有备份，可以利用这个列表来决定从那个地方恢复。git stash clear: 清空Git栈。此时使用gitg等图形化工具会发现，原来stash的哪些节点都消失了。 放弃本地修改，直接覆盖之 git reset –hardgit pull​]]></content>
      <categories>
        <category>技术</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016年阅读书单]]></title>
    <url>%2F2016%2F01%2F28%2F%E9%98%85%E8%AF%BB%2FBook-List-2016%2F</url>
    <content type="text"></content>
      <categories>
        <category>生活</category>
        <category>读书</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git常用命令速查表]]></title>
    <url>%2F2015%2F01%2F30%2F%E6%8A%80%E6%9C%AF%2FGit-Resources%2F</url>
    <content type="text"><![CDATA[master: 默认开发分支 origin: 默认远程版本库 Head: 默认开发分支 Head^: Head的父提交 创建版本库12$ git clone &lt;url&gt; #克隆远程版本库$ git init #初始化本地版本库 修改和提交123456789$ git status #查看状态$ git diff #查看变更内容$ git add . #跟踪所有改动过的文件$ git add &lt;file&gt; #跟踪指定的文件$ git mv &lt;old&gt;&lt;new&gt; #文件改名$ git rm&lt;file&gt; #删除文件$ git rm --cached&lt;file&gt; #停止跟踪文件但不删除$ git commit -m "commit messages" #提交所有更新过的文件$ git commit --amend #修改最后一次改动 查看提交历史123$ git log #查看提交历史$ git log -p &lt;file&gt; #查看指定文件的提交历史$ git blame &lt;file&gt; #以列表方式查看指定文件的提交历史 撤销1234$ git reset --hard HEAD #撤销工作目录中所有未提交文件的修改内容$ git checkout HEAD &lt;file&gt; #撤销指定的未提交文件的修改内容$ git revert &lt;commit&gt; #撤销指定的提交$ git log --before="1 days" #退回到之前1天的版本 分支与标签1234567$ git branch #显示所有本地分支$ git checkout &lt;branch/tag&gt; #切换到指定分支和标签$ git branch &lt;new-branch&gt; #创建新分支$ git branch -d &lt;branch&gt; #删除本地分支$ git tag #列出所有本地标签$ git tag &lt;tagname&gt; #基于最新提交创建标签$ git tag -d &lt;tagname&gt; #删除标签 合并与衍合12$ git merge &lt;branch&gt; #合并指定分支到当前分支$ git rebase &lt;branch&gt; #衍合指定分支到当前分支 远程操作12345678$ git remote -v #查看远程版本库信息$ git remote show &lt;remote&gt; #查看指定远程版本库信息$ git remote add &lt;remote&gt; &lt;url&gt; #添加远程版本库$ git fetch &lt;remote&gt; #从远程库获取代码$ git pull &lt;remote&gt; &lt;branch&gt; #下载代码及快速合并$ git push &lt;remote&gt; &lt;branch&gt; #上传代码及快速合并$ git push &lt;remote&gt; :&lt;branch/tag-name&gt; #删除远程分支或标签$ git push --tags #上传所有标签 资料链接 Try Git]]></content>
      <categories>
        <category>技术</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Mac上用LaTeX写漂亮的简历]]></title>
    <url>%2F2014%2F12%2F06%2F%E6%8A%80%E6%9C%AF%2FMake-resume-by-LaTeX%2F</url>
    <content type="text"><![CDATA[你会搜索查看到这篇文章，相信就不需要我解释为什么要用LaTeX写Resume了：） 今晚报名Facebook China Tech Talk，最后一步需要上传简历。看着已经2年没有更新过的简历，好捉急。那时真是年轻，不舍得做减法，恨不能一张A4纸写尽一生。于是索性重新制作一份简历。 ###需要准备 安装好的LaTeX，如果没有安装请参考在Mac上通过Sublime、Skim编辑LaTeX 互联网 ###资料 Using the LaTeX Resume Templates LaTeX Templates ###步骤 在上述资料中寻找自己喜欢的模板 下载模板对应的tex文件 用LaTeX打开对应文件，编辑，编译 这个时候，如果你使用的是Mac系统，非常不幸，大多数情况下都将编译失败。因为网上多数模板需要使用windows环境下的Tex应用程序，而Mac环境下MacTex应用程序会缺少部分文件。没关系，我们有办法解决。 ###解决方案一：moderncv 进入http://www.ctan.org/pkg/moderncv 下载moderncv package 解压，找到模板文件template.tex 用已经安装好的LaTeX打开模板文件，编辑，编译，成功 但是呢，我个人觉得moderncv模板并不够好，虽然其结构清新简洁，但布局过于稀疏。没关系，我们仍然有办法。感谢一个我无意中发现的网站：ShareLaTeX.com ###解决方案二：ShareLaTeX.com也许你在上面的资料中找到了你最喜欢的模板，却苦于在Mac OS X系统下无法编译成功。这时可以求助于ShareLaTeX，这是一个在线LaTeX编辑网站，并且提供Resume,Cover Letter,Journal Article,Presentation,Thesis,Bibliographies等不同分类的多种模板。最重要的一点事，只需要确定Latex语法无误，再也不需担心什么编译环境、文件缺失等乱七八糟的问题。 进入ShareLaTeX，注册账号 点击New Project，选择CV or Resume，挑选你喜欢的简历模板 根据自己的情况编辑，自动或手动编译，保存PDF ###后记既然写到这里了，还想讲讲自己对于简历的体会。但我真的是困得不行了。。。。北京第一次不归夜。。。改天再来补全。。。。]]></content>
      <categories>
        <category>技术</category>
        <category>latex</category>
      </categories>
      <tags>
        <tag>LaTeX</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown输入LaTeX数学公式]]></title>
    <url>%2F2014%2F09%2F16%2F%E6%8A%80%E6%9C%AF%2FMarkdown-Math%2F</url>
    <content type="text"><![CDATA[Markdown是读写性都非常好的轻量文本编辑语言，这个博客以及世界上许多博客的文章都是用其书写的。但是，在写“科研”博客时，难免会需要频繁地输入数学公式，而Markdown本身并不支持数学公式的输入。我曾经想偷懒直接用Markdown的语法去代替LaTeX数学公式，最后页面显示的结果有点儿丑。却一直也没有去修改。直到前天收到了印卧涛老师的一封邮件，邮件里所有的数学公式都是用LaTeX代码写的，正规而美观。由此觉得自己做事还是水了点。做事要认真啊亲。 本文默认我们是会使用LaTeX编辑数学公式的。 ###解决办法： 将数学公式以图片形式保存，再在Markdown中将其插入。 或者，使用LaTeX在线编辑器，输入数学公式，获得html代码，将其插入Markdown。 ###步骤： 进入CodeCogs 在盒子里书写公式 在页面下方复制html代码 将复制的html代码拷贝到Markdown里 ###缺点：Markdown文件的易读性却因此下降了很多。]]></content>
      <categories>
        <category>技术</category>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>LaTeX</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 0：实际问题]]></title>
    <url>%2F2014%2F09%2F12%2F%E6%8A%80%E6%9C%AF%2FLinux-Problems%2F</url>
    <content type="text"><![CDATA[《鸟哥的Linux私房菜——基础学习篇》 《鸟哥的Linux私房菜——服务器架设篇》 本系列文章分为两部分： 系统学习上面两本书的笔记。 实际中遇到的问题及解决方案，即本文内容。 ##实际问题 ##1. 建立网络映射 Mac：Finder-&gt;前往-&gt;连接服务器-&gt;输入smb://IPaddress/samba-&gt;连接 Linux：位置-&gt;连接服务器-&gt;“服务类型”选择自定义位置-&gt;输入smb://IPaddress/samba-&gt;连接 ##2. ssh登陆失败以root身份远程登陆服务器，密码正确，却显示如下警告： 12345678910111213@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that a host key has just been changed.The fingerprint for the RSA key sent by the remote host is3a:17:4b:6e:62:e6:94:df:09:78:99:90:51:68:18:62.Please contact your system administrator.Add correct host key in /Users/AnyaLin/.ssh/known_hosts to get rid of this message.Offending RSA key in /Users/AnyaLin/.ssh/known_hosts:4RSA host key for 222.195.93.129 has changed and you have requested strict checking.Host key verification failed. 解决方法： 12345vi ~/.ssh/known_hosts #选中最后一条登陆记录，双击`d`删除，按“：”进入末行编辑模式，输入“x”，回车ssh root@222.195.93.129 #再次登陆The authenticity of host '222.195.93.129 (222.195.93.129)' can't be established.RSA key fingerprint is 3a:17:4b:6e:62:e6:94:df:09:78:99:90:51:68:18:62.Are you sure you want to continue connecting (yes/no)? #输入yes ##3. tar.gz 文件解压 打开终端 进入需要解压的xxxx.tar.gz文件所在目录 $ tar xvfz xxxx.tar.gz -C /指定的目录 压缩并打包目录 123tar -czf small.tar.gz small(目录名)tar zcvf backup.tar.gz site/* --exclude=site/attach --exclude=site/images注意 --exclude后面的排除目录后不能带/ ，否则不起作用 ​ ##4. 新建文件命令 1touch a.txt =======]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Mac上通过Sublime、Skim编辑LaTeX]]></title>
    <url>%2F2014%2F08%2F10%2F%E6%8A%80%E6%9C%AF%2FUsing-LaTeX-with-Sublime-and-Skim-for-Mac%2F</url>
    <content type="text"><![CDATA[Sublime Text是一款非常优秀的编辑器，速度快，界面简洁，插件众多。并且能跨平台使用，在Mac和Windows上都能完美使用。虽然是一款付费软件，但作者很厚道地给了无限期的试用期限。这一切正如其官网广告词说的那样：The text editor you’ll fall in love with. Skim是一款免费轻量的PDF阅读、标注工具，布局贴心友好，与OS X自带的Previewer相比，Skim能更好的注释PDF文件。 LaTeX是一款权威的科技论文排版软件，不仅可以写论文，也可以处理日常的各种文档工作，甚至是做幻灯片。相比于Word，LaTeX最大的优势是对于复杂公式的编辑与排版非常漂亮。并且用简单的命令就可以生成脚注、索引、目录和参考文献等复杂的结构。这一切优点都使得世界上众多的“科学家”们不再需要身兼作者与排版工两职，从而将更多的精力集中于文章内容本身。 本文的目的是将上述三种软件综合部署在Mac上。完成之后，你将可以在Sublime Text里面进行LaTeX代码编辑，用Skim预览生成的PDF文件。更重要的是，让你觉得，写论文也可以是一件很优美的事。 ###准备工作： Mac上至少4GB的空余空间 高速的互联网连接 ###第一步：安装MacTeX 进入MacTeX官网下载MacTeX.pkg文件。文件大约2GB，需要一段时间才能完成下载，趁现在去喝杯咖啡吧。 下载完成之后，双击MacTeX.pkg进行安装。 安装完成之后，会看到许多与TeX有关的程序图标，暂时忽略它们。 ###第二步：安装Sublime Text 进入Sublime Text官网下载最新版本的Sublime Text。这里我下载的是Sublime Text 3. 下载完成之后，将文件拖入应用程序文件夹安装。 ###第三步：在Sublime Text中安装Package Control我们需要在Sublime Text中下载插件以便能够很好地操作与LaTeX有关的文件。而插件是通过Package Control下载的。 进入Package Control官网复制灰色区块的代码。 打开Sublime Text。 使用快捷键“control+~”（~就在Esc键的下方）打开控制面板Console。你会在Sublime Text的底部看到弹出一个白色窗口。 将刚才复制的代码粘贴到控制面板。 按下“Enter”回车键。然后退出并重启Sublime Text。 ###第四步：安装LaTeX Tools Sublime Text重启后，按下“Command+Shift+P”打开命令托盘Command pallet，这一步也可以通过Tools下拉菜单完成。 在命令托盘里输入“Install Package”，按下Enter回车建。 完成之后，输入“LaTeX Tools”，找到这一项并回车安装。 退出并重启Sublime Text。 ###第五步：安装Skim 进Skim下载Skim并安装 打开Skim，在菜单栏中Skim &gt; Preference(选项) &gt; Sync(同步) 在预设菜单中选择Sublime Text 关闭上面这个窗口。 ###全部完成，✌️现在，我们已经做完了所有的步骤，可以打开Sublime Text，Command+N新建文件并在里面编写LaTeX代码了，完成编辑之后，Command+S保存文件，Command+B编译并运行，这时就可以在Skim里面看到PDF预览了。]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>LaTeX Mac</tag>
      </tags>
  </entry>
</search>
