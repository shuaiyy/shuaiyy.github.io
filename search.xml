<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2017%2F11%2F07%2F%E6%8A%80%E6%9C%AF%2FPyQt5%20%E4%BD%BF%E7%94%A8pyinstaller%E6%89%93%E5%8C%85PyQt5%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[使用pyinstaller打包PyQt5程序]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F11%2F07%2F%E6%8A%80%E6%9C%AF%2FPython%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Python多线程网络编程基础TCP/UDP协议的特点套接字]]></content>
  </entry>
  <entry>
    <title><![CDATA[Gevent 源码分析]]></title>
    <url>%2F2017%2F11%2F05%2F%E6%8A%80%E6%9C%AF%2FGevent%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Gevent 源码分析 gevent包括2个重要的部分，libev和greenlet。 libev实现事件循环， watcher(叶子，事件监控处理)，ev_run(主干，事件循环引擎)，ev_loop(watcher管理)。 greenlet提供对协程的完整支持，用于执行异步任务。 源码分析工具：pycharm ctrl + B 跳转到对象声明的源码位置， ctrl + alt + 箭头 前进或后退 选择右侧导航栏的Structure窗口，可以快速的查看类的属性和方法。 下载gevent源码 从github上下载源码 源码目录结构 doc：项目文档 examples：简单的使用示例 benchmarks：压力测试 src：源代码所在目录 __init__.py __all__属性由列表构成，它规定了模块的所有可见方法，会使属性列表之外的成员全部私有化。 只有在执行语句 from module import * 时，__all__属性才会起作用。此时所有枚举的成员被import，而其他成员被私有化。 它不仅在第一时间展现了模块的内容大纲，而且也更清晰的提供了外部访问接口。 __dependencies_for_freezing() ，为打包工具如py2exe等指明hidden-import 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849__version__ = '1.3.0.dev0'__all__ = ['get_hub', 'Greenlet', 'GreenletExit', 'spawn', 'spawn_later', 'spawn_raw', 'iwait', 'wait', 'killall', 'Timeout', 'with_timeout', 'getcurrent', 'sleep', 'idle', 'kill', 'signal', 'fork', 'reinit']import sysif sys.platform == 'win32': # trigger WSAStartup call import socket # pylint:disable=unused-import,useless-suppression del socketfrom gevent.hub import get_hub, iwait, waitfrom gevent.greenlet import Greenlet, joinall, killalljoinall = joinall # export for pylintspawn = Greenlet.spawnspawn_later = Greenlet.spawn_laterfrom gevent.timeout import Timeout, with_timeoutfrom gevent.hub import getcurrent, GreenletExit, spawn_raw, sleep, idle, kill, reinittry: from gevent.os import forkexcept ImportError: __all__.remove('fork')from gevent.hub import signal as _signal_classfrom gevent import signal as _signal_moduledef __dependencies_for_freezing(): # pylint:disable=unused-variable from gevent import core from gevent import resolver_thread from gevent import resolver_ares from gevent import socket as _socket from gevent import threadpool from gevent import thread from gevent import threading from gevent import select from gevent import subprocess import pprint import traceback import signal as _signaldel __dependencies_for_freezing libev原理libev源码解读 Reactor模式 工作流程： 获取ev_loop实例，它代表了一个事件循环，也是代码的主要组织者。 创建和初始化watcher，并绑定到loop实例。libev中定义了一系列的watcher（如io，timer），每类watcher负责一类特定的事件。当loop中检测到感兴趣的事件发生，便会通知相关的watcher。 启动事件循环，ev_run函数。事件循环启动后，当前线程/进程将会被阻塞，直到循环被终止。 当watcher监听的事件发生时，wathcher被放入就绪状态队列，等待调用（执行watcher的回调函数）。 watcher对象： watcher是Reactor中的Event Handler。一方面，它向事件循环提供了统一的调用接口，监听事件;另一方面，它是外部代码的注入口，维护着具体的watcher信息，如：绑定的回调函数，watcher的优先级，是否激活等。 active: 表示当前watcher是否被激活。 pending: 表示当前watcher有事件就绪，等待处理。 priority: 是当前watcher的优先级； data: 附加数据指针，用来在watcher中携带额外所需的数据； cb：是事件触发后的回调函数定义。 ev_loop对象: ev_loop则是一个Reactor的角色，是事件循环的上下文环境，就像一根竹签，把前面的watcher实例像糖葫芦一样串起来。ev_loop实现对watcher的管理，维护watcher就绪队列，触发watcher执行。 ev_run： 执行事件循环的的引擎，即Reactor模式中的select方法。通过向ev_run函数传递一个ev_loop实例，便可以开启一个事件循环。 ev_run实际上是一个巨大的do-while循环，期间会检查loop中注册的各种watcher的事件。如果有事件就绪，则触发相应的watcher。这个循环会一直持续到ev_break被调用或者无active的watcher为止。当然，也可以通过传递EVRUN_NOWAIT或EVRUN_ONCE等flag来控制循环的阻塞行为。 gevent core 由于gevent封装的libev是c语言实现的事件循环框架，因此了解libev的工作原理是非常重要的。 core.py 123from gevent.libev import corecext as _core# CFFI/PyPyfrom gevent.libev import corecffi as _core 从gevent.libev中导入_core,分python版和pypy版本。 gevent.libev corecext.ppyx gevent封装了libev。libev是c实现的高效事件循环框架，其核心为ev_run,主要要做了五件事情： 更新更改的FD事件 进行必要的sleep backend_poll收集pending的IO事件 收集pending的timer事件 调用所有pending的事件 corecext.ppyx是cython写的，看不懂 corecext.ppyx用Cython实现loop类，编译时会转换为gevent.core.c-&gt;gevent.core.so，对libev的结构体和接口进行封装，比如时间循环loop，回调callback，观察者watcher 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def __init__(self, object flags=None, object default=None, size_t ptr=0): cdef unsigned int c_flags cdef object old_handler = None libev.ev_prepare_init(&amp;self._prepare, &lt;void*&gt;gevent_run_callbacks)#ifdef _WIN32 libev.ev_timer_init(&amp;self._periodic_signal_checker, &lt;void*&gt;gevent_periodic_signal_check, 0.3, 0.3)#endif libev.ev_timer_init(&amp;self._timer0, &lt;void*&gt;gevent_noop, 0.0, 0.0) # ........................ libev.ev_prepare_start(self._ptr, &amp;self._prepare) libev.ev_unref(self._ptr) self._callbacks = []# ........cdef public class loop [object PyGeventLoopObject, type PyGeventLoop_Type]: # loop对象的实现 cdef public class watcher [object PyGeventWatcherObject, type PyGeventWatcher_Type]: cdef public class io(watcher) [object PyGeventIOObject, type PyGeventIO_Type]: cdef public class timer(watcher) [object PyGeventTimerObject, type PyGeventTimer_Type]: cdef _run_callbacks(self): cdef callback cb cdef object callbacks cdef int count = 1000 libev.ev_timer_stop(self._ptr, &amp;self._timer0) while self._callbacks and count &gt; 0: callbacks = self._callbacks self._callbacks = [] for cb in callbacks: libev.ev_unref(self._ptr) gevent_call(self, cb) count -= 1 if self._callbacks: libev.ev_timer_start(self._ptr, &amp;self._timer0) def run(self, nowait=False, once=False): CHECK_LOOP2(self) cdef unsigned int flags = 0 if nowait: flags |= libev.EVRUN_NOWAIT if once: flags |= libev.EVRUN_ONCE with nogil: libev.ev_run(self._ptr, flags) def run_callback(self, func, *args): CHECK_LOOP2(self) cdef callback cb = callback(func, args) self._callbacks.append(cb) libev.ev_ref(self._ptr) return cb 查看core的对象loop及其方法 pycharm的python shell（ipython）有自动补全，当单步执行时，可以利用其查看对象对外提供的方法 或者使用debug查看对象 12345678910111213141516171819202122232425262728def hello(): print 'hello'from gevent import coreloop = core.loop()# loop对象对外提供的方法和属性loop.run_callback(hello)loop.run()loop.depthloop.fileno()loop.update()loop.now()t = loop.timer(3, 2) # 每3秒执行2次t = loop.timer(3) # 3秒后执行并退出t.start(hello)loop.run() # 必须loop运行起来，timer事件监听才会执行。# ------- timer对象的声明和start方法class timer(watcher): _watcher_type = 'ev_timer' def __init__(self, loop, after=0.0, repeat=0.0, ref=True, priority=None): if repeat &lt; 0.0: raise ValueError("repeat must be positive or zero: %r" % repeat) watcher.__init__(self, loop, ref=ref, priority=priority, args=(after, repeat)) def start(self, callback, *args, **kw): # ... 直接ctrl + B，查看loop对象的声明。loop初始化了几个watcher，重要的方法有run_callback, run, timer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class loop(object): error_handler = None def __init__(self, flags=None, default=None): self._in_callback = False self._callbacks = [] # self._check is a watcher that runs in each iteration of the mainloop, just after the blocking call self._check = ffi.new("struct ev_check *") # self._prepare is a watcher that runs in each iteration of the mainloop, just before the blocking call self._prepare = ffi.new("struct ev_prepare *") # A timer we start and stop on demand. If we have callbacks, self._timer0 = ffi.new("struct ev_timer *") libev.ev_timer_init(self._timer0, libev.gevent_noop, 0.0, 0.0) def _run_callbacks(self, _evloop, _, _revents): count = 1000 libev.ev_timer_stop(self._ptr, self._timer0) while self._callbacks and count &gt; 0: callbacks = self._callbacks self._callbacks = [] for cb in callbacks: self.unref() callback = cb.callback args = cb.args if callback is None or args is None: # it's been stopped continue cb.callback = None try: callback(*args) except: # 。。。 if self._callbacks: libev.ev_timer_start(self._ptr, self._timer0) def destroy(self): # 销毁loop对象 def run(self, nowait=False, once=False): flags = 0 if nowait: flags |= libev.EVRUN_NOWAIT if once: flags |= libev.EVRUN_ONCE libev.ev_run(self._ptr, flags) def timer(self, after, repeat=0.0, ref=True, priority=None): return timer(self, after, repeat, ref, priority) def callback(self, priority=None): return callback(self, priority) def run_callback(self, func, *args): cb = callback(func, args) self._callbacks.append(cb) self.ref() return cb def fileno(self): if self._ptr: fd = self._ptr.backend_fd if fd &gt;= 0: return fd gevent hub hub是greentlet子类，一个greenlet对象，是main greenlet。 hub是gevent的核心，依赖libev调度所有greenlet 当有协程需要调度时，主协程调用switch方法切换到子协程。子协程阻塞或者主动调用switch时，切回主协程，由主协程调度其他协程运行。 主要的对象和方法 get_hub sleep kill wait iwait signalClass HubClass WaiterClass sleep触发调度当执行gevent.sleep(0)语句时，当前协程立即被切换出去，回到gevent的主协程，紧接着主协程执行调度其他协程执行。我们从sleep函数入手分析。 gevent.sleep sleep(0)意味着，协程立即yield，其他runnable greenlets将有机会被执行。等到下次loop到该协程时，它才会被恢复执行。 具体流程：通过get_hub获取主协程和loop对象，然后调用waiter.switch或者hub.wait 12345678910111213# ------- hub.py -----def sleep(seconds=0, ref=True): """ Put the current greenlet to sleep for at least *seconds*. """ hub = get_hub() loop = hub.loop if seconds &lt;= 0: waiter = Waiter() loop.run_callback(waiter.switch) waiter.get() else: hub.wait(loop.timer(seconds, ref=ref)) get_hub 从当前线程上下文中获取hub对象，如果没有就创建一个 12345678910# ------- hub.py -----def get_hub(*args, **kwargs): """ Return the hub for the current thread. """ hub = _threadlocal.hub if hub is None: hubtype = get_hub_class() hub = _threadlocal.hub = hubtype(*args, **kwargs) return hub hub.loop 默认是一个GEVENT_LOOP对象 Waiter Waiter是低层次的greenlets通信工具，用来安全的实现switch()或throw()调用。类中的doc如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Waiter(object): """ A low level communication utility for greenlets. Waiter is a wrapper around greenlet's ``switch()`` and ``throw()`` calls that makes them somewhat safer: * switching will occur only if the waiting greenlet is executing :meth:`get` method currently; * any error raised in the greenlet is handled inside :meth:`switch` and :meth:`throw` * if :meth:`switch`/:meth:`throw` is called before the receiver calls :meth:`get`, then :class:`Waiter` will store the value/exception. The following :meth:`get` will return the value/raise the exception. The :meth:`switch` and :meth:`throw` methods must only be called from the :class:`Hub` greenlet. The :meth:`get` method must be called from a greenlet other than :class:`Hub`. &gt;&gt;&gt; result = Waiter() &gt;&gt;&gt; timer = get_hub().loop.timer(0.1) &gt;&gt;&gt; timer.start(result.switch, 'hello from Waiter') &gt;&gt;&gt; result.get() # blocks for 0.1 seconds 'hello from Waiter' If switch is called before the greenlet gets a chance to call :meth:`get` then :class:`Waiter` stores the value. &gt;&gt;&gt; result = Waiter() &gt;&gt;&gt; timer = get_hub().loop.timer(0.1) &gt;&gt;&gt; timer.start(result.switch, 'hi from Waiter') &gt;&gt;&gt; sleep(0.2) &gt;&gt;&gt; result.get() # returns immediatelly without blocking 'hi from Waiter' .. warning:: This a limited and dangerous way to communicate between greenlets. It can easily leave a greenlet unscheduled forever if used incorrectly. Consider using safer classes such as :class:`gevent.event.Event`, :class:`gevent.event.AsyncResult`, or :class:`gevent.queue.Queue`. """ def switch(self, value=None): """Switch to the greenlet if one's available. Otherwise store the value.""" greenlet = self.greenlet if greenlet is None: self.value = value self._exception = None else: assert getcurrent() is self.hub, "Can only use Waiter.switch method from the Hub greenlet" switch = greenlet.switch try: switch(value) except: # pylint:disable=bare-except self.hub.handle_error(switch, *sys.exc_info()) def get(self): """If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called.""" Waiter的switch方法，如果Waiter对象当前绑定了greenlet对象(hub实例)，就调用greenlet.switch方法。 hub.wait(loop.timer(seconds, ref=ref)) 等待watcher对象（计时器timer）就绪之前，阻塞该协程 12345678910111213141516171819202122# -------- hub.py ------- def wait(self, watcher): """ Wait until the *watcher* (which should not be started) is ready. The current greenlet will be unscheduled during this time. .. seealso:: :class:`gevent.core.io`, :class:`gevent.core.timer`, :class:`gevent.core.signal`, :class:`gevent.core.idle`, :class:`gevent.core.prepare`, :class:`gevent.core.check`, :class:`gevent.core.fork`, :class:`gevent.core.async`, :class:`gevent.core.child`, :class:`gevent.core.stat` """ waiter = Waiter() unique = object() watcher.start(waiter.switch, unique) try: result = waiter.get() if result is not unique: raise InvalidSwitchError('Invalid switch into %s: %r (expected %r)' % (getcurrent(), result, unique)) finally: watcher.stop() ​ gevent socket 当python底层的socket使用非阻塞模式时，执行socket操作会立即返回exception。gevent的socket在原始socket的基础上，通过处理exception，并监听socket 可读可写事件来实现异步。 12345import geventfrom gevent import socketconn = socket.create_connection(('localhost', '8888'))print conn socket.py 协作式底层网络接口 提供了套接字操作和相关函数。API与python标准库一致。但同步函数只会阻塞当前协程，其他协程会继续运行。 主要方法分析： create_connection 创建socket对象,调用socket.connect方法 主要sock = socket(af, socktype, proto) ,这里的socket是gevent封装的一个对象，在_socket2.py中定义的。 123456789101112131415161718192021# ----- gevent socket.py -------def create_connection(address, timeout=_GLOBAL_DEFAULT_TIMEOUT, source_address=None): """Connect to *address* and return the socket object. and return the socket object. """ host, port = address err = None for res in getaddrinfo(host, port, 0 if has_ipv6 else AF_INET, SOCK_STREAM): af, socktype, proto, _, sa = res sock = None try: sock = socket(af, socktype, proto) if timeout is not _GLOBAL_DEFAULT_TIMEOUT: sock.settimeout(timeout) if source_address: sock.bind(source_address) sock.connect(sa) return sock except error as ex: # .... _socket2.py 中socket对象 实现gevent封装的socket对象，接口和python原始的socket一致。 self._sock = _realsocket(family, type, proto)创建一个real socket（python的socket对象） self._sock.setblocking(0)设置real socket为非阻塞的工作模式 self.hub = get_hub()拿到gevent中的主协程greenlet对象，hub 设置两个监听事件，可读和可写事件监听 self._read_event = io(fileno, 1) self._write_event = io(fileno, 2) 1234567891011121314151617181920212223242526# ----- gevent _socket2.py -----class socket(object): """ gevent `socket.socket &lt;https://docs.python.org/2/library/socket.html#socket-objects&gt;`_ for Python 2. This object should have the same API as the standard library socket linked to above. """ def __init__(self, family=AF_INET, type=SOCK_STREAM, proto=0, _sock=None): if _sock is None: self._sock = _realsocket(family, type, proto) self.timeout = _socket.getdefaulttimeout() else: if hasattr(_sock, '_sock'): self._sock = _sock._sock self.timeout = getattr(_sock, 'timeout', False) if self.timeout is False: self.timeout = _socket.getdefaulttimeout() else: self._sock = _sock self.timeout = _socket.getdefaulttimeout() self._sock.setblocking(0) fileno = self._sock.fileno() self.hub = get_hub() io = self.hub.loop.io self._read_event = io(fileno, 1) self._write_event = io(fileno, 2) gevent socket对象的方法实现: 打开pycharm的Structure导航窗口。 connect 使用了非常巧妙的while循环，执行result = sock.connect_ex(address)进行socket连接，如果没有异常，则连接成功，跳出while。如果返回异常如连接未建立，缓冲区满等，则调用self._wait(self._write_event)等待socket变成可写状态(表示连接成功)，在继续while，等待期间协程交出运行权。 重点socket._wait,它调用了self.hub.wait(watcher),实现阻塞当前的greenlet，等待对应的watcher就绪 123456789101112131415161718192021222324252627282930313233343536373839404142434445# ----- gevent _socket2.py -----class socket(object): def connect(self, address): if self.timeout == 0.0: return self._sock.connect(address) sock = self._sock if isinstance(address, tuple): r = getaddrinfo(address[0], address[1], sock.family) address = r[0][-1] if self.timeout is not None: timer = Timeout.start_new(self.timeout, timeout('timed out')) else: timer = None try: while True: err = sock.getsockopt(SOL_SOCKET, SO_ERROR) if err: raise error(err, strerror(err)) result = sock.connect_ex(address) if not result or result == EISCONN: break elif (result in (EWOULDBLOCK, EINPROGRESS, EALREADY)) or (result == EINVAL and is_windows): self._wait(self._write_event) else: raise error(result, strerror(result)) finally: if timer is not None: timer.cancel() def _wait(self, watcher, timeout_exc=timeout('timed out')): """Block the current greenlet until *watcher* has pending events.""" if watcher.callback is not None: raise _socketcommon.ConcurrentObjectUseError('This socket is already used by another greenlet: %r' % (watcher.callback, )) if self.timeout is not None: timeout = Timeout.start_new(self.timeout, timeout_exc, ref=False) else: timeout = None try: self.hub.wait(watcher) finally: if timeout is not None: timeout.cancel() send 首先拿到原生的socket(self._sock),然后调用socket.send发送数据，因为socket是非阻塞的，所以会立即抛出异常 EWOULDBLOCK，然后调用 self._wait(self._write_event), 等待可写事件发生后再次send数据。 1234567891011121314151617181920# ----- gevent _socket2.py -----class socket(object): def send(self, data, flags=0, timeout=timeout_default): sock = self._sock if timeout is timeout_default: timeout = self.timeout try: return sock.send(data, flags) except error as ex: if ex.args[0] != EWOULDBLOCK or timeout == 0.0: raise sys.exc_clear() self._wait(self._write_event) try: return sock.send(data, flags) except error as ex2: if ex2.args[0] == EWOULDBLOCK: return 0 raise recv send重试一次如果失败就会 return 0，而recv使用while循环，不停的尝试读数据，知道成功返回数据。 当有EWOULDBLOCK异常时，等待self._wait(self._read_event)可读事件发生，然后再次尝试读取数据。 1234567891011121314# ----- gevent _socket2.py -----class socket(object): def recv(self, *args): sock = self._sock # keeping the reference so that fd is not closed during waiting while True: try: return sock.recv(*args) except error as ex: if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0: raise # QQQ without clearing exc_info test__refcount.test_clean_exit fails sys.exc_clear() self._wait(self._read_event) accept 同样是while循环，当接收连接成功时跳出循环，然后创建一个gevent socket对象（sockobj = socket(_sock=client_socket)）并返回。如果发生异常则阻塞等待可读事件发生（self._wait(self._read_event)）。 123456789101112131415161718# ----- gevent _socket2.py -----class socket(object): def accept(self): sock = self._sock while True: try: client_socket, address = sock.accept() break except error as ex: if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0: raise sys.exc_clear() self._wait(self._read_event) sockobj = socket(_sock=client_socket) if PYPY: client_socket._drop() return sockobj, address gevent server BaseServer为服务器端实现了一些基础功能的抽象基类 StreamServer实现了通用TCPServer，在监听端口上接受新连接并为每个连接创建一个协程，协程函数是用户提供的。 分析Select，Poll部分的实现 12345678910111213141516from gevent import monkeyfrom gevent.server import StreamServermonkey.patch_all()def handler(sock, addr): while True: data = sock.recv(1024) if data: print data else: print addr, 'closed!' returnif __name__ == '__main__': server = StreamServer(('localhost', 12222), handler) server.serve_forever() StreamServer从StreamServer对象入手，其doc文档表明，这是一个通用的TCP Server，从监听的sock里接收连接，然后根据用户提供的handle回调函数spawn（生成，创建并运行greenlet）出协程。handle回调函数接收两个对象，socket连接和客户端地址。 backlog为服务器最大接收的连接数，默认为256. 只有服务器初始化，新创建socket时才需要在init里传入此参数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# ---- gevent server.py ------class StreamServer(BaseServer): """ A generic TCP server. Accepts connections on a listening socket and spawns user-provided *handle* function for each connection with 2 arguments: the client socket and the client address. """ # the default backlog to use if none was provided in __init__ backlog = 256 reuse_addr = DEFAULT_REUSE_ADDR def __init__(self, listener, handle=None, backlog=None, spawn='default', **ssl_args): BaseServer.__init__(self, listener, handle=handle, spawn=spawn) try: if ssl_args: # 初始化ssl参数 if backlog is not None: if hasattr(self, 'socket'): raise TypeError('backlog must be None when a socket instance is passed') self.backlog = backlog except: self.close() raise def set_listener(self, listener): BaseServer.set_listener(self, listener) try: self.socket = self.socket._sock except AttributeError: pass def init_socket(self): if not hasattr(self, 'socket'): self.socket = self.get_listener(self.address, self.backlog, self.family) self.address = self.socket.getsockname() if self.ssl_args: # 封装支持ssl socket的handle函数 self._handle = self.wrap_socket_and_handle else: self._handle = self.handle if PY3: def do_read(self): # 。。。 else: # 针对python2的 do_read函数 def do_read(self): try: client_socket, address = self.socket.accept() except _socket.error as err: if err.args[0] == EWOULDBLOCK: return raise sockobj = socket(_sock=client_socket) if PYPY: client_socket._drop() return sockobj, address def do_close(self, sock, *args): sock.close() def wrap_socket_and_handle(self, client_socket, address): # used in case of ssl sockets ssl_socket = self.wrap_socket(client_socket, **self.ssl_args) return self.handle(ssl_socket, address) BaseServer.__init__ 主要是初始化listener，spawn，handle， loop。 1234567891011121314151617# ----- BaseServer.__init__ ---- def __init__(self, listener, handle=None, spawn='default'): self._stop_event = Event() self._stop_event.set() self._watcher = None self._timer = None self._handle = None self.pool = None try: self.set_listener(listener) self.set_spawn(spawn) self.set_handle(handle) self.delay = self.min_delay self.loop = get_hub().loop except: self.close() raise server_forever() 是基类BaseServer里提供的方法，如果server没有启动，则启动(start方法)并ready for 接收数据，直至停止。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# ----- baseserver.py BaseServer ---- def serve_forever(self, stop_timeout=None): """Start the server if it hasn't been already started and wait until it's stopped.""" if not self.started: self.start() try: self._stop_event.wait() finally: Greenlet.spawn(self.stop, timeout=stop_timeout).join() def start(self): self.init_socket() self._stop_event.clear() try: self.start_accepting() except: self.close() raise def start_accepting(self): if self._watcher is None: self._watcher = self.loop.io(self.socket.fileno(), 1) self._watcher.start(self._do_read) def _do_read(self): for _ in xrange(self.max_accept): if self.full(): self.stop_accepting() return try: args = self.do_read() self.delay = self.min_delay if not args: return except: # 。。。。 else: try: self.do_handle(*args) except: self.loop.handle_error((args[1:], self), *sys.exc_info()) if self.delay &gt;= 0: self.stop_accepting() self._timer = self.loop.timer(self.delay) self._timer.start(self._start_accepting_if_started) self.delay = min(self.max_delay, self.delay * 2) break def do_read(self): raise NotImplementedError() def do_handle(self, *args): spawn = self._spawn handle = self._handle close = self.do_close try: if spawn is None: _handle_and_close_when_done(handle, close, args) else: spawn(_handle_and_close_when_done, handle, close, args) except: close(*args) raise start()方法 首先self.init_socket()初始化socket，用clear将self._stop_event停止事件设为false状态，然后调用self.start_accepting() 接收连接请求。 init_socket() init_socket由继承BaseServer的StreamServer实现,主要是获取socket和address，然后绑定handle方法，如果是ssl，则要先对handle方法进行ssl支持的封装。 123456789# --- server.py StreamServer ---- def init_socket(self): if not hasattr(self, 'socket'): self.socket = self.get_listener(self.address, self.backlog, self.family) self.address = self.socket.getsockname() if self.ssl_args: self._handle = self.wrap_socket_and_handle else: self._handle = self.handle self.start_accepting() 设置一个watcher进行事件监听，watcher绑定了_do_read()方法 _do_read()： 如果达到最大连接数，停止接收新连接， 否则执行do_read()方法 do_read()由子类实现，在StreamServer中的实现如下，就是通过self.socket.accept()获取socket连接对象，并返回gevent socket对象和客户端地址。 123456789101112# server.py StreamServerdef do_read(self): try: client_socket, address = self.socket.accept() except _socket.error as err: if err.args[0] == EWOULDBLOCK: return raise sockobj = socket(_sock=client_socket) if PYPY: client_socket._drop() return sockobj, address ​ 然后执行self.do_handle(*args)方法 self.do_handle(*args)方法调用的是_handle_and_close_when_done,这是个函数，不是类方法 _handle_and_close_when_done 函数主要是执行handle，并调用close关闭handle。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# ----- baseserver.py BaseServer ----def _handle_and_close_when_done(handle, close, args_tuple): # 6 try: return handle(*args_tuple) finally: close(*args_tuple)class BaseServer(object): def start_accepting(self): if self._watcher is None: self._watcher = self.loop.io(self.socket.fileno(), 1) self._watcher.start(self._do_read) # 1 def _do_read(self): # 1 for _ in xrange(self.max_accept): if self.full(): self.stop_accepting() # 2 return try: args = self.do_read() # 3 self.delay = self.min_delay if not args: return except: self.loop.handle_error(self, *sys.exc_info()) ex = sys.exc_info()[1] if self.is_fatal_error(ex): self.close() sys.stderr.write('ERROR: %s failed with %s\n' % (self, str(ex) or repr(ex))) return if self.delay &gt;= 0: self.stop_accepting() self._timer = self.loop.timer(self.delay) self._timer.start(self._start_accepting_if_started) self.delay = min(self.max_delay, self.delay * 2) break else: # 4 try: self.do_handle(*args) # 4 except: self.loop.handle_error((args[1:], self), *sys.exc_info()) if self.delay &gt;= 0: self.stop_accepting() self._timer = self.loop.timer(self.delay) self._timer.start(self._start_accepting_if_started) self.delay = min(self.max_delay, self.delay * 2) break def do_handle(self, *args): # 4 spawn = self._spawn handle = self._handle close = self.do_close try: if spawn is None: _handle_and_close_when_done(handle, close, args) # 5 else: spawn(_handle_and_close_when_done, handle, close, args) # 5 except: close(*args) raise ​ ​]]></content>
      <categories>
        <category>技术</category>
        <category>并发编程</category>
        <category>Gevent</category>
      </categories>
      <tags>
        <tag>gevent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gevent 实现网络爬虫]]></title>
    <url>%2F2017%2F11%2F04%2F%E6%8A%80%E6%9C%AF%2FGevent%E7%88%AC%E8%99%AB%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E8%99%AB%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[Gevent爬虫与多线程爬虫对比爬虫通用网页解析模块 网页字符集检测 根据指定规则解析数据 url处理子模块 能够抽取页面中所有的url以供后续爬取 抽取的url过滤处理，如图片，文件等url肯定不需要递归爬取 抽取的url分类http 和 https 判断url中是否有非法字符 Gevent爬虫 每个任务是一个url object，存储url，页面深度等信息 1234class UrlObject(object): def __init__(self, url, depth): self.url = url self.depth = depth 使用greenlet来执行下载网页、解析内容、feed url的任务,任务从队列里获取 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Handler(Greenlet): # urlObj：任务对象；spider爬虫对象；setting爬虫的设置信息 def __init__(self, urlObj, spider, settings): super(Handler, self).__init__() self.url_obj = urlObj self.spider = spider self.settings = settings # 下载html def download(self, url): print u'开始下载：', url res = requests.get(url) text = res.text print u'下载完成！' return text # 从html中发掘新的任务url def feed(self, html): soup = BeautifulSoup(html, 'lxml') urls = [i.get('href') for i in soup.select('a[href^="http"]')] return urls # 从Html中提取数据 def parse_html(self, html): soup = BeautifulSoup(html, 'lxml') title = soup.find('title') print title.text if title else '' # 处理文本也可以用greenlet对象 pass def _run(self): if not hash(self.url_obj.url) in self.spider.finished_set: # 如果没下载过该网页，则下载，并处理 html = self.download(self.url_obj.url) # 加入已处理的集合 self.parse_html(html) self.spider.finished_set.add(hash(self.url_obj.url)) # 页面深度+1 depth = self.url_obj.depth + 1 # 发现新的url任务,如果没被执行就加入到任务队列 for url in self.feed(html): if not hash(url) in self.spider.finished_set: url_obj = UrlObject(url, depth) self.spider.queue.put(url_obj) pool控制greenlet的并发 使用threading的Timer计时器来控制主线程，当计时结束，结束爬虫。 pool.join() 等待pool里的greenlet结束。 timer.cancel 跳出spider调度的主循环的条件 任务调度的主循环内，判断timer的状态，如果timer是激活状态，则执行任务。否则跳出循环，程序结束。 12345678910111213141516171819202122232425262728293031323334353637383940414243class GeventSpider(object): def __init__(self, root_url, max_depth=5, max_count=100, concurrancy=5, live_time=60*12): monkey.patch_all() self.setting = dict(zip(['root_url', 'max_depth', 'max_count', 'concurrancy', 'live_time'], [root_url, max_depth, max_count, concurrancy, live_time])) self.queue = Queue() self.pool = Pool(concurrancy) self.timer = Timer(live_time, self.stop_spider) self.finished_set = set() # 将种子任务添加到队列 self.queue.put(UrlObject(root_url, 0)) def stop_spider(self): # 将timer取消，则 _run_loop就会跳出循环，主线程结束 self.timer.cancel() # 等待协程池里的所有greenlet结束后，程序结束 self.pool.join() def run(self): self.timer.start() self._run_loop() # 任务执行部分 def _run_loop(self): page_count = 0 while self.timer.is_alive(): # 从pool中踢出执行完毕的greenlet for greenlet in list(self.pool): if greenlet.dead: self.pool.discard(greenlet) try: url_obj = self.queue.get(timeout=5) except Empty: continue # 创建执行下载任务的greenlet对象 # 这里传入的spider对象为self自己 self.pool.start(Handler(url_obj, self, self.setting)) # 爬取数量+1，如果超过所需结果，则调用stop page_count += 1 print '已下载%d个网页' %page_count if page_count &gt;= self.setting['max_count']: print u'下载网页数量达到最大值' self.stop_spider() 完整代码： requests库不是异步的，因此monkey patch 前后的执行时间分别为44s和11s。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130#!/usr/bin/env python# -*- coding:utf-8 -*-# @File : gspider.py# @Author: Shuaiyy# @Date : 2017/10/31 15:45# @Desc : import geventfrom gevent.queue import Queue, Emptyfrom gevent import monkey, Greenletfrom gevent.pool import Poolfrom threading import Timerimport requestsfrom bs4 import BeautifulSoupclass UrlObject(object): def __init__(self, url, depth): self.url = url self.depth = depth class Handler(Greenlet): def __init__(self, urlObj, spider, settings): super(Handler, self).__init__() self.url_obj = urlObj self.spider = spider self.settings = settings # 下载html def download(self, url): print u'开始下载：', url res = requests.get(url) text = res.text print u'下载完成！' return text # 从html中发掘新的任务url def feed(self, html): soup = BeautifulSoup(html, 'lxml') urls = [i.get('href') for i in soup.select('a[href^="http"]')] return urls # 从Html中提取数据 def parse_html(self, html): soup = BeautifulSoup(html, 'lxml') title = soup.find('title') print title.text if title else '' # 处理文本也可以用greenlet对象 pass def _run(self): if not hash(self.url_obj.url) in self.spider.finished_set: # 如果没下载过该网页，则下载，并处理 html = self.download(self.url_obj.url) # 加入已处理的集合 self.parse_html(html) self.spider.finished_set.add(hash(self.url_obj.url)) # 页面深度+1 depth = self.url_obj.depth + 1 # 发现新的url任务,如果没被执行就加入到任务队列 for url in self.feed(html): if not hash(url) in self.spider.finished_set: url_obj = UrlObject(url, depth) self.spider.queue.put(url_obj)class GeventSpider(object): def __init__(self, root_url, max_depth=5, max_count=100, concurrancy=5, live_time=60*12): monkey.patch_all() self.setting = dict(zip(['root_url', 'max_depth', 'max_count', 'concurrancy', 'live_time'], [root_url, max_depth, max_count, concurrancy, live_time])) self.queue = Queue() self.pool = Pool(concurrancy) self.timer = Timer(live_time, self.stop_spider) self.finished_set = set() # 将种子任务添加到队列 self.queue.put(UrlObject(root_url, 0)) def stop_spider(self): # 将timer取消，则 _run_loop就会跳出循环，主线程结束 self.timer.cancel() # 等待协程池里的所有greenlet结束后，程序结束 self.pool.join() def run(self): self.timer.start() self._run_loop() # 任务执行部分 def _run_loop(self): page_count = 0 while self.timer.is_alive(): # 从pool中踢出执行完毕的greenlet for greenlet in list(self.pool): if greenlet.dead: self.pool.discard(greenlet) try: url_obj = self.queue.get(timeout=5) except Empty: continue # 创建执行下载任务的greenlet对象 # 这里传入的spider对象为self自己 self.pool.start(Handler(url_obj, self, self.setting)) # 爬取数量+1，如果超过所需结果，则调用stop page_count += 1 print '已下载%d个网页' %page_count if page_count &gt;= self.setting['max_count']: print u'下载网页数量达到最大值' self.stop_spider()class MySpider(object): def __init__(self, max_depth, max_count, root_url): self.spider = GeventSpider(max_dept h=max_depth, max_count=max_count, root_url=root_url) def run(self): self.spider.run()if __name__ == '__main__': root_url = 'http://www.csdn.com' max_depth, max_count = 3, 100 import time t1 = time.time() MySpider(max_depth, max_count, root_url).run() print time.time() - t1 # 44.4679999352 # 11.4879999161 多线程爬虫基本逻辑：不停的往任务队列里添加url任务，从队列取出任务，创建任务线程添加到线程池，将完成的任务线程从线程池移除。根据条件控制线程并发，停止。注意多线程的同步锁问题。 多线程必须要考虑同步的问题，要使用Lock python 内置的线程池没有实现pool，可以使用第三方的threadpool或者自己实现 计算pool的长度，往pool里添加线程，从队列里移除线程时，必须加锁 检查url是否重复时，需要lock python的Queue对象是线程安全的，向队列里添加，获取任务无需加锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111# 爬虫主线程class ThreadSpider(object): def __init__(self, max_depth, max_count, root_url): self.strategy = Strategy(max_depth, max_count) self.queue = Queue() # 任务队列 self.url_set = set() # url去重的集合 self.handler_num = 0 self.lock = Lock() self.thread_lock = Lock() self.thread_pool = &#123;&#125; self.thread_id = 0 self.is_stop = False # 停止爬虫 self.thread_num = 0 self.currency_limit = False # 停止往pool里添加线程 self.last_data = None obj = UrlObject(root_url, 0) self.put(obj) def put(self, obj): # 添加任务前，判断去重 hash_val = hash(obj.url) self.lock.acquire() res = hash_val in self.url_set # 操作集合时加锁 self.lock.release() if res: return self.url_set.add(hash_val) self.queue.put(obj) def _run_loop(self): while True: if self.is_stop: time.sleep(1) continue if self.currency_limit: # 如果达到并发上限，则线程sleep 1s后在判断pool是否已满 time.sleep(1) self.thread_lock.acquire() # 操作pool要上锁 self.thread_num = len(self.thread_pool) if self.thread_num == self.strategy.concurrency: self.thread_lock.release() continue else: self.currency_limit = False self.thread_lock.release() else: # 没有达到并发上限，则从队列获取任务，创建新的子线程 try: url = self.queue.get() except: continue # 创建任务子线程，并发子线程对象放入pool队列里 self.thread_id = self.thread_id+1 thd = Handler(url, self, self.thread_id) self.thread_lock.acquire() self.thread_pool[self.thread_id] = thd # 判断pool是否已满 if len(self.thread_pool) == self.strategy.concurrency: self.currency_limit = True self.thread_lock.release() self.thread_num = self.thread_num+1 print "add thread ", self.thread_id thd.start() # 运行线程 self.handler_num = self.handler_num+1 if self.strategy.max_count &lt;= self.handler_num: print "handler num %d is full so stop " % self.handler_num self.is_stop = True def remove_thread(self, thd_id): # 当任务线程执行完毕时，调用该方法 self.thread_lock.acquire() if thd_id in self.thread_pool: del self.thread_pool[thd_id] print "del threadid ", thd_id self.thread_lock.release() def run(self): self._run_loop()# 执行下载任务的线程对象class Handler(Thread): def __init__(self, urlobj, spider, thd_id): Thread.__init__(self) print "begin thread %d with url %s" %(thd_id, urlobj.url) self.urlobj= urlobj self.spider = spider self.thread_id = thd_id self.charset = "utf-8" def run(self): try : html = self.open(self.urlobj.url) except Exception,why: return depth = self.urlobj.depth + 1 if depth &gt; self.spider.strategy.max_depth: return # 生成新的任务放入任务队列 for link in self.feed(html): if hash(link) in self.spider.url_set: continue url = UrlObject(link, depth) self.spider.put(url) # 任务线程到此结束，调用从pool移除线程的方法 self.spider.remove_thread(self.thread_id) def open(self, url): '''下载，处理网页''' return resp.text def feed(self, html): '''挖掘新的url任务''' return urls]]></content>
      <categories>
        <category>技术</category>
        <category>并发编程</category>
        <category>Gevent</category>
      </categories>
      <tags>
        <tag>gevent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gevent 学习笔记]]></title>
    <url>%2F2017%2F11%2F01%2F%E6%8A%80%E6%9C%AF%2FGevent%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[python Gevent 进程、线程、协程 python 中的yield提供对协程的有限支持。 进程 正在运行的程序的实例 具有独立地址空间 是操作系统资源分配的基本单位 进程上下文： 进程的物理实体与支持进程运行的物理环境，包括地址空间，系统栈，打开文件表，…… 上下文切换：由一个进程的上下文转到另外一个进程的上下文 系统开销：操作系统完成系统管理工作所花费的时间和空间 一个进程可以包含多个线程 线程 线程是程序执行的最小单位 多线程可以提高程序的并发性 由于python的GIL机制，一个进程只能使用一个cpu核心，因此多线程并不适合解决CPU密集的运算，此时应该使用多进程。 协程 可以认为是一种用户态的线程 线程是由系统调度，而协程需要主动让出CPU时间，即控制权在程序员手中 线程里可以包含多个协程 优缺点 进程创建和销毁成本高 线程开销比进程低，但切换成本高，线程间同步复杂 协程在不陷入内核的情况下进行上下文切换，没有同步问题，但需要手动切换。 简单示例 进程与进程池 要注意，进程的开销很大 123456789101112131415161718192021222324252627#!/usr/bin/env python# -*- coding:utf-8 -*-import multiprocessingimport timedef hello(name): # print threading.currentThread().getName() print multiprocessing.current_process().name print name time.sleep(1)def process1(): print multiprocessing.current_process().name # thread1 = threading.Thread(target=hello, args=['a']) # thread1.start() # thread1.join() processes = [ multiprocessing.Process(target=hello, args=(x,)) for x in 'abcde'] for p in processes: p.start() p.join()def process2(): print multiprocessing.current_process().name pool = multiprocessing.Pool(10) result = pool.map(hello,[x for x in 'abcde']) # result是子进程返回的结果，顺序不定 pool.close() # 关闭进程池，不在接收新的进程加入 pool.join() # 主进程阻塞，等该子进程的退出 if name == ‘main‘: start_time = time.time() process1() print &apos;finished time: %d&apos; %(time.time() - start_time) start_time = time.time() process2() print &apos;finished time: %d&apos; %(time.time() - start_time) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 ​+ 线程与线程池 threadpool是第三方模块，要注意python的GIL机制，CPU密集任务不要使用多进程 ```python #!/usr/bin/env python # -*- coding:utf-8 -*- import threading import threadpool import time def hello(name): print threading.currentThread().getName() print name time.sleep(1) # 线程，按顺序执行 def func1(): print threading.currentThread().getName() # thread1 = threading.Thread(target=hello, args=[&apos;a&apos;]) # thread1.start() # thread1.join() threads = [ threading.Thread(target=hello, args=[x]) for x in &apos;abcde&apos;] for thread in threads: thread.start() thread.join() # 线程池，并发执行，理论上效率更高 def func2(): # pip install thradpool print threading.currentThread().getName() pool = threadpool.ThreadPool(10) requests = threadpool.makeRequests(hello,[x for x in &apos;abcde&apos;]) for req in requests: pool.putRequest(req) pool.wait() if __name__ == &apos;__main__&apos;: start_time = time.time() func1() print &apos;finished time: %d&apos; %(time.time() - start_time) start_time = time.time() func2() print &apos;finished time: %d&apos; %(time.time() - start_time) 协程 12 ​ 网络IO阻塞和非阻塞 阻塞调用是指在调用结果返回之前，当前线程会被挂起。函数只有在得到结果之后才会返回 非阻塞是指在不能立即得到结果之前，函数不会阻塞当前进程，而是立即返回。 网络IO阻塞 网络IO主要指socket socket会在connect/read/write时发生阻塞 连接时阻塞 接收数据时阻塞 ​ 非阻塞模型 接收数据非阻塞模型 非阻塞面临的问题 非阻塞的socket连接要捕获处理相应的异常 使用select判断socket是否可读写 或者捕获非阻塞的exception，不断重试发送、读取数据 123456import loggingimport timeimport errnoimport sysimport socketimport select logger = logging.getLogger(‘Client’)console_handler = logging.StreamHandler(sys.stdout)format = logging.Formatter(‘%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s: %(message)s’, &apos;%Y-%m-%d %H:%M:%S&apos;) console_handler.setFormatter(format)logger.addHandler(console_handler)logger.setLevel(logging.DEBUG) if name == ‘main‘: ip = ‘localhost’ port = 13518 logger.debug(‘creating socket’) s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) logger.debug(‘connecting to server’) s.setblocking(0) # 设置成非阻塞 try: s.connect((ip, port)) # 非阻塞下，立即返回，此时不一定建立好连接 except socket.error, msg: if msg[0] not in [errno.EINPROGRESS, errno.EWOULDBLOCK]: # 如果不是正在连接,或者数据阻塞 exit(1) # 使用select判断socket是否可写，即发送数据 while True: ready_to_read, read_to_write, in_error = select.select([], [s,], [], 0.001) if read_to_write: break message = &apos;Hello, world&apos; logger.debug(&apos;sending data: &quot;%s&quot;&apos;, message) while True: try: len_sent = s.send(message) break except socket.error, msg: # Eagain Linux下，缓冲区数据不可用，请重试。数据还在准备中 # EWouldBlock， Windows下，缓冲区数据不可用，请重试。数据还在准备中 if msg[0] not in [errno.EWOULDBLOCK, errno.EAGAIN]: print 1 exit(1) # Receive a response logger.debug(&apos;waiting for response&apos;) while True: try: response = s.recv(len_sent) break except socket.error, msg: if msg[0] not in [errno.EWOULDBLOCK, errno.EAGAIN]: logger.exception(&apos;recv error&apos;) exit(1) logger.debug(&apos;response from server: &quot;%s&quot;&apos;, response) # Clean up logger.debug(&apos;closing socket&apos;) s.close() logger.debug(&apos;done&apos;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495 ​### 同步异步+ 同步：发生调用时，一定等待结果返回整个调用才结束+ 异步：调用发生后，立即返回，不用等待结果。被调用者通过状态、通知来告知调用者，或者通过回调函数来处理这个调用。#### 实例：异步查询数据库+ 主线程将查询任务交给逻辑线程处理+ 使用队列Queue来实现同步+ Queue中存放任务对象task_item，task_item包含任务所需数据，任务执行结果等，完成任务后回调函数等+ 逻辑线程执行数据库查询操作后，2种处理方式 + 将task_item交由主线程处理 使用全局变量Queue保存task_item,并在主线程中处理 + 在子线程中直接调用task_item回调函数处理结果​```python#!/usr/bin/env python# -*- coding:utf-8 -*-# @File : redis_client.py# @Author: Shuaiyy# @Date : 2017/10/27 9:44# @Desc : import sysimport timeimport redisfrom threading import Thread, current_threadfrom queue import Queuemain_queue = Queue(maxsize=1024) # 为了在主线程中同步task_itemdef handle_res(res): # 处理结果的回调函数，只是打印结果 print current_thread().getName() print &quot;获取到data：&quot; + str(res) class GetValueTask(object): # task_item对象，保存数据，回调函数和任务结果 result = None def __init__(self, key, handler): self.key = key self.callback = handlerclass RedisAsyncGet(Thread): def __init__(self): super(RedisAsyncGet, self).__init__() self.r = redis.Redis(&apos;localhost&apos;, 6379, 0, password=&apos;mima&apos;) self.queue = Queue(maxsize=1024) def get_value_cmd(self, key): task_item = GetValueTask(key, handle_res) self.queue.put(task_item) def run(self): while True: if not self.queue.empty(): # 获取任务 task_item = self.queue.get() # 执行任务，得到结果 res = self.r.get(task_item.key) # 直接在子线程中调用回调函数处理结果 task_item.callback(res) # 将任务放到全局队列中，以便主线程处理task_item task_item.result = res main_queue.put(task_item) time.sleep(0.1) # 避免cpu空转在多if __name__ == &apos;__main__&apos;: handle = RedisAsyncGet() handle.start() handle.get_value_cmd(&apos;a&apos;) handle.get_value_cmd(&apos;test&apos;) # handle.join() # # 下面是主线程中处理task_item的返回结果 while True: if not main_queue.empty(): task_item = main_queue.get() res = task_item.result task_item.callback(res) time.sleep(0.1) 同步异步与阻塞非阻塞的区别 阻塞/非阻塞，描述的是程序在等待消息（不管是同步消息还是异步消息）是的状态。 同步/异步, 描述的是程序获得其关注消息的通知机制。 同步异步与阻塞非阻塞的组合 同步阻塞：效率最低 同步非阻塞：效率也低，需要伦旭 异步阻塞：一般模式的线程回调 异步非阻塞：IOCP机制，难度高，一般用的少 并发和并行并发•并发是指两个或多个事件在同一时间间隔发生。就是同时处理很多事情，比如串行同时处理一件事情。 •在单核系统中，为了提高cpu利用率，系统采用时间片轮询等调度方式，对多个线程轮换执行，在宏观上看，线程是同时执行的，从微观上看，某一时刻只执行一个线程。 在发生资源竞争或者大量的上下文切换会导致性能消耗 。 并行 并行是同时处理多件事情。 比如线程可以真正的做到同一时刻多个运行，每个线程可以在不同的CPU核上运行。 greenlet实现并发 greenlet是stacklesspython（支持微线程tasklet的CPython版本）的副产品。Tasklet以伪并发运行着（如果在单个或者很少的系统级线程内） greenlet是一个原始的微线程的概念，没有调度，可以称为协程。所以green需要自己调度。 如果有阻塞调用，将greenlet主动切换出去。 单个线程内可以运行任意哥greenlet微线程，不同线程之间不能切换greenlet。 pip install greenlet 123456789101112131415161718192021import threadingfrom greenlet import greenletdef test1(): print threading.current_thread().getName() print '1' g2.switch() print "2" g2.switch()def test2(): print threading.current_thread().getName() print '3' g1.switch() print "4"if __name__ == '__main__': g1 = greenlet(test1) g2 = greenlet(test2) g1.switch() # 输出：1324 Gevent简介 gevent是一个基于libev和greenlet的并发库。它为各种并发和网络相关的任务提供了整洁的API。libev是高性能事件循环/事件模型的网络库，并且包含大量新特性。 Python通过yield提供了对协程的基本支持，但是不完全。而第三方的gevent为Python提供了比较完善的协程支持。 gevent是第三方库，通过greenlet实现协程，其基本思想是： 当一个greenlet遇到IO操作时，比如访问网络，就自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换回来继续执行。由于IO操作非常耗时，经常使程序处于等待状态，有了gevent为我们自动切换协程，就保证总有greenlet在运行，而不是等待IO。 由于切换是在IO操作时自动完成，所以gevent需要修改Python自带的一些标准库，这一过程在启动时通过monkey patch完成。 Gevent特点 Gevent之Greenlet对象 继承greetlet Greentlet(run=None, args, *kwargs)创建一个greenlet greenlet.start()将greenlet置入geventIO调度内 greenlet.spawn(args,*kwargs) 创建greenlet，并运行start greenlet.kill()杀死greenlet greenlet间的切换由,gevent.sleep()方法交出控制权，然后由gevent进行调度协程 下面程序的输出顺序为1,2,,1-1,2-1 1234567891011121314151617181920212223242526272829303132import threadingimport geventfrom gevent import Greenletdef run_in_greenlet1(arg_x): print threading.current_thread().getName() print '1' , arg_x gevent.sleep(0) print "1-1" , arg_xdef run_in_greenlet2(arg_x): print threading.current_thread().getName() print '2' , arg_x gevent.sleep(0) print "2-1" , arg_xif __name__ == '__main__': g1 = Greenlet(run_in_greenlet1, '这是参数') g2 = Greenlet(run_in_greenlet2, '这是参数') g1.start() g2.start() # 构造和启动可以由spawn函数实现 # g1 = gevent.spawn(run_in_greenlet1, '参数') g1.join() g2.join() # 可以一次join多个greenlet # gevent.joinall([g1, g2]) # 更为简洁的写法 gevent.joinall([gevent.spawn(run_in_greenlet1) , gevent.spawn(run_in_greenlet2)]) Greenlet状态与超时处理超时处理当协程对运行时间有要求时，可以设置Timeout计时器，超时的协议会引发timeout异常 gevent.Timeout(seconds, exception)，没有指定exception时，超时会抛出gevent.Timeout。 1234567891011121314151617181920212223def run_in_greenlet1(arg_x): print '1' , arg_x gevent.sleep(5) # 强制协程等待5秒，使用time.sleep无效 print "1-1" , arg_xif __name__ == '__main__': # 或者使用with上下文管理器 max_time = 2 try: with gevent.Timeout(max_time, Exception("超时了")): gevent.spawn(run_in_greenlet1, '参数').join() except gevent.Timeout: print 'timeout' except Exception as e: print e.message # 或者调用timeout对象的start方法 max_time = 2 timeout = gevent.Timeout(max_time) timeout.start() try: gevent.spawn(run_in_greenlet1, '参数').join() except gevent.Timeout : print 'timeout' 状态 started– Boolean, 指示此Greenlet是否已经启动 ready()– Boolean, 指示此Greenlet是否已经停止 successful()– Boolean, 指示此Greenlet是否已经停止而且没抛异常，即运行成功 value– 任意值, 此Greenlet代码返回的值 exception– 异常, 此Greenlet内抛出的未捕获异常 定制Greentlet进行数据管理，超时设置等。 1234567891011121314class MyGreenlet(Greenlet): def __init__(self, message, timeout=2): super(MyGreenlet, self).__init__() self.message = message self.timeout = gevent.Timeout(timeout) # 这里重载了run方法，因此可以不必在构造对象时传入回调函数 def _run(self): self.timeout.start() print self.message if __name__ == '__main__': g1 = MyGreenlet('hello', timeout=3) g1.start() g1.join() Event对象，协程间同步如果不用event事件通知，可以用全局变量进行消息传递，但全局变量会有很多问题，同步不安全，浪费cpu资源等。 windows中有Events，作为线程间同步的方法 Gevent中则是Greenlet间“同步”的一种方法 使用方法 Event对象 AsyncResult可以传递任意类型的数据 1234567891011121314151617181920212223242526from gevent.event import Event, AsyncResultevt1 = Event()evt2 = AsyncResult()def boss(): global evt1, evt2 print 'start to work!' tasklist = dict(zip(range(3), 'abc')) evt2.set(tasklist) gevent.sleep(5) print 'time to relax!' evt1.set()def worker(n): global evt1, evt2 tasklist = evt2.get(timeout=2) print '%d is working task %s!' %(n, tasklist[n]) evt1.wait() print '%d stop working!' %nif __name__ == '__main__': b = Greenlet(boss) b.start() gevent.joinall([gevent.spawn(worker, i) for i in range(3)]) b.join() Queue对象，实现通信python内置的Queue Queue（队列），用于存取数据的有序数据结构。Queue(先进先出)，LifoQueue(先进后出)和PriorityQueue(优先级队列) Queue模块实现了多生产者、多消费者队列。它特别适用于信息必须在多个线程间安全地交换的多线程程序中。这个模块中的Queue 类实现了所有必须的锁语义。 模块实现了三类队列，主要差别在于取得数据的顺序上。FIFO队列中，最早加入的任务会被最先得到。LIFO队列中，最后加入的任务会被最先得到（就像栈一样）。在优先队列中，任务被保持有序，拥有最小值的任务（优先级最高）被最先得到。 虽然线程安全，但同步线程开销 Gevent中的Queue 无线程同步开销，但有Greenlet之间的线程内同步，无法线程间操作。 注意queue是否为空 12345678910111213141516171819202122232425262728from gevent.queue import Queue, LifoQueue, PriorityQueue, Emptytasks_queue = Queue()def boss(): for i in xrange(30): tasks_queue.put_nowait(i) # put_nowait不用判断队列是否满了，直接入队。因为此处的queue没指定大小 # tasks_queue.put(i) 如果对满了，会等待。def worker(name): while True: while not tasks_queue.empty(): task = tasks_queue.get(timeout=2) # 如果没有数据会阻塞，因此设置超时时间，并捕获错误 print 'worker %s got task %d' %(name, task) gevent.sleep(0) gevent.sleep(1) # 捕获队列为空的异常 try: while True: task = tasks_queue.get(timeout=0.1) except gevent.queue.Empty: print 'quit'if __name__ == '__main__': gevent.joinall([gevent.spawn(boss),].extend(gevent.spawn(worker, name) for name in 'abcde')) # gevent.joinall([gevent.spawn(worker, name) for name in 'abcde'].append(gevent.spawn(boss))) 优先级队列 123456789101112131415161718192021222324252627282930313233343536373839from gevent.queue import Queue, LifoQueue, PriorityQueue, Emptyclass Job(object): def __init__(self, no, priority, desc): self.priority = priority self.desc = desc self.no = no def __cmp__(self, other): return cmp(self.priority, other.priority) def __str__(self): return 'Job %d:: %d :: %s' %(self.no, self.priority, self.desc)tasks_queue = PriorityQueue()def boss(): for i in xrange(30): tasks_queue.put_nowait(Job(i, 30 - i, str(i)*5)) # put_nowait不用判断队列是否满了，直接入队。因为此处的queue没指定大小 # tasks_queue.put(i) 如果对满了，会等待。def worker(name): while True: while not tasks_queue.empty(): task = tasks_queue.get(timeout=2) # 如果没有数据会阻塞，因此设置超时时间，并捕获错误 print task gevent.sleep(0) gevent.sleep(1) # 捕获队列为空的异常 try: while True: task = tasks_queue.get(timeout=0.1) except gevent.queue.Empty: print 'quit'if __name__ == '__main__': gevent.joinall([gevent.spawn(boss),].extend(gevent.spawn(worker, name) for name in 'abcde')) 自定义一个Queue 随机取出的队列 12345678910111213141516from gevent.queue import Queue, LifoQueue, PriorityQueue, Emptyimport randomclass RandomQueue(Queue): def _init(self, maxsize, items=None): self.queue = [] def _put(self, item): self.queue.append(item) def _get(self): return self.queue.pop(random.randint(0, len(self.queue) - 1))q = RandomQueue()for i in range(10): q.put(i)for i in range(10): print q.get() Greenlet间同步机制semaphore(信号量) 信号量是一个允许Greenlet相互合作，限制并发访问或运行的低层次的同步原语。信号量也被称为锁。 信号量有两个方法，acquire和release。在信号量是否已经被acquire或release，和拥有资源的数量之间不同,被称为此信号量的范围。 如果一个信号量的范围已经降低到0，它会阻塞acquire操作直到另一个已经获得信号量的greenlet作出释放。 1234567891011121314151617181920212223import geventfrom gevent.pool import Poolfrom gevent.lock import BoundedSemaphoresem = BoundedSemaphore(1)def worker1(n): sem.acquire() print('worker %d acquire sem' %n) gevent.sleep(0) sem.release() print('woker %d release sem' %n)def worker2(n): # 可以用with上下文管理器 with sem: print('worker %d acquire sem' %n) gevent.sleep(0) print('woker %d release sem' %n)pool = Pool()pool.map(worker2, xrange(0,5))pool.join() Greenlet管理Group Group是一个运行中Greenlet的集合，集合中的Greenlet会像一个组一样被共同管理和调度。 API：add,join,kill,killone,map 1234567891011121314151617from gevent.pool import Pool, Groupif __name__ == '__main__': print 1 group = Group() group.add(gevent.spawn(run_in_greenlet1, 'a')) group.add(gevent.spawn(run_in_greenlet1, 'b')) group.add(gevent.spawn(run_in_greenlet1, 'c')) # map方法 group.map(run_in_greenlet1, 'defg') group.join() # imap生成的是迭代器对象，不会立即执行 for result in group.imap(run_in_greenlet1, 'hijk'): print result for result in group.imap_unordered(run_in_greenlet1, 'lmnopq'): print result Pool+ Pool来自子类化Group，是一个为处理数量变化并且需要限制并发Greenlet而设计的类。+ 在需要并行的受限于网络和IO的任务时常常需要用到它。+ 设置最大并发数 1234567pool = Pool(4)pool.map(run_in_greenlet1, 'hijk')pool.spawn(run_in_greenlet1, 'm')pool.join()# 杀死所有的协程pool.kill() 子进程与协程协作 python内置的子进程无法与greenlet协作,当子进程空闲时，协程也无法执行 1234567891011121314151617import geventimport subprocessdef test(): while True: print 'test' gevent.sleep(2)if __name__ == '__main__': g = gevent.spawn(test) # 等待10秒后打印用户 sub = subprocess.Popen('ping 1.1.1.1 -n 1 -w 10000 &amp;&amp; dir', stdout=subprocess.PIPE, shell=True) sub.wait() output, err = sub.communicate() print output.decode('gbk').encode('utf-8'), err # 在子进程等待10秒的过程中协程g没有被调用，就被kill掉了 g.kill() 使用gevent.subprocess 在子进程执行的10多秒的时间里， 协程g调用了6次test函数 1234567891011121314151617import gevent# import subprocessfrom gevent import subprocessdef test(): while True: print 'test' gevent.sleep(2)if __name__ == '__main__': g = gevent.spawn(test) # 等待10秒后打印用户 sub = subprocess.Popen('ping 1.1.1.1 -n 1 -w 10000 &amp;&amp; dir', stdout=subprocess.PIPE, shell=True) sub.wait() output, err = sub.communicate() print output.decode('gbk').encode('utf-8'), err # 在子进程等待10秒的过程中协程g没有被调用，就被kill掉了 g.kill() gevent.socket 与multiprocessing模块协作 multiprocessing模块多进程本身也无法和greenlet进行协作， 下面的例子在Linux下实现，使用两个Pipe进行进程间通信，两个协程一个从a写入pipe1，一个从d读取pipe2，子进程负责从b端接受pipe1数据，从c端写入pipe2. 子进程中的msg = b.recv()是阻塞的，此时协程可以获得cpu控制权 123456789101112131415161718192021222324252627from multiprocessing import Process, Pipefrom gevent import socketa, b = Pipe() # Pipe返回的是两个连接到管道2端的对象c, d = Pipe()def relay(): for i in range(10): msg = b.recv() # 没有数据时会阻塞 c.send('%s in %d' %(msg, i))def put_msg(): for i in range(10): socket.wait_write(a.fileno()) a.send('hi %d' %i)def get_msg(): for i in range(10): socket.wait_read(d.fileno()) print d.recv()if __name__ == '__main__': proc = Process(target=relay) proc.start() # 创建进程 g1 = gevent.spawn(put_msg) g2 = gevent.spawn(get_msg) gevent.joinall([g1, g2]) Monkey Patch什么是monkey patch在动态语言中，不去改变源码而对功能进行追加和变更就叫做MonkeyPatching（猴子补丁） 追加功能 功能变更 修正程序错误 增加钩子，在执行某个方法的同时执行一些其他的处理，如打印日志，实现AOP等， python实现monkey patch 123456789101112131415161718class Bird(object): def fly(self): print 'i can fly!'def fly(self): print 'i cannot fly!'def run(self): print 'i can run!'if __name__ == '__main__': bird = Bird() bird.fly() # 下面是动态补丁，对象的方法会发生改变 Bird.fly = fly Bird.run = run bird.fly() bird.run() Gevent 中的monkey patch patch的模块有：socket,dns, time, select, thread, os, ssl, subprocess, sys, builtins, signal 默认阻塞的模块都被替换成非阻塞，协作式的。比如gevent支持异步协作的DNS，gevent的time.sleep只是协程内休眠，不阻塞线程。 12345678910111213141516171819import timeitimport urllib2import geventfrom gevent import monkeydef download(url): data = urllib2.urlopen(url) gevent.sleep(0) text = data.read()[:50] print url, textdef test(): urls = ['http://gunicorn.org/#docs', 'http://gunicorn.org/', 'https://www.liaoxuefeng.com'] gevent.joinall([gevent.spawn(download, url) for url in urls])monkey.patch_all()# 没有patch之前，用时24.5653685739,使用patch后执行时间为13.973405013# timeit是python的计时器模块print timeit.Timer(stmt="test()", setup="from __main__ import test").timeit(number=20) Server的使用服务器概念一个管理资源并为用户提供服务的计算机软件，通常分为文件服务器（能使用户在其它计算机访问文件），数据库服务器和应用程序服务器。 服务器软件工作在客户端-服务器或浏览器-服务器的方式，常用的分类： 文件服务器（FileServer） 数据库服务器（DatabaseServer）——MySQL 邮件服务器（MailServer）——Microsoft Exchange 网页服务器（WebServer）——如Apache 应用程序服务器 TCP服务器python的socket，select，以及SocketServer模块都是阻塞的。 Gevent提供了非阻塞的socket server python socket的简单服务器 123456789101112131415import socketimport sysif __name__ == '__main__': s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.bind(('localhost', 12222)) s.listen(100) while True: coon, addr = s.accept() print 'connected with %s:%s ' % (addr[0], addr[1]) # 阻塞，只有recv操作完成后， s才能接受新的连接 data = coon.recv(1024) print data coon.close() s.close() 基于python select的异步服务器 123456789101112131415161718192021222324252627import socketimport selectif __name__ == '__main__': s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.bind(('localhost', 12222)) s.listen(100) read_list = [s] address = &#123;&#125; while True: read_socks, write_socks, error = select.select(read_list, [], []) for sock in read_socks: if sock == s: # 如果是服务器的sock，则允许服务器接收一个连接,并加入可读list coon, addr = s.accept() print 'connected with %s:%s '%(addr[0], addr[1]) read_list.append(coon) address[coon] = addr else: # 来自客户端的socket可读时 data = sock.recv(1024) if data: print 'get:', data else: read_list.remove(sock) print 'client %s:%s closed!'% address[sock] del address[sock] sock.close() s.close() 使用SocketServer类 123456789101112131415import SocketServer# 需要定义一个Handler类处理socket连接class MyRequestHandler(SocketServer.BaseRequestHandler): def handle(self): while True: data = self.request.recv(1024) if not data or len(data) == 0: return print data returnif __name__ == '__main__': address = ('localhost', 12222) server = SocketServer.TCPServer(address, MyRequestHandler) server.serve_forever() Gevent Server 12345678910111213141516from gevent import monkeyfrom gevent.server import StreamServermonkey.patch_all()def handler(sock, addr): while True: data = sock.recv(1024) if data: print data else: print addr, 'closed!' returnif __name__ == '__main__': server = StreamServer(('localhost', 12222), handler) server.serve_forever() 使用基于协程的client对上述服务器测试 可以看到Gevent的server是支持协程的，能同时处理多个连接。 1234567891011121314151617181920from gevent import monkeymonkey.patch_all()import geventimport socketdef do_connect(addr, index): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.connect(addr) sock.send("hello world from %d" %index) gevent.sleep(2) sock.send("hello world2 from %d" %index) sock.close()addr = ('localhost', 12222)greenlets = []num = 10for i in xrange(num): g = gevent.spawn(do_connect, addr, i) greenlets.append(g)gevent.joinall(greenlets) ​ WSGIServer Web服务器网关接口（Python Web Server Gateway Interface，缩写为WSGI）是为Python语言定义的Web服务器和Web应用程序或框架之间的一种简单而通用的接口。 WSGIserver所做的工作仅仅是将从客户端收到的请求传递给WSGI application，然后将WSGI application的返回值作为响应传给客户端。 WSGI application可以是Flask，Django等web框架 WSGI application接口应该实现为一个可调用对象，例如函数、方法、类、含call方法的实例。这个可调用对象可以接收2个参数： 一个字典，该字典可以包含了客户端请求的信息以及其他信息，可以认为是请求上下文，一般叫做environment（编码中多简写为environ、env）； 一个用于发送HTTP响应状态（HTTP status ）、响应头（HTTP headers）的回调函数。 同时，可调用对象的返回值是响应正文（response body），响应正文是可迭代的、并包含了多个字符串。 Python内置的简单的WSGI Server不支持并发，Gunicore是基于gevent的，协程支持的协作式并发。 Gevent WSGI server 支持的并发度比python内置的wsgi高很多 123456789101112from flask import Flaskimport gevent.pywsgiimport geventapp = Flask(__name__)@app.route('/')def handle(): return 'welcome to gevent lesson!'gevent_server = gevent.pywsgi.WSGIServer(('', 5000), app)gevent_server.serve_forever() 在实际的web项目部署时，我们一般使用Gunicore或Uwsgi做wsgi服务器。 Gevent 长轮询浏览网页时，浏览器会传HTTP请求到服务器，服务器会根据请求将网页的内容传给浏览器，但是在很多的情况下，使用者会需要看到最新的即时性资讯，例如观看股票市场行情，如果靠重新载入网页才能获得最新信息，不但很浪费时间，实时效果差，也会浪费网络资源。 轮询：每隔一段时间向服务器发送一次请求，以获取最新的数据。 长时间轮询（long-polling）是让服务器在接收到浏览器发出的HTTP请求后，服务器会等待一段时间，若在这段时间里面伺服器有新的数据更新，它就会把最新的数据传给浏览器，如果等待的时间到了之后也没有新资料的话，就会送一个回应给浏览器，告知浏览器资料没有更新。 长时间轮询可以减少产生轮询（polling）造成网路频宽浪费的状况。 实现原理 浏览器向服务器发送Ajax请求，当接收到服务器响应后，需要向服务求发送新的请求 服务器端要能够一直保持住客户端的请求，直到有响应消息；同时服务器对请求的处理要支持非阻塞模式 需要使用Event，python内置Event是阻塞的，gevent的却是非阻塞的。 示例不停请求数据的ajax js脚本：complete: longPolling 执行完成后继续回调自身，循环执行。 Post的数据包含请求服务器数据的ID，服务器根据ID返回的数据对浏览器来说就是最新的数据。 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Long Pooling&lt;/title&gt;&lt;head&gt;&lt;body&gt; &lt;div id="main"&gt; &lt;div id="inbox"&gt;&lt;/div&gt; &lt;/div&gt; &lt;div id="state"&gt;&lt;/div&gt; &lt;script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" charset="utf-8"&gt; var id = null; // id为空时，服务器返回最新的数据 function longPolling() &#123; $.ajax(&#123; url: "update", data: &#123;"id": id&#125;, type: "POST", error: function (XMLHttpRequest, textStatus, errorThrown) &#123; $("#state").append("[state: " + textStatus + ", error: " + errorThrown + " ]&lt;br/&gt;"); &#125;, success: function (result, textStatus) &#123; console.log(result) msg_data = eval("(" + result + ")"); $("#inbox").append(msg_data.html); id = msg_data.id; console.log(msg_data) $("#message").val(""); $("#state").append("[state: " + textStatus + " ]&lt;br/&gt;"); &#125;, complete: longPolling &#125;); &#125; $(function()&#123; longPolling(); &#125;) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 服务端要记录数据的编号，根据浏览器请求数据的ID返回数据： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273from gevent.pywsgi import WSGIServerfrom gevent.event import Eventfrom cgi import escapeimport uuidimport urlparseimport stringimport randomdef get_request_data(field, env): try: request_body_size = int(env.get('CONTENT_LENGTH', 0)) except (ValueError): request_body_size = 0 request_body = env['wsgi.input'].read(request_body_size) d = urlparse.parse_qs(request_body) data = d.get(field, [''])[0] return datadef generate_response_data(response_body, start_response): response_headers = [('Content-Type', 'text/html'), ('Content-Length', str(len(response_body)))] start_response('200 OK', response_headers) return [response_body]def generate_json_data(msg_list): msg_dict = &#123;&#125; msg_dict["html"] = "" for msg in msg_list: msg_dict["html"] += "&lt;div&gt;&#123;0&#125;&lt;/div&gt;".format(msg["msg"]) msg_dict["id"] = msg_list[-1]["id"] res = str(msg_dict) return resdef id_generator(size=6, chars=string.ascii_uppercase + string.digits): return ''.join(random.choice(chars) for _ in range(size))file = open('longpooling.html')chat_html = file.read()class MessgaeBuffer(object): def __init__(self): self.cache = [] self.message_event = Event() def empty(self): return len(self.cache) == 0def application(env, start_response): env_val = env['PATH_INFO'] if env_val == "/create": msg_item = &#123;&#125; msg_item["id"] = str(uuid.uuid4()) msg_item["msg"] = id_generator() print "create msg %s" % str(msg_item) msgBuffer.cache.append(msg_item) # 当有新数据时，通知协程 msgBuffer.message_event.set() # 此时event.wait()不会阻塞 msgBuffer.message_event.clear() # 此时event.wait()会阻塞 return generate_response_data("", start_response) elif env_val == "/update": lastid = escape(get_request_data("id", env)) if msgBuffer.empty() or msgBuffer.cache[-1]["id"] == lastid: msgBuffer.message_event.wait() # 没有数据或没有新数据，协程阻塞等待 for index,m in enumerate(msgBuffer.cache): if m["id"] == lastid: return generate_response_data( generate_json_data( msgBuffer.cache[index+1:]) , start_response) return generate_response_data(generate_json_data(msgBuffer.cache), start_response) else: return generate_response_data(chat_html, start_response)msgBuffer = MessgaeBuffer()WSGIServer(('localhost', 8080), application).serve_forever() 使用websocket替代轮询实现推送web socket WebSocket是HTML5开始提供的一种在单个TCP 连接上进行全双工通讯的协议。WebSocket通讯协议于2011年被IETF定为标准RFC 6455，WebSocketAPI被W3C定为标准。 在WebSocketAPI中，浏览器和服务器只需要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道。两者之间就直接可以数据互相传送 很多网站实现推送技术所用的技术都是轮询。轮询是在特定的的时间间隔（如每1秒），由浏览器对服务器发出HTTPrequest，然后由服务器返回最新的数据给客户端的浏览器。这种传统的模式带来很明显的缺点，即浏览器需要不断的向服务器发出请求，然而HTTPrequest的header是非常长的，里面包含的数据可能只是一个很小的值，这样会占用很多的带宽和服务器资源。 而比较新的技术去做轮询的效果是Comet，使用了AJAX。但这种技术虽然可达到双向通信，但依然需要发出请求，而且在Comet中，普遍采用了长链接，这也会大量消耗服务器带宽和资源。 面对这种状况，HTML5定义了WebSocket协议，能更好的节省服务器资源和带宽并达到实时通讯。 web socket优势 服务器与客户端之间交换的数据包档头很小，大概只有2字节 服务器可以主动传送数据给客户端。 websocket实例 html中创建websocket对象, ws.onmessage当有新message时进行处理 1234567891011121314151617181920212223242526272829303132&lt;html&gt; &lt;head&gt; &lt;title&gt;Minimal websocket application&lt;/title&gt; &lt;script type="text/javascript" src="http://libs.baidu.com/jquery/2.1.4/jquery.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function() &#123; // Open up a connection to our server var ws = new WebSocket("ws://localhost:10000/"); // What do we do when we get a message? ws.onmessage = function(evt) &#123; $("#placeholder").append('&lt;p&gt;' + evt.data + '&lt;/p&gt;') &#125; // Just update our conn_status field with the connection status ws.onopen = function(evt) &#123; $('#conn_status').html('&lt;b&gt;Connected&lt;/b&gt;'); &#125; ws.onerror = function(evt) &#123; $('#conn_status').html('&lt;b&gt;Error&lt;/b&gt;'); &#125; ws.onclose = function(evt) &#123; $('#conn_status').html('&lt;b&gt;Closed&lt;/b&gt;'); &#125; &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;WebSocket Example&lt;/h1&gt; &lt;div id="conn_status"&gt;Not Connected&lt;/div&gt; &lt;div id="placeholder" style="width:600px;height:300px;"&gt;&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; ​ 服务器中实现websocket处理 1234567891011121314151617import jsonimport randomfrom gevent import pywsgi, sleepfrom geventwebsocket.handler import WebSocketHandlerclass WebSocketApp(object): def __call__(self, env, start_response): ws = env['wsgi.websocket'] # 获取websocket对象 x = 0 while True: # 创建并发送消息 data = json.dumps(&#123;'x':x, 'y' :random.randint(1,5)&#125;) ws.send(data) x += 1 sleep(0.5)server = pywsgi.WSGIServer(('', 10000), WebSocketApp(), handler_class=WebSocketHandler)server.serve_forever() ​]]></content>
      <categories>
        <category>技术</category>
        <category>并发编程</category>
        <category>Gevent</category>
      </categories>
      <tags>
        <tag>gevent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 内省机制]]></title>
    <url>%2F2017%2F10%2F17%2F%E6%8A%80%E6%9C%AF%2Fpython-%E5%86%85%E7%9C%81(Introspection)%2F</url>
    <content type="text"><![CDATA[Python 内省机制 内省(Introspection) 是什么？在计算机科学中，内省指一种能力，可以确定对象是什么，包含何种信息，可以做什么。Python 就是一门提供了内省机制的语言。 Python 的内省机制 help 函数 sys 模块sys模块中包含了系统，当前进程等相关的信息，例如 123456sys.platformsys.versionsys.maxintsys.argsys.pathsys.modules123456 keyword模块keyword.kwlist 包含Python所有的关键词 123&gt;&gt;&gt; import keyword&gt;&gt;&gt; keyword.kwlist[&apos;and&apos;, &apos;as&apos;, &apos;assert&apos;, &apos;break&apos;, &apos;class&apos;, &apos;continue&apos;, &apos;def&apos;, &apos;del&apos;, &apos;elif&apos;, &apos;else&apos;, &apos;except&apos;, &apos;exec&apos;, &apos;finally&apos;, &apos;for&apos;, &apos;from&apos;, &apos;global&apos;, &apos;if&apos;, &apos;import&apos;, &apos;in&apos;, &apos;is&apos;, &apos;lambda&apos;, &apos;not&apos;, &apos;or&apos;, &apos;pass&apos;, &apos;print&apos;, &apos;raise&apos;, &apos;return&apos;, &apos;try&apos;, &apos;while&apos;, &apos;with&apos;, &apos;yield&apos;] dir函数返回由传入对象的属性排序后构成的列表 builtins模块包含Python中的内建函数 docstring __name__属性 hasattr函数 getattr函数 type函数 id函数 callable函数 isinstance函数 issubclass函数]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python socket编程]]></title>
    <url>%2F2017%2F10%2F15%2F%E6%8A%80%E6%9C%AF%2Fpython%20socket%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[python socket编程参考资料： socket编程 SocketServer Socket简介file可以对指定的文件进行打开、读写、关闭操作。socket是针对网络IO通信的，服务器和客户端之间进行打开、读写、关闭操作，也就是网络通信的核心与基础。 socket套接字，由Ip地址和port端口组成的元组，可以唯一确定网络通信的主体。 简单的示例 这个示例是阻塞型的，即服务器只能一次处理一个连接 同时处理多个连接，可以使用进程fork，线程，以及异步I/O 服务器端 12345678910111213141516171819202122232425262728293031#!/usr/bin/env python# -*- coding:utf-8 -*-import socketclass Server(object): def __init__(self, Host=None, Port=None): self.socket = socket.socket() host = Host if Host else socket.gethostname() port = Port if Port else 12345 self.socket.bind((host, port)) print u'服务器主机名：', host def run_server(self): self.socket.listen(5) coon, addr = self.socket.accept() print u'地址为%s的主机已连接！' % str(addr) coon.send(u'连接成功！你的地址%s' % str(addr)) while True: if str(coon.recv(1024)) == 'q': coon.send('bye!') coon.close() break else: text = coon.recv(1024) coon.send(u'你的问题是：%s' %text)if __name__ == '__main__': s = Server() s.run_server() 客户端 12345678910111213141516171819# -*- coding:utf-8 -*-import socketif __name__ == '__main__': s = socket.socket() s.connect((socket.gethostname(), 12345)) print s.recv(1024) s.send(u'hello, 这是一个客户端连接！') while True: text = raw_input("请输入：\n") if text == 'q': s.send(text) print s.recv(1024) break else: s.send(text) ret_text = s.recv(1024) print str(ret_text) ​ SocketServer利用socket模块创建的服务无法进行多进程的处理，当需要进行大量请求处理时，请求就会阻塞在队列中，甚至发生请求丢弃。并且如果我们需要大量的socket时，就需要重复创建许多socket、绑定端口….. SocketServer简化了网络服务器的编写。在进行socket创建时，使用SocketServer会大大减少创建的步骤，并且SocketServer使用了select它有4个类：TCPServer，UDPServer，UnixStreamServer，UnixDatagramServer。这4个类是同步进行处理的，另外通过ForkingMixIn和ThreadingMixIn类来支持异步。 使用步骤使用SocketServer的步骤简介 创建服务器的步骤。首先，你必须创建一个请求处理类，它是BaseRequestHandler的子类并重载其handle()方法。 实例化一个服务器类，传入服务器的地址和请求处理程序类。 最后，调用handle_request()(一般是调用其他事件循环或者使用select())或serve_forever()。 集成ThreadingMixIn类时需要处理异常关闭。daemon_threads指示服务器是否要等待线程终止，要是线程互相独立，必须要设置为True，默认是False。 无论用什么网络协议，服务器类有相同的外部方法和属性 实例：echo服务器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586#!/usr/bin/env python# -*- coding:utf-8 -*-import loggingimport sysimport SocketServerimport timelogging.basicConfig(level=logging.DEBUG, format='%(name)s: %(message)s',)class EchoRequestHandler(SocketServer.BaseRequestHandler): def __init__(self, request, client_address, server): self.logger = logging.getLogger('EchoRequestHandler') self.logger.debug('__init__') SocketServer.BaseRequestHandler.__init__(self, request, client_address, server) def setup(self): self.logger.debug('setup') return SocketServer.BaseRequestHandler.setup(self) def handle(self): self.logger.debug('handle') # Echo the back to the client data = self.request.recv(1024) self.logger.debug('recv()-&gt;"%s"', data) self.request.send(data) return def finish(self): self.logger.debug('finish') return SocketServer.BaseRequestHandler.finish(self)class EchoServer(SocketServer.TCPServer): def __init__(self, server_address, handler_class=EchoRequestHandler): self.logger = logging.getLogger('EchoServer') self.logger.debug('__init__') SocketServer.TCPServer.__init__(self, server_address, handler_class) return def server_activate(self): self.logger.debug('server_activate') SocketServer.TCPServer.server_activate(self) return def serve_forever(self): self.logger.debug('waiting for request') self.logger.info('Handling requests, press &lt;Ctrl-C&gt; to quit') while True: self.handle_request() return def handle_request(self): self.logger.debug('handle_request') return SocketServer.TCPServer.handle_request(self) def verify_request(self, request, client_address): self.logger.debug('verify_request(%s, %s)', request, client_address) return SocketServer.TCPServer.verify_request(self, request, client_address) def process_request(self, request, client_address): self.logger.debug('process_request(%s, %s)', request, client_address) return SocketServer.TCPServer.process_request(self, request, client_address) def server_close(self): self.logger.debug('server_close') return SocketServer.TCPServer.server_close(self) def finish_request(self, request, client_address): self.logger.debug('finish_request(%s, %s)', request, client_address) return SocketServer.TCPServer.finish_request(self, request, client_address) def close_request(self, request_address): self.logger.debug('close_request(%s)', request_address) return SocketServer.TCPServer.close_request(self, request_address)if __name__ == '__main__': address = ('localhost', 0) # let the kernel give us a port server = EchoServer(address, EchoRequestHandler) ip, port = server.server_address # find out what port we were given logger = logging.getLogger('client') logger.info('Server on %s:%s', ip, port) server.serve_forever()]]></content>
      <categories>
        <category>技术</category>
        <category>网络编程</category>
        <category>socket</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python logging模块使用]]></title>
    <url>%2F2017%2F10%2F14%2F%E6%8A%80%E6%9C%AF%2FPython%20logging%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[Python logging模块 为啥要用日志模块当我们编写python程序时，希望能到输出一些调试信息，一般情况我们用print打印出想要的信息。 使用print的缺点： 输出信息没有分级 程序发布时，要修改所有的print语句 无法输出信息到文件，或者发邮件 使用python的日志系统，就可以解决上述问题。 简单使用示例1234567891011121314151617181920212223242526272829303132333435# -*- coding: utf-8 -*- import loggingimport sys # 获取logger实例，如果参数为空则返回root loggerlogger = logging.getLogger("AppName") # 指定logger输出格式formatter = logging.Formatter('%(asctime)s %(levelname)-8s: %(message)s') # 文件日志file_handler = logging.FileHandler("test.log")file_handler.setFormatter(formatter) # 可以通过setFormatter指定输出格式 # 控制台日志console_handler = logging.StreamHandler(sys.stdout)console_handler.formatter = formatter # 也可以直接给formatter赋值 # 为logger添加的日志处理器logger.addHandler(file_handler)logger.addHandler(console_handler) # 指定日志的最低输出级别，默认为WARN级别logger.setLevel(logging.INFO) # 输出不同级别的loglogger.debug('this is debug info')logger.info('this is information')logger.warn('this is warning message')logger.error('this is error message')logger.fatal('this is fatal message, it is same as logger.critical')logger.critical('this is critical message')# 移除一些日志处理器logger.removeHandler(file_handler) 获取一个logger对象 添加Handler日志处理器 设置logger输出级别 GetLogger这是最基本的入口，该方法参数可以为空，默认的logger名称是root，如果在同一个程序中一直都使用同名的logger，其实会拿到同一个实例，使用这个技巧就可以跨模块调用同样的logger来记录日志。 另外你也可以通过日志名称来区分同一程序的不同模块，比如这个例子。 12logger = logging.getLogger("App.UI")logger = logging.getLogger("App.Service") 注意： 尽量不使用root logger，因为所有其他logger的输出，默认都会传给root一份，因而会出现多次root的日志输出。 同一日志被多次输出的原因，是绑定了多个handler。 比如下面这种常见的错误，每次调用get_logger()拿到的都是同一个logger实例App，但是每次调用都绑定一个console_handler，当这个函数被调用3次，App logger就会一次输出3条信息。 12345678def get_logger(): fmt = '%(levelname)s: %(message)s' console_handler = logging.StreamHandler() console_handler.setFormatter(logging.Formatter(fmt)) logger = logging.getLogger('App') # 返回的是同一个实例 logger.setLevel(logging.INFO) logger.addHandler(console_handler) return logger Formatter日志格式format = logging.Formatter(fmt=&quot;&quot;, datefmt=&quot;&quot;) Formatter对象定义了log信息的结构和内容，构造时需要带两个参数： 一个是格式化的模板fmt，默认会包含最基本的level和 message信息 一个是格式化的时间样式datefmt，默认为 2003-07-08 16:49:45,896 (%Y-%m-%d %H:%M:%S) fmt中允许使用的变量可以参考下表。 %(name)s Logger的名字 %(levelno)s 数字形式的日志级别 %(levelname)s 文本形式的日志级别 %(pathname)s 调用日志输出函数的模块的完整路径名，可能没有 %(filename)s 调用日志输出函数的模块的文件名 %(module)s 调用日志输出函数的模块名| %(funcName)s 调用日志输出函数的函数名| %(lineno)d 调用日志输出函数的语句所在的代码行 %(created)f 当前时间，用UNIX标准的表示时间的浮点数表示| %(relativeCreated)d 输出日志信息时的，自Logger创建以来的毫秒数| %(asctime)s 字符串形式的当前时间。默认格式是“2003-07-08 16:49:45,896”。逗号后面的是毫秒 %(thread)d 线程ID。可能没有 %(threadName)s 线程名。可能没有 %(process)d 进程ID。可能没有 %(message)s 用户输出的消息 fmt示例 123format = '%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s'datefmt='%a, %d %b %Y %H:%M:%S'# output: Sun, 24 May 2009 21:48:54 demo2.py[line:11] DEBUG This is debug message SetLevel 日志级别Logging有如下级别: NOTSET， DEBUG，INFO，WARNING，ERROR，CRITICAL，默认级别是WARNING，logging模块只会输出指定level以上的log。这样的好处, 就是在项目开发时debug用的log，在产品release阶段不用一一注释，只需要调整logger的级别就可以了，很方便。 Handler 日志处理器最常用的是StreamHandler和FileHandler, Handler用于向不同的输出端打log。Logging包含很多handler, 可能用到的有下面几种 StreamHandler instances send error messages to streams (file-like objects). FileHandler instances send error messages to disk files. RotatingFileHandler instances send error messages to disk files, with support for maximum log file sizes and log file rotation. TimedRotatingFileHandler instances send error messages to disk files, rotating the log file at certain timed intervals. SocketHandler instances send error messages to TCP/IP sockets. DatagramHandler instances send error messages to UDP sockets. SMTPHandler instances send error messages to a designated email address. Configuration 配置方法logging的配置大致有下面几种方式。 通过代码进行完整配置，主要是通过getLogger方法实现,并绑定handler。 通过代码进行简单配置，主要是通过basicConfig方法实现。 basicConfig获取到的是root logger，默认的handler是stream。 传入的参数: 12345678filename FileHandler be created, using the filename, rather than a StreamHandler.filemode the mode to open the file, if filename is specified ( defaults to &apos;a&apos;).format Use the specified format string for the handler.datefmt Use the specified date/time format.level Set the root logger level to the specified level.stream Use the specified stream to initialize the StreamHandler. Note that this argument is incompatible with &apos;filename&apos; - if both are present, &apos;stream&apos; is ignored. 通过配置文件，下面有例子，主要是通过 logging.config.fileConfig(filepath)]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python super与多重继承顺序]]></title>
    <url>%2F2017%2F10%2F14%2F%E6%8A%80%E6%9C%AF%2FPython-super%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[python多重继承的顺序 Python super对象 Base.__init__(self, args) 在Python2.2以前，未绑定的父类构造方法经常被使用A.__init__(self) 这种做法的缺点是， 当多重继承以后，想要修改代码的话，比如某个基类改名字了，那么所有调用该类方法的代码部分都要修改。 基类的方法可能会被多次调用 要保证子类能够覆盖父类，及生成所有的子类后再生成父类。这要看代码的质量，如果继承关系复杂，很容易把人搞晕。 super(Child, self).__init__(args) 在Python类的方法中，要调用父类的某个方法，一般会用super(X, self).func(args),尤其是在类初始化中。 使用super不会显示的调用父类方法。super对象会根据继承关系的顺序依次执行每个父类的__init__方法,如果每个类都使用super调用父类方法的话，那么可以确保每个类的__init__只会执行一次。 12345678910111213141516class A(object)： def __init__(self): print 'A'class B(A): # 直接调用类方法 def __init__(self): A.__init__(self) print 'B' class C(A): # 使用super构造 def __init__(self): super(C, self).__init__(self) print 'C' 假如有如下继承关系，F继承E, D; E继承B和C；C和D都继承A， A和B都继承Object对象，且ABCDE都使用了super() 那么实例化F对象时，继承顺序为F E B C D A object,符合拓扑排序。 拓扑排序满足关系，A的子节点B，一定不会出现在A之前。对于继承关系，这可以保证所有的子类先被访问，然后在访问它们的父类，不然就会造成父类方法没有被子类覆盖的情况。 MRO算法和C3算法MRO(Method Resolution Order决定python多重继承的继承顺序)。 MRO在python2.7和python3中没有采用BFS或DFS，而是C3算法，能够产生一个拓扑序列，保证唯一性和可重载性。 拓扑排序对一个有向无环图(Directed Acyclic Graph简称DAG)G进行拓扑排序，是将G中所有顶点排成一个线性序列，使得图中任意一对顶点u和v，若边(u,v)∈E(G)，则u在线性序列中出现在v之前。通常，这样的线性序列称为满足拓扑次序(Topological Order)的序列，简称拓扑序列。简单的说，由某个集合上的一个偏序得到该集合上的一个全序，这个操作称之为拓扑排序 ### C3算法的原理1234567L[D(B,C)] = D + merge(L[B],L[C],[B,C])以上表达式也等同于： ==&gt; Ｌ[D(B,C)] = D + merge(mro(B,object),mro(C,object),[B,C]) ==&gt; Ｌ[D(B,C)] = D + merge( [B,object], [C, object],[B,C]) [] : 列表表达式merge: Ｃ3算法的核心 《python高级编程》中是这样写的： 取第一个列表的头，也就是L[B,object] ，如果这个头不在任何表的尾部，那么将它加到class D的线性表中，并且从合并中的列表里删除 ；否则查找下一个列表的头，如果是个好的表头则取出它。 需要注意的是： 表头指是第一个元素 ，尾部是指除表头之外的其它所有元素 。如[A,B,C,D,E,F],A是表头，[B,C,D,E,F]是尾部。 方式解析： L(D(B,C)) = D + merge( [B,object] ,[C,object] , [B,C] ) ​ #列表[B,object]的表头是Ｂ，没有出现在其它表([C,object] 、[B,C] )的尾部 ​ = [D, B] + merge( [object], [C,object] , [C] ) ​ #列表[Ｃ,object]的表头是Ｃ，没有出现在其它表([object] 、[C] )的尾部 ，注意 [C] 这个列表只有表头，没有尾部 ​ = [D, B,C] + merge( [object] , [object] ) ​ = [D, B,C,object] C3算法产生拓扑序列对于以下的继承关系，其顺序为：ABECDF object 首先找入度为0的点，只有一个A，把A拿出来，把A相关的边剪掉，再找下一个入度为0的点，有两个点（B,C）,取最左原则，拿B，这是排序是AB，然后剪B相关的边，这时候入度为0的点有E和C，取最左。这时候排序为ABE，接着剪E相关的边，这时只有一个点入度为0，那就是C，取C，顺序为ABEC。剪C的边得到两个入度为0的点（DF），取最左D，顺序为ABECD，然后剪D相关的边，那么下一个入度为0的就是F，然后是object。那么最后的排序就为ABECDF object。]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyQt5 编译PyQt5 (python27, windows x64)]]></title>
    <url>%2F2017%2F10%2F13%2F%E6%8A%80%E6%9C%AF%2FPyQt5.9%E7%BC%96%E8%AF%91%20python27%2064%E4%BD%8DWindows%E7%89%88%2F</url>
    <content type="text"><![CDATA[编译PyQt5.9 基于python2.7 Windows 64位版 写在前面自己编译的PyQt5的原因 python27下，pyqt5没有官方支持的安装包， python3下可以直接安装 python-qt5是第三方编译提供的，pypi上pyqt版本为5.3，github上源码为5.7，不是最新版 使用python-qt5编写的程序使用pyinstaller打包，import sip error， sip对于py27也是需要自己编译安装的。 在我折腾了1天编译成功PyQt5.9后，突发奇想，尝试把PyQt5.sip这个包，直接复制到site-packages目录下，然后打包时使用--hidden-import import sip就可以了。 参考相关资料和教程： 构建PyQt5.8/python2.7 py32版本的，作者给出了编译后的win，linux，mac下的PyQt包，编译步骤较为详细 记录在Python2.7 x64 bit 下 PyQt5.8的编译过程 配置编译环境可供参考，编译过程嘛，呵呵。 编译的configure参数，可以参考pyqt5官网 记录编译过程准备工作 vs2015编译环境 我用的 visual studio 2015社区版本，安装的时候选择自定义，把c++的组件勾选上。 由于我选了默认安装，没有c++编译工具，安装好后还得打开vs2015，在新建c++项目的界面选择安装C++组件，又多花了近一个小时。 VS的安装过程十分漫长，需要数个小时，因为是在线安装。如果下载了完整的离线安装包，应该会很快。 Qt5.8环境 如果是vs2013环境，就下对应MSVC2013的Qt MSVC2015版64位Qt5.8 python2.7 在虚拟python环境下编译sip失败，编译pyqt5时没有报错。 后来干脆搞个全新的python27来编译PyQt5，干净清爽。 环境变量 新增变量名：QTDIR，值：C:\Qt\Qt5.8.0\5.8\msvc2015_64 在path变量中添加路径：C:\Qt\Qt5.8.0\5.8\msvc2015_64\bin 在path变量中添加新装的python27的路径。 为了避免不必要的问题，新增的变量放在path的最前面，编译完成后再删掉即可。 PyQt相关源码包 下载对应版本的pyqt，PyQt5， PyQt3D，Qscintilla2，Sip。 应该是pyqt5.8对应qt5.8。一开始我用的pyqt5.9+qt5.8，智障了。 开始编译编译SIP1234$ cd sip-4.19.2$ c:\python27\python.exe configure.py$ nmake$ nmake install # 这个安装命令放到最最后面在执行 得到的sip可执行文件在sipgen中，sip的Python模块（sip.pyd或sip.so）在siplib中。这时不要nmake install安装sip，先编译安装pyqt5。 编译时出现问题LINK : fatal error LNK1181: 无法打开输入文件“python27.lib” 原因可能是我用的anaconda的Python虚拟环境，后来用c:\python27\python.exe 没有报错。 编译PyQt5这里的sip使用的是编译生成的，不要使用nmake install之后的python路径下的那个。 QtNfc存在bug，不安装这个模块。 整个编译过程大概半个小时。 c:\Python27\python.exe configure.py --sip=..\sip-4.19.2\sipgen\sip.exe --sip-incdir=..\sip-4.19.2\siplib --disable=QtNfc --qmake=C:\Qt\Qt5.8.0\5.8\msvc2015_64\bin\qmake.exe 1234$ cd PyQt5_gpl-5.8.2$ c:\Python27\python.exe configure.py --sip=..\sip-4.19.2\sipgen\sip.exe --sip-incdir=..\sip-4.19.2\siplib --disable=QtNfc --qmake=C:\Qt\Qt5.8.0\5.8\msvc2015_64\bin\qmake.exe$ nmake$ nmake install 此时安装成功后，import PyQt5.QtCore 提示找不到dll，后面给解决方法。看打包整理部分。 编译PyQt3D1234$ cd PyQt3D_gpl-5.9$ c:\Python27\python.exe configure.py --sip=..\sip-4.19.3\sipgen\sip.exe --sip-incdir=..\sip-4.19.3\siplib --qmake=C:\Qt\Qt5.8.0\5.8\msvc2015_64\bin\qmake.exe$ nmake$ nmake install 编译QScintilla21234$ cd QScintilla_gpl-2.10\Qt4Qt5$ C:\Qt\Qt5.8.0\5.8\msvc2015_64\bin\qmake.exe$ nmake release$ nmake install 打包整理问题描述 PyQt5 ImportError: DLL load failed from PyQt5 import QtCore时提示找不到dll ‘unresolved reference’ pycharm idea里导入包没有自动补全提示 原因 由于nmake install安装的PyQt5只是把 编译生成的文件copy到了Python site-packages的PyQt5文件夹下，bat，exe和dll文件需要我们手动复制，很麻烦的说。 解决 复制相应的文件到PyQt5对应的位置 建议使用everything这个软件，可以快速定位windows上的文件。 Plugins 从C:\Qt\Qt5.8.0\5.8\msvc2015_64\plugins\复制到PyQt5下plugins文件夹， 会有3种文件，只保留xxx.dll,删除xxxd.dll和xxxd.pdb文件。 *.dll 从C:\Qt\Qt5.8.0\5.8\msvc2015_64\bin\下全部内容复制到PyQt5目录下 同上，包括xxx.dll，*.exe 等文件。不要复制xxxd.dll，*.pdb sip lib siplib中的sip.pyd或sip.so复制到pyqt5 上面的操作就是PyQt5.sip模块，如果还同时想安装sip包，nmake install sip包 将这4个文件复制进PyQt5。 同时将其复制到site-packages下。 bat文件复制到python安装目录下 pylupdate5.bat、pyrcc5.bat和pyuic5.bat 编辑C:\Python27\Lib\site-packages\PyQt5\__init__.py 我直接借用python-qt5包里的 1234567891011121314151617181920212223242526272829303132import osimport sys# Setup environment variablesdirname = os.path.dirname(__file__)sys.path.insert(0, dirname)os.environ['PATH'] += os.pathsep + dirname# Addresses error: "QtQuick" is not installedos.environ['QML2_IMPORT_PATH'] = os.path.join(dirname, 'qml')# Addresses error: Problem creating accessible interfaceos.environ['QT_PLUGIN_PATH'] = os.path.join(dirname, 'plugins')# Addresses error: ..could not find or load the Qt platform plugin "windows"os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = os.path.join(dirname, 'plugins', 'platforms')# Expose versionsversion_info = (0, 1, 1) # Version of this releaseversion = "%s.%s.%s" % version_info__version__ = versionpyqt_version_info = (5, 9, 1)pyqt_version = "%s.%s.%s" % pyqt_version_info__pyqt_version__ = pyqt_versionqt_version_info = (5, 8, 1)qt_version = "%s.%s.%s" % qt_version_info__qt_version__ = qt_version 这是知乎上给出的 123456import sysimport ospyqt5_path = os.path.realpath(os.path.dirname(__file__))sys.path.append(pyqt5_path)os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = pyqt5_path + '/plugins/platforms'os.environ['QT_PLUGIN_PATH'] = pyqt5_path + '/plugins' 复制到其他python，并配置系统环境变量 复制PyQt5整个文件夹到其他python27 64的site-packages即可完成PyQt5.9的安装。 为了可以正常运行qt designer.exe等，需要配置系统环境变量： QT_QPA_PLATFORM_PLUGIN_PATH QT_PLUGIN_PATH QML2_IMPORT_PATH 使用编译好的PyQt5 复制PyQt5和sip到site-packages 复制3个bat文件到python.exe同级目录 复制sip.h 到Python的include目录下 配置环境变量 QT_QPA_PLATFORM_PLUGIN_PATH QT_PLUGIN_PATH QML2_IMPORT_PATH 经过我在本机测试，PyQt5运行正常。编译好的文件分享给大家。 ​ ​]]></content>
      <categories>
        <category>技术</category>
        <category>GUI编程</category>
        <category>PyQt5</category>
      </categories>
      <tags>
        <tag>PyQt5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyQt5入门]]></title>
    <url>%2F2017%2F10%2F12%2F%E6%8A%80%E6%9C%AF%2FPyQt%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[PyQt入门开发环境配置Python2.7 + PyQt5 + Pycharm pypi源下载超时的话可以切换豆瓣pip源 我用的是python27，pyqt5.3， pycharm 安装PyQt5 如果你想使用py27+ pyqt58，参考知乎上的这篇，自行编译或使用作者提供的编译好的二进制包。我没试过，不保证成功。 python3 PyQt5官方编译版本仅支持python3，pip install pyqt5 python2.7 好在网上有编译的python2.7版本的pyqt，官网主页指出使用pip安装的版本为qt5.3,git安装的版本为qt5.7 pip install --index-url https://pypi.douban.com/simple python-qt5 pyqt5 designer python3需要单独安装pip install pyqt5-tools python2 python-qt5 里打包了designer工具，无需单独安装 配置Pycharm下面以python2为例，python3对应的designer.exe的位置不同。 打开Pycharm，进入设置，添加外部工具，QtDesigner和PyUIC QtDesigner QtDesigner用来生成ui文件 $FileDir$是pycharm的宏变量，即当前文件的路径 PyUIC 将ui文件转换成对应的python文件 参数为-m PyQt5.uic.pyuic $FileName$ -o $FileNameWithoutExtension$.py pyrcc 用于生成资源文件 我在虚拟环境的pyqt5文件夹里没找到pyrcc5.exe,但在系统Python的bin目录下有一个，就copy了一份，使用正常。系统的Python用pip list看了一下，并没有安装pyqt5，但是有qtpy这个包，有点奇怪，我系统的Python是anaconda发行版。 使用方法 首先生成资源的qrc文件： 然后右键生成资源的py文件 然后在需要引用资源的地方，先import py文件，然后资源的路径使用格式:文件路径/prefix_dir/文件名,冒号必不可少，文件路径是指_rc.py文件相对qrc资源文件的路径。当两者在同一路径时，可以不写。 问题描述 在pycharm &gt; tools &gt; external-tools中打开Qtdesigner时， This application failed to start because it could not find or load the Qt platform plugin “windows”. 原因： 环境变量没有添加。如果已经配置了环境变量，需要关闭并重新打开pycharm。 解决方法： 参考官方wiki在环境变量中增加： 路径中的替换为PyQt包所在的位置，查看你的安装位置 123import PyQt5PyQt5.__path__['D:\\qt_env\\lib\\site-packages\\PyQt5'] QT_QPA_PLATFORM_PLUGIN_PATH：&lt;QT_DIR&gt;/plugins/platforms QT_PLUGIN_PATH Example: &lt;QT_DIR&gt;/plugins QML_IMPORT_PATH： &lt;QT_DIR&gt;/qml QML2_IMPORT_PATH： &lt;QT_DIR&gt;/qml 配置eric6 这玩意儿是智障，自动补全一点也不智能 唯一优势就是可以通过选择组件和触发事件的方式批量生成一堆槽函数。 我还是用的pycharm 由于Qscintilla2在python2下没有编译的安装包，需要自行编译安装。 故选择在python3下安装。 如果使用了Python虚拟环境，则需要临时将python3.exe的目录设置在系统环境变量path里。 下载eric6，解压， 执行 ..\..\python.exe install.py PyQt学习教程和资料 建议先看完鱼c网友的帖子 知乎 - 大家是怎么学pyqt5的？ pyqt5教程-鱼c论坛 pyqt5 reference 官方 Qt官方文档 PyQt5与PyQt4的区别信号槽的不同 qt5中没有以下： QObject.connect() QObject.emit() SIGNAL() SLOT() All methods that had arguments that are usually the results of calls to SIGNAL() or SLOT() are no longer supported disconnect() takes no arguments and disconnects all connections to the QObject instance. Moudle组件的变化 PyQt4’s QtGui module has been split into PyQt5’s QtGui, QtPrintSupport and QtWidgets modules. Only the QGLContext, QGLFormat and QGLWidget classes are supported by PyQt5. PyQt4’s QtDeclarative, QtScript and QtScriptTools modules are not supported. These have been replaced by PyQt5’s QtQml andQtQuick modules. Unlike PyQt4, PyQt5 supports the creation of Python objects from QML. PyQt4’s QtWebKit module has been split into PyQt5’s QtWebKit and QtWebKitWidgets modules. PyQt4’s pyqtconfig module is not supported. The section The PyQt5 Extension API describes the support that PyQt5 provides to third-party packages (e.g. QScintilla) that want to build on top of PyQt5. 更多内容 Qt组件label 改变文本 改变字体 使用图片 buttonradio button 使用GroupBox放置一组互斥的单选框 LineEdit object.setText(“”) object.text() TextEdit多行文本框，只读/编辑属性,多行文本持续输出 TextBrowserProgressBar MenuBar 每个菜单选项为一个Qaction对象 Qdesigner中可以拖拽菜单项进行排序 Qaction可以添加图标， png格式的。在属性的icon中右边有个…打开文件按钮，点击添加icon 打开/关闭文件 QFileDialog.getSaveFileName QFileDialog.getOpenFileName win32com操作word打开文档 SliderBar Dial对话框 通知对话框 询问对话框 警告对话框 严重警告对话框 关于对话框 AboutQt对话框 标准输入对话框 getText getInt getItem 下拉 123456reply, ok = QInputDialog.getText( self, &apos;输入框&apos;, &apos;请输入姓名&apos;, QLineEdit.Normal, &apos;在此处输入你的姓名...&apos;)# title，提示，当前值，最小值，最大值reply, ok = QInputDialog.getInt(self, &apos;输入框&apos;, &apos;请输入你的年龄&apos;, 10, 5, 25)reply, ok = QInputDialog.getItem( self, &apos;请选择&apos;, &apos;最爱的水果是？&apos;, [&apos;香蕉&apos;, &apos;苹果&apos;, &apos;大菠萝&apos;]) 自定义输入对话框 一次获取多个变量 修改全区变量 个性化定制 通过额外创建一个窗体ui界面实现， 图片显示 使用designer右键组件，选择更改样式表，添加图片资源。 boader-img，自适应组件大小的填充图片。背景图片，按原图显示。原比例显示，自动缩放，保持比例不变。 可以在图片上添加自定义点击事件 label显示图片Qt Designer 右键 change StyleSheet GraphView图片Qt Designer 右键 change StyleSheet 为图片绑定鼠标点击事件 self.graphicsView.mousePressEvent = self.photo_clicked 更换图片 self.graphicsView_photo.setStyleSheet(&quot;border-image: url(:/source/3345.jpg);&quot;) 启动界面 QSplashScreen简介 程序启动时加载大量资源，启动缓慢，使用启动界面告知用户，程序运行中 显示logo 一些重要或有趣的图片和文字信息 显示启动界面时不能阻塞gui事件响应 实例 sleep是因为主窗口初始化很快就完成了，为了看到启动效果而加的延迟 123456789101112131415161718app = QApplication(sys.argv)splash = QSplashScreen(QPixmap(':source/4532.png'))splash.show()sleep(2)# 显示文字信息，文字的位置和颜色splash.showMessage('正在加载图片资源...', Qt.AlignBottom, Qt.blue)sleep(2)splash.showMessage('正在加载视频资源...', Qt.AlignBottom, Qt.blue)sleep(2)splash.showMessage('正在启动中...', Qt.AlignBottom, Qt.blue)sleep(2)app.processEvents() # 避免阻塞gui事件# 主窗口初始化mywindow = MyWindow()mywindow.show()# 关闭启动窗口splash.close()sys.exit(app.exec_()) 效果： 信号槽机制信号种类事件源：鼠标点击拖拽等，值改变。。。 信号和槽 所有的组件都可以产生信号，组件只负责发射信号。 信号需要连接到槽函数，才能得到处理。 通过QtDesigner连接信号与槽 通过手动代码连接信号与槽 装饰器方法 自定义信号参考博客 多重载的信号 1234567891011# 声明一个多重载版本的信号，包括了一个带int和str类型参数的信号，以及带str参数的信号preview_signal = pyqtSignal([int,str],[str])# 参数为int，str的信号help_signal = pyqtSignal(int, str)# 信号发射self.preview_signal[int, str].emit(1, 'ok') # 指定信号的版本self.preview_signal[str].emit('success!')# 槽函数绑定，同样需要指定信号的版本self.preview_signal[int, str].connect(self.func_1) # 指定信号的版本self.preview_signal[str].connect(self.func_2) ​ 一般流程如下：1、定义信号2、定义槽函数3、绑定信号和槽4、发射信号 注意事项： 1、自定义的信号在init()函数之前定义；2、自定义型号可以传递，str、int、list、object、float、tuple、dict等很多类型的参数；3、注意signal和slot的调用逻辑，避免signal和slot之间出现死循环。如在slot方法中继续发射该信号； 问题描述 &#39;PyQt5.QtCore.pyqtSignal&#39; object has no attribute &#39;connect&#39; 原因 pyqtSignal只有绑定了对象后才有connect方法 解决方法 在Qobject对象init()初始化之前定义信号pyqtSignal, 使其成为类变量而不是实例变量 使用connect方法时，要先创建Qobject对象的实例，然后调用实例的信号变量，直接Qobject.signal.connect就会出现这种错误 ​]]></content>
      <categories>
        <category>技术</category>
        <category>GUI编程</category>
        <category>PyQt5</category>
      </categories>
      <tags>
        <tag>PyQt5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyQt5 不同窗体之间传值]]></title>
    <url>%2F2017%2F10%2F11%2F%E6%8A%80%E6%9C%AF%2FPyQt5%E4%B8%8D%E5%90%8C%E7%AA%97%E4%BD%93%E4%B9%8B%E9%97%B4%E4%BC%A0%E5%80%BC%2F</url>
    <content type="text"><![CDATA[PyQt5 不同窗体之间传值 类静态方法自定义对话框返回值给主窗体 PyQt提供了一系列的标准对话框用于获取用户输入，QFileDialog、QInputDialog、QColorDialog、QFontDialog 第一种，自定义的Dialog对话框，我们可以通过类静态方法实现传值 主要注意几点：1、使用两个button(ok和cancel)分别连接accept()和reject()槽函数。2、实现一个静态函数，将参数通过该函数传出。3、在静态函数中实例化该类，并调用.exec()函数来显示执行这个对话框。4、通过.exec()的返回值来判断用户点的是ok还是cancel按钮，进而做出下一步判断。 在主窗口中调用静态函数方法，就可以得到传递的参数值，代码如下： data, ok = TestDialog.get_user() 1234567891011121314151617181920212223242526# 自定义对话框from test import Ui_Dialog as Test_Uiclass TestDialog(QDialog, Test_Ui): def __init__(self): super(TestDialog, self).__init__() self.setupUi(self) # accept/reject关闭窗口返回 1/0 self.buttonBox.accepted.connect(self.accept) self.buttonBox.rejected.connect(self.reject) self.data = &#123;'ok':1&#125; @staticmethod def get_user(parent=None): test_dialog = TestDialog() ok = test_dialog.exec_() data = test_dialog.data data['user'] = 'test input' return data, ok # 在主窗口中使用data, ok = TestDialog.get_user()if ok: print 'ok button clicked!'else: print 'cancel button clicked!' ​ 第二种，在主窗体中实例化自定义对话框，然后调用其属性和方法，最后destory对话框实例 12345test = TestDialog()ok = test.exec_()if ok: print test.datatest.destroy() 信号槽对于自定义的信号，使用方法是： 自定义的信号在init()函数之前定义signal_test= pyqtSignal(str) 在子对话框的槽函数发射信号，或在事件中发射信号。self.signal_test.emit((self.datetime.dateTime()).toString()) 在主窗体中连接信号和槽。dialog.signal_test.connect(self.getStrDate) 然后实现槽函数。 在自定义对话框里定义一个信号 12345678910111213141516171819from PyQt5.QtCore import pyqtSignal, pyqtSlotfrom test import Ui_Dialog as Test_Uiclass TestDialog(QDialog, Test_Ui): mysignal = pyqtSignal(dict) def __init__(self): super(TestDialog, self).__init__() self.setupUi(self) # accept/reject关闭窗口返回 1/0 self.buttonBox.accepted.connect(self.accepted) self.buttonBox.rejected.connect(self.reject) self.data = &#123;'ok':1&#125; @pyqtSlot(bool) def accepted(self, value): self.accept() data = self.data data['user'] = 'test input' self.mysignal.emit(data) # 发射信号，将dict数据传递出去 ​ 在主窗体里实例化自定义对话框，并为信号绑定槽函数 12345678test = TestDialog()test.mysignal.connect(self.process_data) # 绑定槽函数，接收数据test.exec_()test.destroy()# 处理数据的槽函数def process_data(self, data): print data 使用信号与槽需要注意的事项： 信号与槽机制与普通函数的调用一样，如果使用不当，在程序执行时也有可能产生死循环。所以在定义槽函数时一定要注意避免间接形成无限循环，即在槽中再次发射所接收到的同样信号。 ​ 如果一个信号与多个槽相连，当这个信号被发射时，与之相连的槽被激活的顺序是随机的。 ​ 宏定义不能用在signal 和slot的参数中。 ​ 信号和槽的参数个数与类型必须一致。]]></content>
      <categories>
        <category>技术</category>
        <category>GUI编程</category>
        <category>PyQt5</category>
      </categories>
      <tags>
        <tag>PyQt5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bootstrap 学习笔记]]></title>
    <url>%2F2017%2F10%2F08%2F%E6%8A%80%E6%9C%AF%2FBootStrap%2F</url>
    <content type="text"><![CDATA[Bootstrap入门 html5文件 引入css 引入jquery和bootstrap.js 查看官网文档和教程 栅格系统布局 grid grid文档 Bootstrap 需要为页面内容和栅格系统包裹一个 .container 容器 grid将屏幕横向像素12等分 媒体查询 媒体查询是进行响应式设计的核心要素，其功能非常强大Bootstrap主要用到min-width,max-width以及and语法，用于在不同的分辨率下设置不同的css样式 示例： 1234567891011@media(max-width:767px)&#123;/在小于767px的屏幕里，这里的样式才生效/&#125;@media(min-width:768px) and (max-width:991px)&#123;/768-991px屏幕里，这里的样式才生效/&#125;@media(min-width:1200px)&#123;/大于1200px屏幕里，这里的样式才生效/&#125; ​ ​ Bootstrap文本排版 列表与代码 Bootstrap Bootstrap表格样式 Bootstrap表单样式 Bootstrap按钮样式 Bootstrap图片 Bootstrap小图标 Bootstrap下拉菜单 Bootstrap按钮组 Bootstrap按钮下拉菜单 Bootstrap输入框 Bootstrap导航 Bootstrap导航条 Bootstrap面包屑导航和分页导航 标签 徽章 大屏展播 页面标题 缩略图和警告框 可关闭的警告框，data-dismiss是html5中绑定方法用的 1234&lt;div class="alert alert-info alert-dismissable" role="alert"&gt; &lt;button class="close " type="button" data-dismiss="alert"&gt;&amp;times;&lt;/button&gt; &lt;p&gt; 登录成功！返回 &lt;a href="#" class="alert-link"&gt;首页&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; ​ ##进度条和媒体内容 媒体对象：图文混排的风格 ##选项卡/标签页 导航栏中点击不同的选项卡，显示其对应的面板页面 可以用date-toggle=&quot;tab&quot;或date-toggle=&quot;pill&quot; 属性绑定面板切换动作 href指向对应面板的id，data-toggle绑定方法 1234567891011121314151617181920&lt;div class="container"&gt; &lt;!--选项卡 --&gt; &lt;ul class="nav nav-tabs"&gt; &lt;li&gt;&lt;a href="#pan1" data-toggle="tab"&gt;tab面板1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#pan2" data-toggle="tab"&gt;tab面板2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#pan3" data-toggle="tab"&gt;tab面板3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;!--面板--&gt; &lt;div class="tab-content"&gt; &lt;div class="tab-pane active" id="pan1"&gt; &lt;h1&gt;第一个面板页面&lt;/h1&gt; &lt;/div&gt; &lt;div class="tab-pane" id="pan2"&gt; &lt;h1&gt;第二个面板页面&lt;/h1&gt; &lt;/div&gt; &lt;div class="tab-pane" id="pan3"&gt; &lt;h1&gt;第三个面板页面&lt;/h1&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; js绑定面板显示，参考文档 12345678&lt;script type="application/javascript"&gt; $(function () &#123; $(".nav.nav-pills li a ").click(function (e) &#123; e.preventDefault(); $(this).tab('show'); &#125;); &#125;)&lt;/script&gt; 事件监听，当点击了某个选项卡，触发事件 12345678&lt;script language="JavaScript"&gt; $(function()&#123; $('a[data-toggle="tab"]').on('shown.bs.tab', function (e) &#123; // e.target // newly activated tab // e.relatedTarget // previous active tab var newobj=e.target.innerHTML; $("#testshow").html(newobj);&#125;); ​ ##工具提示框 移动到某个元素上弹出提示信息 文档 需要声明式语法和javascript配合，指定data-toggle、data-placement、$(“select”).tooltip() ##折叠 Collapse ##焦点广告/轮播图 ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ##]]></content>
      <categories>
        <category>技术</category>
        <category>Web</category>
        <category>前端</category>
      </categories>
      <tags>
        <tag>Bootstrap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL生成测试数据]]></title>
    <url>%2F2017%2F10%2F08%2F%E6%8A%80%E6%9C%AF%2Fmysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Ubuntu安装配置MySQL 安装mysqlsudo apt-get install mysql-server 安装过程中会提示设置root用户的密码 启动sudo service mysql start 重启sudo service mysql restart 停止sudo service mysql stop 配置文件主配置入口文件/etc/mysql/mysql.cnf, 这个文件里是全局配置，默认只设置了2个目录，告诉我们其中的配置文件，文件名后缀为cnf。 12!includedir /etc/mysql/conf.d/!includedir /etc/mysql/mysql.conf.d/ 挨个查看后发现mysql.conf.d/mysqld.cnf文件配置[client]和[mysqld]。 1234567891011121314151617181920212223242526[client]# 设置utf8可以支持中文default-character-set = utf8[mysqld_safe]socket = /var/run/mysqld/mysqld.socknice = 0[mysqld]user = mysqlpid-file = /var/run/mysqld/mysqld.pidsocket = /var/run/mysqld/mysqld.sockport = 3306basedir = /usr# 需要远程连接时，注释掉下面语句bind-address = 0.0.0.0 datadir = /var/lib/mysqltmpdir = /tmplc-messages-dir = /usr/share/mysqlmyisam-recover-options = BACKUPquery_cache_limit = 1Mquery_cache_size = 16Mlog_error = /var/log/mysql/error.log 设置MySQL允许远程访问注释掉bind-address = 0.0.0.0 设置用户允许远程登录mysql -h localhost -u root -p, 连接数据库，在命令台输入命令： use mysql; select host, user from user; GRANT ALL PRIVILEGES ON *.* TO root@&quot;%&quot; IDENTIFIED BY &#39;用户root的密码&#39;; 现在root可以支持远程登陆了，root的host变为%通配符。 允许root用户以任意ip地址远程登陆，一旦密码泄露是非常危险的。 推荐的做法: 允许root从内网ip 192.168.1.122上登陆，指定内网单个ip GRANT ALL PRIVILEGES ON *.* TO root@&quot;192.168.1.122&quot; IDENTIFIED BY &#39;用户root的密码&#39;; 创建新的用户，为其分配有限的权限，指定ip 12345678//创建库create database dbTest;//创建用户，设置密码create user &apos;useTest&apos;@&apos;%&apos; IDENTIFIED BY &apos;pwdTest&apos;;// 赋给该用户在dbTest数据上增删查，插入的权限， 连接ip为%，任意ipgrant select,update,delete,insert on dbTest.* to useTest@&apos;%&apos;;flush privileges;use dbTest; ​ 设置utf8字符集解决中文乱码问题，打开mysql控制台，输入命令show variables like &#39;character%&#39;;查看字符集。 修改配置文件 1234567[client]default-character-set=utf8[mysqld]default-storage-engine=INNODBcharacter-set-server=utf8collation-server=utf8_general_ci 在命令台输入命令set names utf8; 重启mysql后，使用命令查看字符集。 character_set_client character_set_connection character_set_results 以上三个控制mysql client的字符集 character_set_database ​ 设置数据库的默认字符集 character_set_server ​ 设置以上所有的默认字符集 安装MySQL驱动Python sudo apt-get install python-pipsudo apt-get install python-devsudo pip install mysql-python pip install mysql-python mysql-python安装时EnvironmentError: mysql_config not found 在安装 mysql-python时，会出现： 123456789sh: mysql_config: not foundTraceback (most recent call last): File &quot;setup.py&quot;, line 15, in &lt;module&gt; metadata, options = get_config() File &quot;/home/zhxia/apps/source/MySQL-python-1.2.3/setup_posix.py&quot;, line 43, in get_config libs = mysql_config(&quot;libs_r&quot;) File &quot;/home/zhxia/apps/source/MySQL-python-1.2.3/setup_posix.py&quot;, line 24, in mysql_config raise EnvironmentError(&quot;%s not found&quot; % (mysql_config.path,))EnvironmentError: mysql_config not found 只要原因是没有安装:libmysqlclient-dev 1sudo apt-get install libmysqlclient-dev 找到mysql_config文件的路径 12sudo updatedblocate mysql_config mysql_config的位置为：/usr/bin/mysql_config 在mysql-python源码包下找到：setup_posix.py 文件，然后找到文件中的 mysql_config.path 将其值改为：/usr/bin/mysql_config,然后 sudo python setup.py install ,就ok了 创建测试数据使用DB 存储过程生成数据 存储过程比较复杂，且sql提供的函数无法实现高级的数据处理功能 创建库和用户 12345create database dbTest;create user 'useTest'@'%' IDENTIFIED BY 'pwdTest';grant select,update,delete,insert on dbTest.* to useTest@'%';flush privileges;use dbTest; 创建表 12345678910DROP TABLE IF EXISTS `tabTest`;CREATE TABLE `tabTest` ( `testID` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT 'ID', `testTitle` varchar(10) DEFAULT NULL COMMENT 'title', `testBody` varchar(100) DEFAULT NULL COMMENT 'boey', `testType` tinyint(10) unsigned DEFAULT NULL COMMENT 'title len', `testValue` int(10) unsigned DEFAULT NULL COMMENT 'body len', PRIMARY KEY (`testID`)) ENGINE=MyISAM AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 COMMENT='test';alter table tabTest auto_increment=1; 创建存储过程 1234567891011121314151617181920212223//创建存储过程，里面一长段汉字，用于随机抽取内容，可以随便改这段文字，但其中最好不要含用符号。DELIMITER $$DROP PROCEDURE IF EXISTS `addTest`$$CREATE DEFINER=`root`@`localhost` PROCEDURE `addTest`(IN insSum int,IN lenTitle int,IN lenBody int)BEGIN DECLARE i INT UNSIGNED DEFAULT 1; SET i=0; set @str='据新华社电日前，中共中央政治局常委、国务院总理李克强主持国务院专题讲座，讨论加快发展先进制造与3D打印等问题。李克强指出，推动中国制造由大变强，要紧紧依靠深化改革和创新驱动' loop1: LOOP SET i=i+1; if i&gt;insSum then LEAVE loop1; end if; set @key=SUBSTRING(@str,FLOOR(rand()*(length(@str)/3-10)),2+FLOOR(rand()*lenTitle)); set @title=SUBSTRING(@str,FLOOR(rand()*(length(@str)/3-20)),5+FLOOR(rand()*lenBody)); insert INTO tabTest(testTitle,testBody,testType,testValue) values (@key,@title,length(@title),length(@key)); select i,insSum-i; END LOOP loop1;END$$DELIMITER ; ​ 调用存储过程插入数据 12//调用过程开始添加数据，数字为要添加的条数call addTest(2,5,80); 使用Python生成数据 使用Python批量插入数据 表结构： 1234567891011121314151617181920212223242526272829303132333435363738394041424344SET FOREIGN_KEY_CHECKS=0;-- ------------------------------ Table structure for major-- ----------------------------DROP TABLE IF EXISTS `major`;CREATE TABLE `major` ( `majorID` int(10) NOT NULL AUTO_INCREMENT, `schooltype` varchar(40) DEFAULT NULL, `major_name` varchar(255) DEFAULT NULL, `major_category` varchar(255) DEFAULT NULL, `major_hot` int(4) DEFAULT NULL, PRIMARY KEY (`majorID`)) ENGINE=InnoDB AUTO_INCREMENT=3787 DEFAULT CHARSET=utf8;-- ------------------------------ Table structure for school-- ----------------------------DROP TABLE IF EXISTS `school`;CREATE TABLE `school` ( `schoolID` int(10) NOT NULL AUTO_INCREMENT, `sname` varchar(255) NOT NULL, `location` varchar(255) DEFAULT NULL, `school_type` varchar(255) DEFAULT NULL, `school_category` varchar(50) DEFAULT NULL, `school_hot` int(4) DEFAULT NULL, PRIMARY KEY (`schoolID`)) ENGINE=InnoDB AUTO_INCREMENT=2779 DEFAULT CHARSET=utf8;-- ------------------------------ Table structure for student-- ----------------------------DROP TABLE IF EXISTS `student`;CREATE TABLE `student` ( `studentID` int(10) unsigned NOT NULL AUTO_INCREMENT, `studentName` varchar(50) NOT NULL, `age` int(2) NOT NULL, `schoolID` int(3) NOT NULL, `majorID` int(3) NOT NULL, `sex` varchar(4) NOT NULL DEFAULT '男', `score_4` int(3) DEFAULT '0', `score_6` int(3) DEFAULT '0', PRIMARY KEY (`studentID`)) ENGINE=InnoDB AUTO_INCREMENT=250708001 DEFAULT CHARSET=utf8; 一次插入多条数据： 1234567conn = MySQLdb.Connection("123.207.235.76", "root", "123456", "dbTest", charset='utf8')data = [(1,1,1), (2,2,2)]cursor = conn.cursor()sql = "insert into student(schoolID, score_4, score_6) " \ "VALUES (%s, %s, %s)"cursor.executemany(sql, data)conn.commit() 生成随机数据： forgery_py模块可以生成随机的英文名，地址，ip，品牌等测试数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#!/usr/bin/env python# coding:utf-8import MySQLdbimport csvimport timeimport forgery_py''' 写入major table'''def insert_major(): t1 = time.time() reader = csv.reader(file('major_result.csv')) reader.next() # 调过csv的第一行 data = [(x[0], x[1], int(x[2]), x[3]) for x in reader ] conn = MySQLdb.Connection("123.207.235.76","root","123456","dbTest", charset='utf8') cursor = conn.cursor() sql = """insert into major(major_name, major_category, major_hot,schooltype) VALUES (%s, %s, %s, %s)""" cursor.executemany(sql, data) conn.commit() cursor.close() conn.close() print time.time() - t1 # 1.23699998856''' 写入school table'''def insert_school(): reader = csv.reader(file('result.csv')) reader.next() data = [(x[0], x[1], x[2], x[4], int(x[3])) for x in reader ] t1 = time.time() conn = MySQLdb.Connection("123.207.235.76","root","123456","dbTest", charset='utf8') cursor = conn.cursor() sql = """insert into school(sname, location, school_type, school_category, school_hot) VALUES (%s, %s, %s, %s, %s)""" cursor.executemany(sql, data) conn.commit() cursor.close() conn.close() print time.time() - t1 #0.851999998093''' 写入school table'''def create_one(): name = forgery_py.name.full_name() sid = forgery_py.name.random.randint(1, 2778) mid = forgery_py.name.random.randint(2525, 3786) age = forgery_py.name.random.randint(18, 35) score_4 = forgery_py.name.random.uniform(400, 750) score_6 = forgery_py.name.random.uniform(400, 750) sex = forgery_py.name.random.choice([u'男', u'女']) return (name, age, sid, mid, sex, score_4, score_6)def insert_student(): conn = MySQLdb.Connection("localhost", "root", "123456", "dbTest", charset='utf8') cursor = conn.cursor() t1 = time.time() for i in range(200): data = [create_one() for n in range(2000)] sql = "insert into student(studentName, age, schoolID, majorID, sex, score_4, score_6) " \ "VALUES (%s, %s, %s, %s, %s, %s, %s)" cursor.executemany(sql, data) conn.commit() del data cursor.close() conn.close() print time.time() - t1 # 1.23699998856if __name__ == '__main__': insert_major() insert_school() insert_student() ​ 查询测试单表查询 在有索引的情况下执行查询： 表的主键、外键是必须有索引的，经常与其他表连接的字段，经常出现在where子句中的字段都应该有索引。 123456789101112131415SELECT * from student where studentID=250000000;/*受影响的行: 0时间: 0.037s*/UPDATE student SET studentName = 'Wanda Lawrence1' WHERE studentID=250000000;/*受影响的行: 1时间: 0.101s*/ 在没有索引的字段执行查询 使用LIMIT进行分页查询，一次查询想要拿到的结果越多，扫描权标花费的时间就越多。 123456789101112131415161718192021222324252627282930313233343536373839404142434445SELECT * from student where studentName = 'Wanda Lawrence1';/* studentName = 'Wanda Lawrence1'的记录仅有一条，且是第第2.5亿条记录受影响的行: 0时间: 1651.702s， 27分钟。*/SELECT * FROM studentWHERE studentName = 'Wanda Lawrence'LIMIT 0, 100;/*受影响的行: 0时间: 6.525s 第100个符合条件的记录，其位置为2297114，也就是条件查询6.5s遍历了229万条数据。*/[SQL]SELECT * FROM studentWHERE studentName = 'Wanda Lawrence'LIMIT 0, 100;/*受影响的行: 0时间: 1.599s第100个符合条件的记录，其位置为2297114。相比上次的6.525秒的执行速度，这次只用了1.599s,说明mysql内部有缓存机制*/[SQL]SELECT * FROM studentWHERE studentName = 'Wanda Lawrence'LIMIT 0, 10;/*受影响的行: 0时间: 0.257s第10个符合条件的记录，其位置为289394， 0.257s遍历了29万条记录， 可能会受到缓存的影响*/[SQL]SELECT * FROM studentWHERE studentName = 'Michelle Warren'LIMIT 0, 10;/*受影响的行: 0时间: 0.161s第10个符合条件的记录，其位置为236033， 与上面的结果相比，mysql内部缓存机制对于30万级别的记录影响不大。*/ 数据库中避免使用空值变量null 对于一个数值字段可以设置0代替null,速度差别10倍 1234567891011121314151617[SQL]SELECT * FROM studentWHERE age = nullLIMIT 0, 10;/*受影响的行: 0时间: 2.161s第10个符合条件的记录，其位置为199712*/[SQL]SELECT * FROM studentWHERE age = 0LIMIT 0, 10;/*受影响的行: 0时间: 0.191s第10个符合条件的记录，其位置为2558733*/ 避免在where子句中使用or 使用Union all 连接2个子查询，比使用or的逻辑条件查询速度快很多。 从下面的执行结果可以看出二者相差8倍，这还是在查询字段有索引的情况下。 1234567891011121314151617181920212223242526-- 有索引的字段[SQL]SELECT * FROM studentWHERE studentID = 254000 or studentID = 220000000;/*受影响的行: 0时间: 0.224s*/[SQL]SELECT * FROM studentWHERE studentID = 254000 UNION ALLSELECT * FROM studentWHERE studentID = 220000000;/*受影响的行: 0时间: 0.038s*/-- 没有索引的字段-- union 联合查询无法配合分页Limit使用，下面的sql语句语法错误[SQL]SELECT * FROM student WHERE studentName = 'Carol Henderson'UNION ALLSELECT * FROM student WHERE studentName = 'Andrea Griffin'LIMIT 0, 100; 不要用模糊匹配，如果有必要可以使用全文检索提高效率。 123456789101112[SQL]SELECT * FROM student WHERE studentName LIKE 'Wanda%2' LIMIT 1;/* 受影响的行: 0时间: 1.046s*/[SQL]SELECT * FROM student WHERE studentName = 'Wanda Lawrence2' LIMIT 1;/* 受影响的行: 0时间: 0.975s*/ in和 not in也要慎用，否则会导致全表扫描： 当查询条件是连续型数值时，使用between替换in语法 1234567891011121314[SQL]SELECT * FROM student WHERE majorID in (2841, 2842, 2843, 2844, 2845)LIMIT 10000;/*受影响的行: 0时间: 7.291s*/[SQL]SELECT * FROM student WHERE majorID BETWEEN 2841 AND 2845LIMIT 10000;/*受影响的行: 0时间: 7.019s*/ 、应尽量避免在 where子句中对字段进行表达式操作 123456789101112131415[SQL]SELECT * FROM student WHERE majorID/2 = 1422LIMIT 1000;/*受影响的行: 0时间: 3.733s*/[SQL]SELECT * FROM student WHERE majorID = 1422*2LIMIT 1000;/*受影响的行: 0时间: 1.174s*/ 应尽量避免在where子句中对字段进行函数操作,不要在 where子句中的“=”左边进行函数、算术运算或其他表达式运算 123456789101112131415[SQL]SELECT * FROM student WHERE SUBSTRING(studentName, 1, 5) = 'Wanda'LIMIT 10000;/*受影响的行: 0时间: 3.015s*/[SQL]SELECT * FROM student WHERE studentName LIKE 'Wanda%'LIMIT 10000;/*受影响的行: 0时间: 2.871s*/ 多表查询 用 exists代替 in执行嵌套查询： 123456789101112131415161718192021222324252627282930313233343536[SQL]SELECT *FROM student st WHERE st.majorID in (SELECT majorID FROM major mj WHERE mj.major_category = '财经类' AND mj.major_name = '证券投资与管理') AND st.schoolID in (SELECT schoolID FROM school sc WHERE sc.school_category = '财经类' AND sc.school_type = '高职高专' AND sc.location = '上海') AND st.age = 33 AND st.sex = '女'LIMIT 1;/*受影响的行: 0时间: 62.578s*/[SQL]SELECT *FROM student st WHERE EXISTS (SELECT 1 FROM major mj WHERE st.majorID = mj.majorID AND mj.major_category = '财经类' AND mj.major_name = '证券投资与管理') AND EXISTS (SELECT 1 FROM school sc WHERE sc.schoolID=st.schoolID AND sc.school_category = '财经类' AND sc.school_type = '高职高专' AND sc.location = '上海') AND st.age = 33 AND st.sex = '女'LIMIT 1;/*受影响的行: 0时间: 68.563s*/ join 123456789101112131415161718192021222324252627282930313233343536[SQL]SELECT *FROM student st, major mj, school scWHERE st.age = 33 AND st.sex = '女' AND st.majorID = mj.majorID AND st.schoolID = sc.schoolID AND sc.school_category = '财经类' AND sc.school_type = '高职高专' AND sc.location = '上海' AND mj.major_category = '财经类' AND mj.major_name = '证券投资与管理'LIMIT 1;/*受影响的行: 0时间: 76.140s扫描到第200万行记录*/[SQL]SELECT *FROM student st JOIN major mj ON (st.majorID = mj.majorID AND mj.major_category = '财经类' AND mj.major_name = '证券投资与管理') JOIN school sc ON (st.schoolID = sc.schoolID AND sc.school_category = '财经类' AND sc.school_type = '高职高专' AND sc.location = '上海')WHERE st.age = 33 AND st.sex = '女'LIMIT 1;/*受影响的行: 0时间: 65.335s*/ ​]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo lightum 博客折腾之旅]]></title>
    <url>%2F2017%2F10%2F07%2F%E6%8A%80%E6%9C%AF%2Fhexo-lightum%2F</url>
    <content type="text"><![CDATA[最近打算将原来的博客从jekyll转到hexo，瞎折腾了好久。最初用的是next主题，好处是next文档丰富，插件齐全。 后来又看上了light主题，但是light用的人不多，相关的插件、功能只好参考next以及其他hexo主题提供的解决方案，自己整合。 我喜欢的博客主题风格 有哪些简洁明快的jekyll模板 我最开始用的是jekyll，主题是首页 | 林安亚的博客 闫肃的博客 现在依然觉得这个博客好看且实用 hexo-next 用了一段时间，功能丰富，配置简单。不过后来看到了light主题，个人审美更喜欢light hexo-lightum 基于light的改进主题，也是我现在在使用的 其他好看的博客 琉璃之鸟 wordpress的博客，当初玩饥荒查攻略时进入的网站，感觉挺不错的。 Crazy Coder SimplyNetKeeper电信E信路由器破解连接软件，作者的博客。 图月志 这个很酷炫，不过是Bo-Blog，mysql+PHP，模板不清楚。 Hexo博客折腾之路 确定主题模板 基于light改进的lightum 提供我现在博客的模板及配置 更改category分级 参考 分级可以设置显示到几级 在theme的config中设置 1234# category显示的层级# 0显示所有分级，-1显示所有但不分级，1 只显示第一层的分类， n显示n层分级。# 不想分级时设值为1category_depth: 3 ​ 在每篇post文章的Front-matter 中category: 可以设置多个值 归档和文章目录 日历云挂件 参考 配置 12calendar: language: zh-CN ​ ​ 自定义layout 一个md文件或html，txt，采用哪种layout渲染，是由Front-matter中的layout字段定义的。默认使用post，如果使用样式，则layout:false hexo默认提供了post和scraft两种样式，这里我自己定义了resume、game、book三种样式。 看了这2个例子，应该就会写自己的layout了，关键还是看网页设计的能力 resume简历 参考教程 book和game ejs语法和百度静态文件cdn 12345678910111213141516//ejs语法//for ，if-else-if&lt;% for (var i in page.books)&#123; %&gt; &lt;% if (page.books[i].status == '已读') &#123;%&gt; &lt;span class="label label-success"&gt;&lt;%- page.books[i].status %&gt;&lt;/span&gt; &lt;% &#125; else if (page.books[i].status == '在读')&#123; %&gt; &lt;span class="label label-info"&gt;&lt;%- page.books[i].status %&gt;&lt;/span&gt; &lt;% &#125; else if (page.books[i].status == '未读') &#123; %&gt; &lt;span class="label label-default"&gt;&lt;%- page.books[i].status %&gt;&lt;/span&gt; &lt;% &#125; %&gt;&lt;% &#125; %&gt; //forEach&lt;% item.photos.forEach(function(photo)&#123; %&gt; &lt;%- photo.url %&gt;&lt;% &#125; %&gt; 使用了bootstrap 百度静态文件cdn资源 在lightum\layout\_partial\head.ejs引入js和css文件 12&lt;% if (page.books || page.games)&#123; %&gt;&lt;script src="//apps.bdimg.com/libs/bootstrap/3.2.0/js/bootstrap.min.js"&gt;&lt;/script&gt;&lt;% &#125; %&gt;&lt;% if (page.books || page.games) &#123; %&gt;&lt;link rel="stylesheet" href="http://apps.bdimg.com/libs/bootstrap/3.2.0/css/bootstrap.min.css" type="text/css"&gt;&lt;%&#125; %&gt; ​ 添加源码 lightum\layout\game.ejs 内容指向实际存放模板的位置&lt;%- partial(&#39;_partial/game&#39;) %&gt; lightum\layout\_partial/game.ejs 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;div class="row"&gt; &lt;div class="col-md-13 aside3-title"&gt; &lt;br&gt; &lt;h2 id="#identifier"&gt;&lt;%- page.title %&gt;&lt;/h2&gt; &lt;/div&gt; &lt;div class="col-md-13 aside3-content"&gt; &lt;br /&gt; &lt;br /&gt; &lt;div class="row"&gt; &lt;% page.games.forEach(function(game)&#123; %&gt; &lt;div class="col-md-12"&gt; &lt;div class="panel panel-primary"&gt; &lt;div class="panel-heading"&gt;&lt;%- game.title %&gt;&lt;/div&gt; &lt;div class="panel-body"&gt; &lt;div class="row"&gt; &lt;div class="col-md-4 col-xs-12 center"&gt; &lt;img src="&lt;%- game.cover %&gt;" alt="cover" class="img-thumbnail"&gt; &lt;/div&gt; &lt;div class="col-md-8 col-xs-12"&gt; &lt;table class="table table-bordered"&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td style="width:80px;"&gt;游戏平台&lt;/td&gt;&lt;td&gt;&lt;%- game.platform %&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;出版商&lt;/td&gt;&lt;td&gt;&lt;%- game.publisher %&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;语言&lt;/td&gt;&lt;td&gt;&lt;%- game.language %&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt; &lt;td&gt;链接&lt;/td&gt;&lt;td&gt; &lt;a href="&lt;%- game.link %&gt;" title="&lt;%- game.link %&gt;"&gt;游戏链接&lt;/a&gt; &lt;/td&gt;&lt;/tr&gt; &lt;tr&gt; &lt;td&gt;状态&lt;/td&gt; &lt;td&gt; &lt;% if (game.status == '已通关')&#123; %&gt; &lt;span class="label label-success"&gt;&lt;%- game.status %&gt;&lt;/span&gt; &lt;% &#125; else if (game.status == '进行中') &#123; %&gt; &lt;span class="label label-info"&gt;&lt;%- game.status %&gt;&lt;/span&gt; &lt;% &#125; else if (game.status == '未开始') &#123;%&gt; &lt;span class="label label-default"&gt;&lt;%- game.status %&gt;&lt;/span&gt; &lt;% &#125;%&gt; &lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;类型&lt;/td&gt;&lt;td&gt;&lt;%- game.type %&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;评价&lt;/td&gt;&lt;td&gt;&lt;%- game.description %&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;% &#125;) %&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; ​ front-matter 1234567891011121314151617181920212223242526272829303132---layout: gametitle: 2016年玩过的游戏category: - 生活 - 游戏keywords: - 游戏 - 娱乐 - 2016tags: - Enjoygames: - title: 饥荒联机版（Don't starve together） cover: http://ww4.sinaimg.cn/large/af9df239gw1f5dmat61rdj209q0d3jrq.jpg link: http://www.gamersky.com/z/dontstarve/ publisher: Klei Entertainment platform: steam type: 动作冒险，收集生存类沙盒游戏，2D画风,背景音乐超赞，在线多人联机 description: steam好评如潮 status: 进行中 language: 可用中文mod - title: 传送门2（Portal 2） cover: http://ww2.sinaimg.cn/large/af9df239gw1f5dmc8dce8j20dw0jjabi.jpg link: http://www.gamersky.com/z/portal2/ publisher: Valve Software language: 支持中文 platform: steam type: 第一人称视角，解迷，冒险，科幻,在线双人合作 description: steam好评如潮 status: 进行中--- ​ 效果 添加导航栏icon图标hexo theme大多数都是用的font-awesome字体图标库，在主题下的config文件，导航栏配置相应的icon即可。 1234567891011121314151617menu: home: / archives: /archives about: /resume Books: /books #This is your books page Movies: /movies #This is your movies page #Leetcode: /leetcode #tools: /toolsmenu_icon: home: home archives: archive about: user Books: book Movies: play-circle #Leetcode: file-text #tools: user 添加打赏 挂件中的打赏，直接替换支付宝图片的url即可 文章内的打赏，我用的是github上别人的代码 在主题的配置文件中设置donate为true 功能实现代码在\themes\lightum\layout\_partial\post\donate.ejs 在\themes\lightum\layout\_partial\article.ejs中引用上述文件 百度分享 去百度分享获取分享代码 功能实现 \themes\lightum\layout\_partial\post\share.ejs 在article的样式中引用 \themes\lightum\layout\_partial\article.ejs 在主题config中添加一个开关 share: true 百度统计和谷歌统计 还有百度收录和谷歌收录 获取百度和谷歌统计的代码，将其中的id或content写入主题配置 1234567google_analytics: UA-107525911-1# &lt;meta name="google-site-verification" content="hLu-CnRbqpthWxa76MJ-vpnGr7yMChNtTW6KA0pRMQo" /&gt;google_site_verification: hLu-CnRbqpthWxa76MJ-vpnGr7yMChNtTW6KA0pRMQo# &lt;meta name="baidu-site-verification" content="SOcrumVYOq" /&gt;# hm.src = "https://hm.baidu.com/hm.js?02f792017724a2c2af494ece7edc5fd1";baidu_analytics: 02f792017724a2c2af494ece7edc5fd1baidu_site_verification: WTddywKheI 安装sitemap和baidu-sitemap插件 站点配置文件中生成sitemap的选项 12345# 自动生成sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 在谷歌或百度中设置自动爬取sitemap或手动提交 百度有个自动提交push脚本，放在每个页面的head中的。 ​ 本地搜索 参考教程1 参考教程2 需要安装插件本地搜索的插件 在站点配置文件中设置 12345search: path: search.xml field: post format: html limit: 10000 微博秀微博秀的API接口，获取代码后贴到微博挂件的ejs文件里即可。 评论系统 disqus国内被q，多说倒闭，其他的不知道还能挺多久。我用的是来必力，支持qq、微博、微信登录后评论 在主题配置文件中添加livere_uid: MTAyMC8yOTQxxxxxxooooo id去官网注册获取 相关代码在article.ejs中引用comment.ejs，更换其他评论系统也不难 豆瓣读书和电影 使用了hexo-douban插件 npm install hexo-douban 安装插件，然后站点配置文件，填入豆瓣ID 123456789# hexo-doubandouban: user: shuaiyy book: title: 我的阅读 quote: 2017年，我使用&lt;a href="https://www.douban.com/people/shuaiyy/" target="_blank"&gt;豆瓣&lt;/a&gt;记录我的阅读。 movie: title: 我的电影 quote: 2017年，我使用&lt;a href="https://www.douban.com/people/shuaiyy/" target="_blank"&gt;豆瓣&lt;/a&gt;记录我看的电影。 ​ 详细参考插件地址 写文章markdown神器 Typora不多BB，本文就是这么写的。 插入网易云音乐歌曲点击生成外链播放器，获取分享的html代码，直接黏贴，比如下面的 flash的代码 iframe的代码 部署和发布 部署到github和coding上，域名解析国内解析到coding，国外解析到github 使用ssh git部署 部署 创建代码仓库并配置ssh公钥 在站点config中添加 12345deploy: type: git repo: coding: git@git.coding.net:shuaiyy/shuaiyy.git,master github: git@github.com:shuaiyy/shuaiyy.github.io.git,master ​ 在git bash中使用ssh-add 添加ssh私钥 这样主机就能和github或coding ssh上传数据了 执行命令hexo d 发布绑定域名 首先去github，告诉github xxx.com对应的是哪个blog。 然后去域名解析商那里，告诉任何访问xxx.com的人，xxx.com是github的服务器提供的。 github github的CNAME文件指定你的域名xxx.com，然后域名解析商那里的CNAME记录国外线路指定’xxx.github.io’,即github page给你的二级域名 ​ coding coding是由pages.coding.me服务器为你做的地址跳转，因此国内DNS解析的CNAME记录指向pages.coding.me，而不是coding为你博客提供的二级域名。 github使用网站根目录的CNAME文件绑定域名，而coding需要在项目的page服务选项里设置绑定域名。 示例 github：source目录下的CNAME文件下，写入你的域名。CNAME文件名必须大写 coding：可以最多绑定5个，github由于是CNAME文件，只能绑定一个 dns解析：我的是dnspod解析商 404页面你的404.html的Front-matter中要设置layout: false title: 404。 然后把他丢到source根目录下即可。 1234567---layout: falsetitle: "404"---&lt;html&gt;你访问的页面找不到了！&lt;/html&gt; 网站的icon图标将favicon.ico文件丢入source目录即可 待改进的地方代码高亮的样式这个不会改，如果可以，我想换成night的代码高亮主题 重写标签云 现在的标签云挂件是一排的，占的地方多且不美观，有空改成一行好几个tag的那种。 点击回到顶部按钮]]></content>
      <categories>
        <category>折腾</category>
        <category>博客</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo lightum 博客折腾之旅]]></title>
    <url>%2F2017%2F10%2F07%2F%E6%8A%98%E8%85%BE%2Fhexo-lightum%2F</url>
    <content type="text"><![CDATA[最近打算将原来的博客从jekyll转到hexo，瞎折腾了好久。最初用的是next主题，好处是next文档丰富，插件齐全。 后来又看上了light主题，但是light用的人不多，相关的插件、功能只好参考next以及其他hexo主题提供的解决方案，自己整合。 我喜欢的博客主题风格 有哪些简洁明快的jekyll模板 我最开始用的是jekyll，主题是首页 | 林安亚的博客 闫肃的博客 现在依然觉得这个博客好看且实用 hexo-next 用了一段时间，功能丰富，配置简单。不过后来看到了light主题，个人审美更喜欢light hexo-lightum 基于light的改进主题，也是我现在在使用的 其他好看的博客 琉璃之鸟 wordpress的博客，当初玩饥荒查攻略时进入的网站，感觉挺不错的。 Crazy Coder SimplyNetKeeper电信E信路由器破解连接软件，作者的博客。 图月志 这个很酷炫，不过是Bo-Blog，mysql+PHP，模板不清楚。 Hexo博客折腾之路 确定主题模板 基于light改进的lightum 提供我现在博客的模板及配置 更改category分级 参考 分级可以设置显示到几级 在theme的config中设置 1234# category显示的层级# 0显示所有分级，-1显示所有但不分级，1 只显示第一层的分类， n显示n层分级。# 不想分级时设值为1category_depth: 3 ​ 在每篇post文章的Front-matter 中category: 可以设置多个值 归档和文章目录 日历云挂件 参考 配置 12calendar: language: zh-CN ​ ​ 自定义layout 一个md文件或html，txt，采用哪种layout渲染，是由Front-matter中的layout字段定义的。默认使用post，如果使用样式，则layout:false hexo默认提供了post和scraft两种样式，这里我自己定义了resume、game、book三种样式。 看了这2个例子，应该就会写自己的layout了，关键还是看网页设计的能力 resume简历 参考教程 book和game ejs语法和百度静态文件cdn 12345678910111213141516//ejs语法//for ，if-else-if&lt;% for (var i in page.books)&#123; %&gt; &lt;% if (page.books[i].status == '已读') &#123;%&gt; &lt;span class="label label-success"&gt;&lt;%- page.books[i].status %&gt;&lt;/span&gt; &lt;% &#125; else if (page.books[i].status == '在读')&#123; %&gt; &lt;span class="label label-info"&gt;&lt;%- page.books[i].status %&gt;&lt;/span&gt; &lt;% &#125; else if (page.books[i].status == '未读') &#123; %&gt; &lt;span class="label label-default"&gt;&lt;%- page.books[i].status %&gt;&lt;/span&gt; &lt;% &#125; %&gt;&lt;% &#125; %&gt; //forEach&lt;% item.photos.forEach(function(photo)&#123; %&gt; &lt;%- photo.url %&gt;&lt;% &#125; %&gt; 使用了bootstrap 百度静态文件cdn资源 在lightum\layout\_partial\head.ejs引入js和css文件 12&lt;% if (page.books || page.games)&#123; %&gt;&lt;script src="//apps.bdimg.com/libs/bootstrap/3.2.0/js/bootstrap.min.js"&gt;&lt;/script&gt;&lt;% &#125; %&gt;&lt;% if (page.books || page.games) &#123; %&gt;&lt;link rel="stylesheet" href="http://apps.bdimg.com/libs/bootstrap/3.2.0/css/bootstrap.min.css" type="text/css"&gt;&lt;%&#125; %&gt; ​ 添加源码 lightum\layout\game.ejs 内容指向实际存放模板的位置&lt;%- partial(&#39;_partial/game&#39;) %&gt; lightum\layout\_partial/game.ejs 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;div class="row"&gt; &lt;div class="col-md-13 aside3-title"&gt; &lt;br&gt; &lt;h2 id="#identifier"&gt;&lt;%- page.title %&gt;&lt;/h2&gt; &lt;/div&gt; &lt;div class="col-md-13 aside3-content"&gt; &lt;br /&gt; &lt;br /&gt; &lt;div class="row"&gt; &lt;% page.games.forEach(function(game)&#123; %&gt; &lt;div class="col-md-12"&gt; &lt;div class="panel panel-primary"&gt; &lt;div class="panel-heading"&gt;&lt;%- game.title %&gt;&lt;/div&gt; &lt;div class="panel-body"&gt; &lt;div class="row"&gt; &lt;div class="col-md-4 col-xs-12 center"&gt; &lt;img src="&lt;%- game.cover %&gt;" alt="cover" class="img-thumbnail"&gt; &lt;/div&gt; &lt;div class="col-md-8 col-xs-12"&gt; &lt;table class="table table-bordered"&gt; &lt;tbody&gt; &lt;tr&gt;&lt;td style="width:80px;"&gt;游戏平台&lt;/td&gt;&lt;td&gt;&lt;%- game.platform %&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;出版商&lt;/td&gt;&lt;td&gt;&lt;%- game.publisher %&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;语言&lt;/td&gt;&lt;td&gt;&lt;%- game.language %&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt; &lt;td&gt;链接&lt;/td&gt;&lt;td&gt; &lt;a href="&lt;%- game.link %&gt;" title="&lt;%- game.link %&gt;"&gt;游戏链接&lt;/a&gt; &lt;/td&gt;&lt;/tr&gt; &lt;tr&gt; &lt;td&gt;状态&lt;/td&gt; &lt;td&gt; &lt;% if (game.status == '已通关')&#123; %&gt; &lt;span class="label label-success"&gt;&lt;%- game.status %&gt;&lt;/span&gt; &lt;% &#125; else if (game.status == '进行中') &#123; %&gt; &lt;span class="label label-info"&gt;&lt;%- game.status %&gt;&lt;/span&gt; &lt;% &#125; else if (game.status == '未开始') &#123;%&gt; &lt;span class="label label-default"&gt;&lt;%- game.status %&gt;&lt;/span&gt; &lt;% &#125;%&gt; &lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;类型&lt;/td&gt;&lt;td&gt;&lt;%- game.type %&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;评价&lt;/td&gt;&lt;td&gt;&lt;%- game.description %&gt;&lt;/td&gt;&lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;% &#125;) %&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; ​ front-matter 1234567891011121314151617181920212223242526272829303132---layout: gametitle: 2016年玩过的游戏category: - 生活 - 游戏keywords: - 游戏 - 娱乐 - 2016tags: - Enjoygames: - title: 饥荒联机版（Don't starve together） cover: http://ww4.sinaimg.cn/large/af9df239gw1f5dmat61rdj209q0d3jrq.jpg link: http://www.gamersky.com/z/dontstarve/ publisher: Klei Entertainment platform: steam type: 动作冒险，收集生存类沙盒游戏，2D画风,背景音乐超赞，在线多人联机 description: steam好评如潮 status: 进行中 language: 可用中文mod - title: 传送门2（Portal 2） cover: http://ww2.sinaimg.cn/large/af9df239gw1f5dmc8dce8j20dw0jjabi.jpg link: http://www.gamersky.com/z/portal2/ publisher: Valve Software language: 支持中文 platform: steam type: 第一人称视角，解迷，冒险，科幻,在线双人合作 description: steam好评如潮 status: 进行中--- ​ 效果 添加导航栏icon图标hexo theme大多数都是用的font-awesome字体图标库，在主题下的config文件，导航栏配置相应的icon即可。 1234567891011121314151617menu: home: / archives: /archives about: /resume Books: /books #This is your books page Movies: /movies #This is your movies page #Leetcode: /leetcode #tools: /toolsmenu_icon: home: home archives: archive about: user Books: book Movies: play-circle #Leetcode: file-text #tools: user 添加打赏 挂件中的打赏，直接替换支付宝图片的url即可 文章内的打赏，我用的是github上别人的代码 在主题的配置文件中设置donate为true 功能实现代码在\themes\lightum\layout\_partial\post\donate.ejs 在\themes\lightum\layout\_partial\article.ejs中引用上述文件 百度分享 去百度分享获取分享代码 功能实现 \themes\lightum\layout\_partial\post\share.ejs 在article的样式中引用 \themes\lightum\layout\_partial\article.ejs 在主题config中添加一个开关 share: true 百度统计和谷歌统计 还有百度收录和谷歌收录 获取百度和谷歌统计的代码，将其中的id或content写入主题配置 1234567google_analytics: UA-107525911-1# &lt;meta name="google-site-verification" content="hLu-CnRbqpthWxa76MJ-vpnGr7yMChNtTW6KA0pRMQo" /&gt;google_site_verification: hLu-CnRbqpthWxa76MJ-vpnGr7yMChNtTW6KA0pRMQo# &lt;meta name="baidu-site-verification" content="SOcrumVYOq" /&gt;# hm.src = "https://hm.baidu.com/hm.js?02f792017724a2c2af494ece7edc5fd1";baidu_analytics: 02f792017724a2c2af494ece7edc5fd1baidu_site_verification: WTddywKheI 安装sitemap和baidu-sitemap插件 站点配置文件中生成sitemap的选项 12345# 自动生成sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 在谷歌或百度中设置自动爬取sitemap或手动提交 百度有个自动提交push脚本，放在每个页面的head中的。 ​ 本地搜索 参考教程1 参考教程2 需要安装插件本地搜索的插件 在站点配置文件中设置 12345search: path: search.xml field: post format: html limit: 10000 微博秀微博秀的API接口，获取代码后贴到微博挂件的ejs文件里即可。 评论系统 disqus国内被q，多说倒闭，其他的不知道还能挺多久。我用的是来必力，支持qq、微博、微信登录后评论 在主题配置文件中添加livere_uid: MTAyMC8yOTQxxxxxxooooo id去官网注册获取 相关代码在article.ejs中引用comment.ejs，更换其他评论系统也不难 豆瓣读书和电影 使用了hexo-douban插件 npm install hexo-douban 安装插件，然后站点配置文件，填入豆瓣ID 123456789# hexo-doubandouban: user: shuaiyy book: title: 我的阅读 quote: 2017年，我使用&lt;a href="https://www.douban.com/people/shuaiyy/" target="_blank"&gt;豆瓣&lt;/a&gt;记录我的阅读。 movie: title: 我的电影 quote: 2017年，我使用&lt;a href="https://www.douban.com/people/shuaiyy/" target="_blank"&gt;豆瓣&lt;/a&gt;记录我看的电影。 ​ 详细参考插件地址 写文章markdown神器 Typora不多BB，本文就是这么写的。 插入网易云音乐歌曲点击生成外链播放器，获取分享的html代码，直接黏贴，比如下面的 flash的代码 iframe的代码 部署和发布 部署到github和coding上，域名解析国内解析到coding，国外解析到github 使用ssh git部署 部署 创建代码仓库并配置ssh公钥 在站点config中添加 12345deploy: type: git repo: coding: git@git.coding.net:shuaiyy/shuaiyy.git,master github: git@github.com:shuaiyy/shuaiyy.github.io.git,master ​ 在git bash中使用ssh-add 添加ssh私钥 这样主机就能和github或coding ssh上传数据了 执行命令hexo d 发布绑定域名 首先去github，告诉github xxx.com对应的是哪个blog。 然后去域名解析商那里，告诉任何访问xxx.com的人，xxx.com是github的服务器提供的。 github github的CNAME文件指定你的域名xxx.com，然后域名解析商那里的CNAME记录国外线路指定’xxx.github.io’,即github page给你的二级域名 ​ coding coding是由pages.coding.me服务器为你做的地址跳转，因此国内DNS解析的CNAME记录指向pages.coding.me，而不是coding为你博客提供的二级域名。 github使用网站根目录的CNAME文件绑定域名，而coding需要在项目的page服务选项里设置绑定域名。 示例 github：source目录下的CNAME文件下，写入你的域名。CNAME文件名必须大写 coding：可以最多绑定5个，github由于是CNAME文件，只能绑定一个 dns解析：我的是dnspod解析商 404页面你的404.html的Front-matter中要设置layout: false title: 404。 然后把他丢到source根目录下即可。 1234567---layout: falsetitle: "404"---&lt;html&gt;你访问的页面找不到了！&lt;/html&gt; 网站的icon图标将favicon.ico文件丢入source目录即可 待改进的地方代码高亮的样式这个不会改，如果可以，我想换成night的代码高亮主题 重写标签云 现在的标签云挂件是一排的，占的地方多且不美观，有空改成一行好几个tag的那种。 点击回到顶部按钮]]></content>
      <categories>
        <category>折腾</category>
        <category>博客</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-Login 登录插件]]></title>
    <url>%2F2017%2F09%2F23%2F%E6%8A%80%E6%9C%AF%2FFlask-Login%2F</url>
    <content type="text"><![CDATA[Flask-Login 在Flask app中注册插件1234567# auth/__init__.pyfrom flask_login import LoginManagerlogin_manager = LoginManager()login_manager.session_protection = 'strong'login_manager.login_view = 'auth.login'login_manager.init_app(app) 扩展User Model 需要继承flask-login中的UserMixin，python支持多重继承 AnonymousUserMixin类是匿名用户 @login_manager.user_loader装饰的方法用于实现用户查找 1234567891011121314151617181920212223242526272829from . import db, login_managerfrom flask_login import UserMixin, AnonymousUserMixinclass User(UserMixin, db.Model): __tablename__ = 'users' id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String) email = db.Column(db.String) password = db.Column(db.String) role_id = db.Column(db.Integer, db.ForeignKey('roles.id')) posts = db.relationship('Post', backref='author') comments = db.relationship('Comment', backref='author') locale = db.Column(db.String, default='zh')class AnonymousUser(AnonymousUserMixin): @property def locale(self): return 'zh' def is_administrator(self): return False login_manager.anonymous_user = AnonymousUser@login_manager.user_loaderdef load_user(user_id): return User.query.get(int(user_id)) Form表单1234567891011121314151617181920from flask.ext.wtf import Formfrom wtforms import StringField, PasswordField, SubmitFieldfrom wtforms.validators import DataRequired, EqualTo, Email, Regexp, Lengthclass RegistrationForm(Form): email = StringField(u'邮箱地址', validators=[DataRequired(), Length(1, 64), Email()]) username = StringField(u'用户名', validators=[DataRequired(), Length(1, 64), Regexp('^[A-Za-z][A-Za-z0-9_.]*$', 0, u'用户名必须由字母数、字数、下划线或 . 组成')]) password = PasswordField(u'密码', validators=[DataRequired(), EqualTo('password2', message=u'密码必须一至')]) password2 = PasswordField(u'确认密码', validators=[DataRequired()]) submit = SubmitField(u'马上注册') 视图 需要登录才能访问的视图使用装饰器login_required 123456789101112131415161718192021222324252627282930313233343536373839404142434445# coding=utf-8from flask import render_template, request, flash, redirect, url_forfrom . import authfrom .forms import LoginForm, RegistrationFormfrom ..models import Userfrom .. import dbfrom flask_login import login_user, logout_user, current_user, login_required@auth.route('/login', methods=['GET', 'POST'])def login(): form = LoginForm() if form.validate_on_submit(): user = User.query.filter_by(name=form.username.data, password=form.password.data).first() if user : login_user(user) return redirect(url_for('main.index')) return render_template('login.html', title=u'登录', form=form)@auth.route('/logout')def logout(): logout_user() return redirect(url_for('auth.login'))@auth.route('/register', methods=['GET', 'POST'])def register(): form = RegistrationForm() if form.validate_on_submit(): user = User(email=form.email.data, name=form.username.data, password=form.password.data) db.session.add(user) db.session.commit() return redirect(url_for('auth.login')) return render_template('register.html', title=u'注册', form=form) 前端模板 判断用户是否登录 current_user.is_authenticated 1234567891011&#123;% raw %&#125;&lt;ul class="nav navbar-nav navbar-right"&gt; &#123;% if current_user.is_authenticated() %&#125; &lt;li&gt;&lt;a href="#"&gt;&#123;&#123; current_user.name &#125;&#125;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="&#123;&#123; url_for('auth.logout') &#125;&#125;"&gt;&#123;&#123; _("退出") &#125;&#125;&lt;/a&gt;&lt;/li&gt; &#123;% else %&#125; &lt;li&gt;&lt;a href="&#123;&#123; url_for('auth.login') &#125;&#125;"&gt;&#123;&#123; _("登录") &#125;&#125;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="&#123;&#123; url_for('auth.register') &#125;&#125;"&gt;&#123;&#123; _("注册") &#125;&#125;&lt;/a&gt;&lt;/li&gt; &#123;% endif %&#125; &lt;/ul&gt;&#123;% endraw %&#125;]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 表单]]></title>
    <url>%2F2017%2F09%2F22%2F%E6%8A%80%E6%9C%AF%2FFlask%E8%A1%A8%E5%8D%95%2F</url>
    <content type="text"><![CDATA[表单元素 什么是表单 表单标签 表单域 表单域的种类 表单按钮 提交按钮 复位按钮 一般按钮 表单提交方式有2种方式，get和post，在form的method属性中声明 GET和POST区别 GET适用场合 POST适用场合 一个示例 一个包含多种元素的表单，可以提交数据到后台，JavaScript可以获取元素的值。 html 1234567891011121314151617181920212223242526&lt;html&gt;&lt;head&gt; &lt;script type="text/javascript" src="checkValue.js"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;form name="form1"&gt; &lt;!-- 单行文本框，密码框不显示明文 --&gt; &lt;input type="text" placeholder="Text" name="text1" /&gt; &lt;input type="password" placeholder="password" name="password" /&gt; &lt;!-- 多行文本框，默认可以调节大小 --&gt; &lt;textarea placeholder="Textarea" name="textarea" style="resize:none"&gt; &lt;/textarea&gt; &lt;!-- 文件上传 --&gt; &lt;input type="file" name="file" /&gt; &lt;!-- 单选框 --&gt; &lt;input type="radio" name="Option" value="Option1" /&gt;选项 1 &lt;input type="radio" name="Option" value="Option2" /&gt;选项 2 &lt;!-- 复选框 --&gt; &lt;input type="checkbox" name="check" value="Option1" /&gt;选项 1 &lt;input type="checkbox" name="check" value="Option2" /&gt;选项 2&lt;!-- 提交、重置、普通按钮， 普通按钮需要绑定onclick方法 --&gt; &lt;input type="submit" value="Submit" /&gt; &lt;input type="reset" value="Reset"/&gt; &lt;input type="button" value="button" onclick="getValue()"/&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; JavaScript 12345678910function getValue()&#123; /* 通过tag的name定位元素，value属性取值*/ /* 当选择单选框时，只有页面中被选中的框才会被选中 */ var value = document.form1.Option.value; /* 复选框通过name取到的是obkect数组，是被选中的项 */ var arr=document.form1.check; alert(arr[0].value);&#125; wtforms 表单扩展 当你必须处理浏览器提交的表单数据时，视图代码很快会变得难以阅读。有一些库可以 简化这个工作，其中之一便是 WTForms ，下面我们将介绍这个库。如果你必须处理 许多表单，那么应当尝试使用这个库。 wtforms可以实现表单验证的组件，使用pip安装。Flask-WTF是二者的简单整合。 定义一个form对象 下面是一个常见的注册用的表单对象，包含用户名，密码，确认密码，接受协议等field。 1234567891011from wtforms import Form, BooleanField, TextField, PasswordField, validatorsclass RegistrationForm(Form): username = TextField('Username', [validators.Length(min=4, max=25)]) email = TextField('Email Address', [validators.Length(min=6, max=35)]) password = PasswordField('New Password', [ validators.Required(), validators.EqualTo('confirm', message='Passwords must match') ]) confirm = PasswordField('Repeat Password') accept_tos = BooleanField('I accept the TOS', [validators.Required()]) ​ 在视图中实例化form 调用 validate() 函数来验证数据 通过 form..data 可以访问表单中单个值 123456789@app.route('/register', methods=['GET', 'POST'])def register(): # 实例化form对象，如果是通过 GET 方法提交的，则相应的是 request.args form = RegistrationForm(request.form) if request.method == 'POST' and form.validate(): # 注册用户 flash('Thanks for registering') return redirect(url_for('login')) return render_template('register.html', form=form) ​ 模板中的form wtform已经帮我们做了很多表单生成的工作 视图函数向模板传递form对象 使用form.生成对应的表单域 12345678910111213141516171819&#123;% raw %&#125;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt; &lt;div align="center"&gt; &lt;h1&gt;User Management&lt;/h1&gt; &#123;% if message %&#125; &#123;&#123;message&#125;&#125; &#123;% endif %&#125; &lt;form method="post"&gt; Username :&#123;&#123;form.username&#125;&#125; &lt;br/&gt; Password :&#123;&#123;form.password&#125;&#125; &lt;br/&gt; &lt;input type="submit" value="Submit" /&gt; &lt;input type="reset" value="reset" /&gt; &lt;/form&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&#123;% endraw %&#125; ​ 使用Flask-Wtf表单安全提交，需要配置csrf secret key 使用flash进行错误提示 定义form对象123456789101112131415161718192021222324252627from flask_wtf import Formfrom wtforms import StringField, PasswordField, DateTimeField, SubmitFieldfrom wtforms.validators import DataRequired, Length, Email, EqualTo, Regexpclass LoginForm(Form): name = StringField(label=u'用户名', validators=[DataRequired(), Length(min=4, max=20)]) password = PasswordField(label=u'密码', validators=[DataRequired(), Length(min=4, max=20)]) submit = SubmitField(label=u'提交') class RegistrationForm(Form): email = StringField(u'邮箱地址', validators=[DataRequired(), Length(1, 64), Email()]) username = StringField(u'用户名', validators=[DataRequired(), Length(1, 64), Regexp('^[A-Za-z][A-Za-z0-9_.]*$', 0, u'用户名必须由字母数、字数、下划线或 . 组成')]) password = PasswordField(u'密码', validators=[DataRequired(), EqualTo('password2', message=u'密码必须一至')]) password2 = PasswordField(u'确认密码', validators=[DataRequired()]) submit = SubmitField(u'马上注册') 在模板中引用12345678910&#123;% raw %&#125;&lt;form action="/add" method="post"&gt; &#123;&#123; form.hidden_tag() &#125;&#125; &#123;&#123; form.name.label &#125;&#125; &#123;&#123; form.name(class='text') &#125;&#125; &#123;&#123; form.password.label &#125;&#125; &#123;&#123; form.password() &#125;&#125; &#123;&#123; form.submit() &#125;&#125; &lt;/form&gt;&#123;% endraw %&#125; Field type Validator 使用Bootstrap wtf宏渲染表单12345&#123;% raw %&#125;&lt;form action="/login" method="post"&gt; &#123;&#123; wtf.quick_form(form) &#125;&#125; &lt;/form&gt;&#123;% endraw %&#125;]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 开发环境搭建]]></title>
    <url>%2F2017%2F09%2F20%2F%E6%8A%80%E6%9C%AF%2Fflask%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Flask 开发环境搭建安装python pip 和虚拟环境virtualenv 虚拟机 docker 开发工具vim 插件 vundle vim包管理工具]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask模块化 蓝图]]></title>
    <url>%2F2017%2F09%2F19%2F%E6%8A%80%E6%9C%AF%2FFlask%E8%93%9D%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[Flask模块化 – 蓝图概念简介 在Django项目中，一个网站可以按模块划分，分别实现几个子APP，然后在settings里注册APP，并在urls里为不同的APP分配不同的url路由。 Flask也有相似的设计模式，被称为蓝图 新建一个py文件，存放一个模块的全部视图函数 ​ 1234567891011# user.pyfrom models import Userfrom flask import Blueprintuser = Blueprint('user', __name__)@user.route('/&lt;r_id&gt;')def get_user_by_id(r_id): u = User.objects(user_id=r_id).first() print u return "&#123;&#125;---&#123;&#125;".format(u.user_name, u.user_id) if u else "Not Exist!" model文件app = Flask(__name__)，可以避免循环引用 123456789101112from flask_mongoengine import MongoEnginefrom flask import Flaskapp = Flask(__name__)app.config['MONGODB_SETTINGS'] = &#123;'db': 'test'&#125;db = MongoEngine(app)class User(db.Document): user_id = db.StringField() user_name = db.StringField() def __str__(self): return "id: &#123;&#125;---name:&#123;&#125;".format(self.user_id, self.user_name) ​ 在主文件中注册蓝图，并指定路由 12from user import userapp.register_blueprint(user, url_prefix=&apos;/user&apos;) 然后访问url localhost:port/user/1 项目模块划分 蓝图的概念类似django中的app，可以将一个项目按功能拆分组织 蓝图是Flask() app对象的一个子集 将相同逻辑的功能放在同一个模块里，并且可以为其分配路由 比如登录注册找回密码等功能放到auth模块，url为/auth/xxxx 假设一个Flask APP项目有登录模块和主模块。 项目结构 在模块中使用蓝图 定义一个蓝图 在 auth/__init__.py中定义,并导入本模块中其他的py文件，避免在其他地方导入产生循环引用 12345from flask import Blueprintauth = Blueprint('auth', __name__)import forms, views 在视图中使用蓝图 auth同app对象一样有route等装饰器 反向路由时可以使用auth.login auth/views.py 1234567891011121314151617181920212223# coding=utf-8from flask import render_template, request, flash, redirect, url_forfrom . import authfrom .forms import LoginForm, RegistrationFormfrom ..models import Userfrom .. import dbfrom flask_login import login_user, logout_user, current_user@auth.route('/login', methods=['GET', 'POST'])def login(): return render_template('login.html', title=u'登录', form=form)@auth.route('/logout')def logout(): return redirect(url_for('auth.login'))@auth.route('/register', methods=['GET', 'POST'])def register(): return 'ok' ​ 蓝图注册 在app初始化方法或工厂方法中注册蓝图 url_prefix指定模块的URL路由域 static_folder指定模块自己的static文件夹路径 123456# /app/__init__.py from auth import auth as auth_blueprint from main import main as main_blueprint app.register_blueprint(auth_blueprint, url_prefix='/auth') app.register_blueprint(main_blueprint, static_folder='static')]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016年玩过的游戏]]></title>
    <url>%2F2017%2F09%2F17%2F%E7%94%9F%E6%B4%BB-%E7%94%B5%E5%BD%B1%E9%9F%B3%E4%B9%90%E6%97%85%E8%A1%8C%E6%91%84%E5%BD%B1%2F2016-06-01-Game-of-2016%2F</url>
    <content type="text"></content>
      <categories>
        <category>生活</category>
        <category>游戏</category>
      </categories>
      <tags>
        <tag>Enjoy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask实战 -- Todo List]]></title>
    <url>%2F2017%2F09%2F17%2F%E6%8A%80%E6%9C%AF%2FFlask%E5%AE%9E%E6%88%98%20--%20Todo%20List%2F</url>
    <content type="text"><![CDATA[Flask实战 – Todo ListFlask应用设计功能介绍 开发流程 项目开发流程 后端开发流程 架构设计 开发技术 前端：Bootstrap 后端：Flask 数据库：MongoDB 插件扩展 Flask-MongoEngine Flask-Script Flask-WTF 项目结构 数据模型 mongo orm 对象 model_form 通过类自动生成对应的表单 12345678910111213from app import dbimport datetimefrom flask_mongoengine.wtf import model_formclass Todo(db.Document): content = db.StringField(required=True, max_length=30) time = db.DateTimeField(default=datetime.datetime.now) status = db.IntField(default=0) def __str__(self): return "content:&#123;&#125; time:&#123;&#125; status:&#123;&#125;".format(self.content, self.time, self.status)TodoFrom = model_form(Todo) 前端模板 pycharm识别指定模板语法 ctrl + alt + s 搜索template，在Python template下选择jinja2 pycharm模板文件自动引用 ctrl + alt + s 搜索 project， 在project structure 里面的选项卡mark template director。 或右键templates文件夹，mark as ，template directory 基类模板 静态文件引用，反向url 1234567891011121314151617181920212223242526272829&#123;% raw %&#125;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&lt;/title&gt; &lt;link href="&#123;&#123; url_for('static',filename='bootstrap.min.css') &#125;&#125;" rel="stylesheet" type="text/css"/&gt; &lt;link href="&#123;&#123; url_for('static',filename='index.css') &#125;&#125;" rel="stylesheet" type="text/css"/&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="container"&gt; &lt;div class="header clearfix"&gt; &lt;h3 class="text-muted"&gt;Todo&lt;/h3&gt; &lt;/div&gt; &lt;div class="jumbotron"&gt; &#123;% block content %&#125; &#123;% endblock %&#125; &lt;/div&gt; &lt;footer class="footer"&gt; &lt;p&gt;&amp;copy; xxx.com 2017&lt;/p&gt; &lt;/footer&gt; &lt;/div&gt; &lt;!-- /container --&gt;&lt;/body&gt;&lt;/html&gt;&#123;% endraw %&#125; index页面 表单的csrf_token 表单的error信息，form.errors.content datetime时间对象格式化输出 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&#123;% raw %&#125;&#123;% extends "base.html" %&#125;&#123;% block content %&#125; &lt;form class="input-group" action="/add" method="post"&gt; &#123;&#123; form.hidden_tag() &#125;&#125; &#123;&#123; form.content(class="form-control") &#125;&#125; &lt;span class="input-group-btn"&gt; &lt;button class="btn btn-default" type="submit"&gt;Add&lt;/button&gt; &lt;/span&gt; &lt;/form&gt; &#123;% for error in form.errors.content %&#125; &lt;div&gt;&#123;&#123; error &#125;&#125;&lt;/div&gt; &#123;% endfor %&#125; &lt;div&gt; &lt;h2&gt;Todo List&lt;/h2&gt; &#123;% if todos %&#125; &lt;table class="table"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;content&lt;/th&gt; &lt;th&gt;status&lt;/th&gt; &lt;th&gt;time&lt;/th&gt; &lt;th&gt;operate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &#123;% for todo in todos %&#125; &lt;tr&gt; &lt;td&gt;&#123;&#123; todo.content &#125;&#125;&lt;/td&gt; &lt;td&gt; &#123;% if todo.status == 1 %&#125; 已完成 &#123;% else %&#125; 未完成 &#123;% endif %&#125; &lt;/td&gt; &lt;td&gt;&#123;&#123; todo.time.strftime('%H:%M %d-%m-%Y') &#125;&#125;&lt;/td&gt; &#123;% if todo.status == 1 %&#125; &lt;td&gt;&lt;a href="/undone/&#123;&#123; todo.id &#125;&#125;" class="btn btn-primary"&gt;Undone&lt;/a&gt;&lt;/td&gt; &#123;% else %&#125; &lt;td&gt;&lt;a href="/done/&#123;&#123; todo.id &#125;&#125;" class="btn btn-primary"&gt;Done&lt;/a&gt;&lt;/td&gt; &#123;% endif %&#125; &lt;td&gt;&lt;a href="/delete/&#123;&#123; todo.id &#125;&#125;" class="btn btn-danger"&gt;Delete&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/tbody&gt; &lt;/table&gt; &#123;% else %&#125; &lt;h3 class="text-info"&gt;No todos,please add&lt;/h3&gt; &#123;% endif %&#125; &lt;/div&gt;&#123;% endblock %&#125;&#123;% endraw %&#125; 404页面 123456&#123;% raw %&#125; &#123;% extends "base.html" %&#125; &#123;% block content %&#125; &lt;h2 class="label-warning"&gt;Not Found&lt;/h2&gt; &#123;% endblock %&#125;&#123;% endraw %&#125; app配置和初始化 配置文件config.py 123SECRET_KEY = "asdadawefda"MONGODB_SETTINGS = &#123;'DB': 'todo'&#125;WTF_CSRF_ENABLED = False # 单元测试post表单时，时取消csrf验证， ​ 在app的__init__.py里定义app对象和创建数据库连接，使用配置文件 12345678from flask import Flaskfrom flask.ext.mongoengine import MongoEngineapp = Flask(__name__)app.config.from_object('config')db = MongoEngine(app) from app import views,models 或者使用一个工厂方法注册所有APP的组件，避免全局变量 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# __init__.pyfrom os import pathfrom flask_bootstrap import Bootstrapfrom flask_sqlalchemy import SQLAlchemyfrom flask_login import LoginManager, current_userfrom flask_pagedown import PageDownfrom flask_gravatar import Gravatarfrom flask_babel import Babel, gettext as _from config import configbasedir = path.abspath(path.dirname(__file__))db = SQLAlchemy()babel = Babel()bootstrap = Bootstrap()pagedown = PageDown()login_manager = LoginManager()login_manager.session_protection = 'strong'login_manager.login_view = 'auth.login'def create_app(config_name='default'): app = Flask(__name__) app.config.from_object(config[config_name]) db.init_app(app) bootstrap.init_app(app) login_manager.init_app(app) pagedown.init_app(app) babel.init_app(app) Gravatar(app, size=64) from auth import auth as auth_blueprint from main import main as main_blueprint app.register_blueprint(auth_blueprint, url_prefix='/auth') app.register_blueprint(main_blueprint, static_folder='static') @app.template_test('current_link') def is_current_link(link): return link == request.path @babel.localeselector def get_locale(): return current_user.locale return app ​ 实现查询、保存、更新、删除功能 视图views.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546from app import appfrom flask import render_template,requestfrom models import Todo, TodoFormfrom datetime import datetime# 查询@app.route('/')def index(): form = TodoForm() todos = Todo.objects.order_by('-time') return render_template("index.html",todos=todos,form=form)# 增加@app.route('/add', methods=['POST',])def add(): form = TodoForm(request.form) if form.validate(): content = form.content.data todo = Todo(content=content,time=datetime.now()) todo.save() todos = Todo.objects.order_by('-time') return render_template("index.html",todos=todos,form=form) # 修改@app.route('/done/&lt;string:todo_id&gt;')def done(todo_id): form = TodoForm() todo = Todo.objects.get_or_404(id=todo_id) todo.update(status=1) todos = Todo.objects.order_by('-time') return render_template("index.html",todos=todos,form=form) # 删除@app.route('/delete/&lt;string:todo_id&gt;')def delete(todo_id): form = TodoForm() todo = Todo.objects.get_or_404(id=todo_id) todo.delete() todos = Todo.objects.order_by('-time') return render_template("index.html",todos=todos,form=form)# 404页面@app.errorhandler(404)def not_found(error): return render_template('404.html') 改进用户体验 时间降序排列list todos = Todo.objects.order_by(&#39;-time&#39;) 格式化时间 在模板中todo.time.strftime(&#39;%H:%M %d-%m-%Y&#39;) 404页面 在视图中使用装饰器@app.errorhandler(404) 应用测试单元测试 编写测试用例123456789101112131415161718192021222324import unittestfrom app import appfrom app.models import Todoclass TodoTestCase(unittest.TestCase): # 开始测试前执行 def setUp(self): self.app = app.test_client() # 测试结束后执行 def tearDown(self): todos = Todo.objects.all() for todo in todos: todo.delete() def test_index(self): rv = self.app.get('/') assert "Todo" in rv.data def test_todo(self): self.app.post('/add', data = dict(content="testtodo")) todo = Todo.objects.get_or_404(content="testtodo") assert todo is not None 开始测试前setUp方法先执行 测试结束后tearDown方法执行 测试方法以test_开头命名 测试POST时关闭csrf验证 执行测试用例 python -m unittest discover 测试覆盖率 安装coverage pip install coverage 使用命令coverage report windows下可以右键测试文件，选择run with coverage]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask数据库的修改和迁移]]></title>
    <url>%2F2017%2F09%2F15%2F%E6%8A%80%E6%9C%AF%2FFlask%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%BF%AE%E6%94%B9%E5%92%8C%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[Flask数据库的修改和迁移 类似django中的manage migrate命令，flask也有类似的插件 使用flask插件pip install flask-migrate 在manage中绑定数据库命令即可 1234567891011121314151617# manage.pyfrom model import dbfrom model import appfrom flask.ext.script import Managerfrom flask.ext.migrate import Migrate, MigrateCommand, upgrademigrate = Migrate(app,db)manager = Manager(app)manager.add_command('db',MigrateCommand)@manage.commanddef deploy(): upgrade() app.run()if __name__ == '__main__': manager.run() ​ 初始化数据库 python manage.py db init 检测数据库变化 python manage.py db migrate 更新数据库表 python manage.py db upgrade]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitbook生成PDF电子书]]></title>
    <url>%2F2017%2F09%2F14%2F%E6%8A%80%E6%9C%AF%2FGitbook%E7%94%9F%E6%88%90PDF%E7%94%B5%E5%AD%90%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[Gitbook生成PDF电子书 看到几个不错的开源书籍，发布在gitbook上，然而gitbook很卡，网页体验不好。。。 哇塞，在gitbook官网找书，部分有下载按钮，应该是作者编译提供的。 好吧，这次就不自己折腾了。 problem-solving-with-algorithms-and-data-structure-using-python 中文版 编程之法：面试和算法心得 程序员的自我修养 LeetCode题解 参考教程 安装nodejs 安装gitbook-cli 安装gitbook 下载phantomjs 下载安装calibre2 配置环境变量]]></content>
      <categories>
        <category>折腾</category>
        <category>电子书</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>gitbook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask Markdown支持]]></title>
    <url>%2F2017%2F09%2F14%2F%E6%8A%80%E6%9C%AF%2FMarkdown%20in%20Flask%2F</url>
    <content type="text"><![CDATA[Markdown in Flaskpip安装markdown，flask-pagedown,可以在编辑内容时实时预览markdown 需要在form对象中设置相应的field 需要在前端页面引入pagedown 的js pagedown Field对应的开关参数only_input或only_preview 在form中设置Field123456789101112131415# forms.pyfrom flask.ext.pagedown.fields import PageDownFieldfrom flask.ext.wtf import Formfrom wtforms import StringField, SubmitFieldfrom wtforms.validators import DataRequiredclass PostForm(Form): title = StringField(label=_(u"标题"), validators=[DataRequired()]) body = PageDownField(label=_(u"正文"), validators=[DataRequired()]) submit = SubmitField(_(u"发表"))class CommentForm(Form): body = PageDownField(label=_(u'评论'), validators=[DataRequired()]) submit = SubmitField(_(u'发表')) 在模板中使用123456789101112131415161718192021222324252627282930&#123;% raw %&#125;&#123;% extends 'base.html' %&#125;&#123;% import "bootstrap/wtf.html" as wtf %&#125;&#123;% block scripts %&#125; &#123;&#123; super() &#125;&#125; &#123;&#123; pagedown.include_pagedown() &#125;&#125;&#123;% endblock %&#125;&#123;% block page_body %&#125; &lt;div class="container"&gt; &lt;form method="POST"&gt; &#123;&#123; form.hidden_tag() &#125;&#125; &lt;div class="form-group"&gt; &#123;&#123; form.title(class="form-control", placeholder=_("请输入文章标题") ) &#125;&#125; &lt;/div&gt; &lt;div class="form-group"&gt; &#123;&#123; form.body(only_input=True,rows=10,class="form-control") &#125;&#125; &lt;/div&gt; &lt;div&gt; &#123;&#123; form.body(only_preview=True) &#125;&#125; &lt;/div&gt; &lt;div class="form-group"&gt; &#123;&#123; form.submit(class="btn btn-primary") &#125;&#125; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt;&#123;% endblock %&#125;&#123;% endraw %&#125;]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-Gravatar用户头像]]></title>
    <url>%2F2017%2F09%2F13%2F%E6%8A%80%E6%9C%AF%2FFlask-Gravatar%2F</url>
    <content type="text"><![CDATA[Flask-Gravatar 关于Gravatar： 我们在很多博客或者网站留言，评论的时候会看到有的人头像很酷很个性化，但是这个博客和网站本身并没有提供设置头像的功能，感觉有点神奇，那么是怎么做到的呢？其实这是使用了Gravatar。 Gravatar的概念首先是在国外的独立WordPress博客中兴起的，当你到任何一个支持Gravatar的网站留言时，这个网站都会根据你所提供的Email地址为你显示出匹配的头像。当然，这个头像，是需要你事先到Gravatar的网站注册并上传的，否则，在这个网站上，就只会显示成一个默认的头像。 flask-gravatar扩展插件，官方文档here pip install Flask-Gravatar 初始化12345678910# app/__init__.py 或者flask app的工厂方法from flask_gravatar import Gravatargravatar = Gravatar(app, size=100, rating='g', default='retro', force_default=False, force_lower=False, use_ssl=False, base_url=None) 在模板中使用12345&#123;% raw %&#125; &lt;img class="media-object img-circle" src="&#123;&#123; comment.author.email | gravatar &#125;&#125;"&gt;&lt;!-- 可以加入参数 --&gt; &#123;&#123; 'zzz.sochi@gmail.com' | gravatar(size=200, rating='x') &#125;&#125;&#123;% endraw %&#125;]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 国际化语言支持]]></title>
    <url>%2F2017%2F09%2F12%2F%E6%8A%80%E6%9C%AF%2FFlask%20%E5%9B%BD%E9%99%85%E5%8C%96%E8%AF%AD%E8%A8%80%E6%94%AF%E6%8C%81%2F</url>
    <content type="text"><![CDATA[Flask 国际化语言支持 flask-babel扩展]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 分页]]></title>
    <url>%2F2017%2F09%2F11%2F%E6%8A%80%E6%9C%AF%2Fflask%20%E5%88%86%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[flask 分页数据伪装 使用 forgery_py, 安装 pip install forgerypy 生成大量符合条件的随机数据,用于开发和测试，此处是用于分页的数据 生成大量随机的符合条件的数据 123456789101112from forgery_py import basic, lorem_ipsum, name, internet, datefrom random import randintPost(title=lorem_ipsum.title(), body=lorem_ipsum.paragraphs(), created=date.date(), author=func_author())User(name=internet.user_name(), email=internet.email_address(), password=basic.text(6, at_least=6, spaces=False), role=guests) 分页的视图逻辑 要有page_index参数，即当前是第几页 12345678910111213@main.route('/')def index(): # posts=Post.query.all() page_index = request.args.get('page', 1, type=int) query = Post.query.order_by(Post.created.desc()) pagination = query.paginate(page_index, per_page=20, error_out=False) posts = pagination.items return render_template('index.html', title=_(u'欢迎来到Ray的博客'), posts=posts, pagination=pagination) 模板中使用分页 使用Bootstrap提供的pagination 默认的页面参数是?page=n 123456789101112131415161718192021222324&#123;% raw %&#125;&#123;% extends 'base.html' %&#125;&#123;% from "bootstrap/pagination.html" import render_pagination %&#125;&#123;% block page_body %&#125; &lt;div class="container"&gt; &lt;a href="&#123;&#123; url_for('main.edit') &#125;&#125;" class="btn btn-primary"&gt;发表新文章&lt;/a&gt; &lt;/div&gt; &lt;div class="container"&gt; &lt;!-- 展示items --&gt; &#123;% for post in posts %&#125; &lt;h2&gt;&lt;a href="&#123;&#123; url_for('main.post', id = post.id) &#125;&#125;"&gt;&#123;&#123; post.title &#125;&#125;&lt;/a&gt;&lt;/h2&gt; &lt;div&gt; &#123;&#123; post.body_html|safe &#125;&#125; &lt;/div&gt; &#123;% endfor %&#125; &lt;!-- 分页栏 --&gt; &#123;% if pagination %&#125; &#123;&#123; render_pagination(pagination) &#125;&#125; &#123;% endif %&#125; &lt;/div&gt;&#123;% endblock %&#125;&#123;% endraw%&#125; ​]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 应用测试]]></title>
    <url>%2F2017%2F09%2F10%2F%E6%8A%80%E6%9C%AF%2FFlask%20%E4%BB%A3%E7%A0%81%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[Flask 应用测试测试内容 模型测试 逻辑测试 view测试 unittest 测试文件以test_开头命名 每个测试文件都要实现一个测试用例 测试用例内的方法命名也是test_开头命名 测试用例有setUp和tearDown方法 目录结构– tests —-__init__.py —-test_models.py 为测试单独提供config参数123456789101112131415161718192021222324252627282930313233343536373839404142434445# config.pyimport osbasedir = os.path.abspath(os.path.dirname(__file__))class Config: SECRET_KEY = os.environ.get('SECRET_KEY') or '\x03d\xf4\x95J\x15\xa4B\xfb\xc0\xaf \xd1A[j$&#125;\x18\x16a\xe7\xd0\xec' SSL_DISABLE = False SQLALCHEMY_COMMIT_ON_TEARDOWN = True SQLALCHEMY_RECORD_QUERIES = True BABEL_DEFAULT_LOCALE = 'zh' @staticmethod def init_app(app): passclass DevelopmentConfig(Config): DEBUG = True SQLALCHEMY_DATABASE_URI = os.environ.get('DEV_DATABASE_URL') or \ 'sqlite:///' + os.path.join(basedir, 'data-dev.sqlite')class TestingConfig(Config): TESTING = True SERVER_NAME = 'localhost:5000' SQLALCHEMY_DATABASE_URI = os.environ.get('TEST_DATABASE_URL') or \ 'sqlite:///' + os.path.join(basedir, 'data-test.sqlite') WTF_CSRF_ENABLED = Falseclass Production(Config): DEBUG=True SQLALCHEMY_DATABASE_URI = os.environ.get('DEV_DATABASE_URL') or \ 'postgresql://ray:?@localhost/blog-db'config = &#123; 'development': DevelopmentConfig, 'testing': TestingConfig, 'production': Production, 'default': DevelopmentConfig&#125; 在flask app的工厂方法中使用参数来选择不同的配置 123456# app/__init__.pyfrom config import configdef create_app(config_name='default'): app = Flask(__name__) app.config.from_object(config[config_name]) 测试代码 Flask提供了测试客户端 测试models对象增删查改 开始测试方法前连接数据库，清空，重建表 测试结束后关闭数据库连接 12345678910111213141516171819202122232425262728293031323334# test_models.pyimport unittestfrom app import create_app, dbfrom app.models import User, Rolefrom forgery_py import internet, basicfrom flask import url_forclass ModelTest(unittest.TestCase): def setUp(self): self.app = create_app('testing') self.app_ctx = self.app.app_context() self.app_ctx.push() self.client = self.app.test_client() db.drop_all() db.create_all() def tearDown(self): self.app_ctx.pop() def test_user_role_set(self): user = User(name=internet.user_name(), email=internet.email_address(), password=basic.text()) db.session.add(user) db.session.commit() self.assertEqual(user.role.name, 'Guests') def test_index_page(self): rep = self.client.get(url_for('main.index')) self.assertEqual(rep.status_code, 200) nose nose extends unittest to make testing easier. web ui 测试 安装selenium，chrom driver chrome， Firefox 在views里实现一个关闭flask进程的功能，当测试结束后，访问该url后flask退出 123456789101112from flask import current_app@main.route('/shoutdown')def shutdown(): if not current_app.testing: abort(404) shoutdown = request.environ.get('werkzeug.server.shutdown') if not shoutdown: abort(500) shoutdown() return u'正在关闭服务端进程...' 将关于同一个页面的selenium操作封装到一个对象里，以方便复用 123456789101112131415161718192021222324from selenium import webdriverclass LoginPage(object): client = None def __init__(self, c): self.client = c @property def title(self): return self.client.title def set_user_name(self, name): user_input = self.client.find_element_by_name('username') user_input.send_keys(name) def set_pwd(self, pwd): pwd_input = self.client.find_element_by_name('password') pwd_input.send_keys(pwd) def submit(self): submit = self.client.find_element_by_name('submit') submit.click() 测试登录的脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# test_selenium.py# coding=utf-8import unittestimport threadingfrom selenium import webdriverfrom app import create_app, dbfrom app.models import Role, Userimport refrom forgery_py import internet, basicclass SeleniumTest(unittest.TestCase): client = None app_ctx = None @classmethod def setUpClass(cls): try: cls.client = webdriver.Firefox() except: pass if cls.client: cls.app = create_app('testing') cls.app_ctx = cls.app.app_context() cls.app_ctx.push() db.drop_all() db.create_all() Role.seed() threading.Thread(target=cls.app.run).start() @classmethod def tearDownClass(cls): # 在服务端关闭flask进程 cls.client.get('http://localhost:5000/shutdown') cls.client.close() db.session.remove() cls.app_ctx.pop() def setUp(self): if self.client is None: self.skipTest(u'略过测试') def tearDown(self): pass def test_user_login(self): from login_page import LoginPage new_user = User(name=internet.user_name(), email=internet.email_address(), password=basic.text()) db.session.add(new_user) db.session.commit() page = LoginPage(self.client) self.client.get('http://localhost:5000/auth/login') self.assertTrue(u'登录' in page.title) page.set_user_name(new_user.name) page.set_pwd(new_user.password) page.submit() # 返回注册结果 self.assertTrue(re.search(u'欢迎来到Ray的博客', self.client.page_source))]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
        <tag>测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask Web应用部署和运维]]></title>
    <url>%2F2017%2F09%2F09%2F%E6%8A%80%E6%9C%AF%2FFlask%20Web%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2%E5%92%8C%E8%BF%90%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[Flask Web应用部署和运维web应用发布服务器要求 云服务器 ssh登录到远程服务器 配置web服务器运行环境 上传数据 通过ssh的sftp协议 通过ssh的lrzsz命令 配置web应用 绑定公网ip，host= 0.0.0.0，允许公网访问 配置日志 配置缓存 WSGI WSGI简介 WSGI具有灵活性和扩展性 WSGI应用 WSGI应用是一个接受两个参数的可调用对象 WSGI服务器 为每个HTTP请求调用WSGI应用。 部署方案设计服务器系统的选择Linux服务器，CentOS或Ubuntu 常用的WSGI服务器 Gunicorn uWSGI CheryPy Tornado Gevent(基于协程，资源占用更少？) mod_wsgi(Apache) Web服务器 Nginx lighttpd Apache Nginx是面向性能设计的HTTP服务器，相比Apache，Lighttpd占用内存少，稳定性更高。 部署方案 Nginx web服务器 Gunicorn WSGI服务器 Virtualenv 管理Python运行环境 Supervisor进程监控管理 部署工作 可以选择将flask app设置为linux系统服务 ubuntu注册系统服务的配置为/etc/init/my_flask.conf 启动服务sudo service my_flask start 123456789101112131415# /etc/init/my_flask.confdescription "My flask app service"start on runlevel [2345]stop on runlevel [!2345]respawnsetuid rootsetgid www-dataenv PATH=/usr/share/www/venv/binchdir /usr/share/www/# wsgi.py是flask app的入口执行文件，application是flask app对象实例exec gunicorn -w 4 -b 127.0.0.1:8000 wsgi:application ​ 可以使用supervisor监控flask app进程 部署工具 virtualenv，串讲独立的python运行环境，pip install 解决版本稳题 依赖问题 权限问题 使用方法 创建虚拟环境 virtualenv flask_env 激活 source flask_env/bin/activate 退出 deactivate Supervisor 进程管理工具， apt-get install supervisor 应用进程控制 多应用进程管理 应用中断后快速重启 使用方法 主配置文件位于/etc/supervisor/supervisord.conf,其中有一行参数[include] files = /etc/supervisor/conf.d/*.conf表示我们自定义的进程管理配置文件的应存放位置 添加程序 123# //etc/supervisor/conf.d/app.conf[program:app]command python /home/ubuntu/new/app.py ​ 使用supervisorctl进行控制 reload stop help Flask应用部署Nginx Gunicorn部署flask应用 配置Python虚拟环境 pip install -r requirement.txt 123456# requirement.txtflaskflask-wtfflask-scriptflask_mongoenginegunicorn ​ 安装配置Nginx apt-get安装 配置文件/etc/nginx/nginx.conf，里面有指定错误日志记录位置，一般在var /etc/nginx/sites-available是可用的配置文件，在此处创建配置 nginx监听80端口，处理静态文件,转发所有请求到9000端口，9000端口由Gunicorn监听 /etc/nginx/sites-enabled是生效的配置文件，我们从available里链接文件过来 gunicorn转发gunicorn -b 0.0.0.0:9000 my_app:app 监听9000端口，my_app.py是flask 应用的入口文件，后面接:app 配置supervisor 轻量级运维方案设计与实现DevOps简介 优点 自动化 快速发布 快速恢复 Fabricpip install fabric 用法示例 编写fab脚本 12345# fabfile.pyfrom fabric import *def hello(): print "hello!" 执行fab命令 fab hello 简介通过SSH进行应用部署以及系统任务管理的命令行工具。 功能 本地或远程执行shell命令 上传和下载文件 提示用户输入 中断操作 轻量级运维方案Pycharm中使用Github 配置账户 首先settings，version control配置github账户。电脑本地要安装git 建立远程仓库并提交代码 菜单栏 VCS， import into version control， share on github commit 选中项目文件夹，右键选择git 或者快捷键 ctrl + k push ctrl + shift + k 从github仓库克隆项目、 VCS ，Check out from version control， github。 pycharm会登录你的github并拉去有仓库git地址，选择你要克隆的项目。 设计 实现 上传项目代码到git服务器 在远程服务器上git clone项目代码 部署应用，创建虚拟环境，安装项目依赖，配置数据库及flask参数 mongodb数据库安装，参考官网指导 Nginx配置 1234567891011server &#123; listen 80; location /static &#123; alias /home/ubuntu/new/todo/app/static; &#125; location / &#123; proxy_pass http://127.0.0.1:7000; &#125;&#125; ​ 使用supervisor监控管理应用 1234[program:todo]command = /home/ubuntu/flask_env/bin/gunicorn -b 0.0.0.0:7000 -w 4 run:appdirectory = /home/ubuntu/new/todo ​ 使用fabric实现自动更新部署脚本 登录服务器后，自动运行shell命令，实现项目更新部署 123456789101112from fabric.api import *# linux server的登录信息env.hosts = ['192.168.1.237']env.user = 'ubuntu'env.password = 'ubuntu'def deploy(): with cd('/home/ubuntu/new/todo'): run('git pull') sudo('supervisorctl restart todo') sudo('supervisorctl status') 本地执行 ：fab deploy]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask Bootstrap]]></title>
    <url>%2F2017%2F09%2F08%2F%E6%8A%80%E6%9C%AF%2FFlask%20Bootstrap%2F</url>
    <content type="text"><![CDATA[Flask Bootstrap 安装pip install flask-bootstrap flask-nav生成导航栏 在app中注册Bootstrap12from flask_bootstrap import BootstrapBootstrap(app) Flask-Bootstrap为我们提供了一些模板和宏 在模板中引入 直接在原来的base模板文件中继承bootstrap/base.html 123456789101112131415161718192021222324252627&#123;% raw %&#125;&#123;% extends 'bootstrap/base.html' %&#125;&#123;% block head %&#125; &#123;&#123; super() &#125;&#125;&#123;# &lt;link href="&#123;&#123; url_for('static',filename='bootstrap.min.css') &#125;&#125;" rel="stylesheet" type="text/css"/&gt;#&#125; &lt;link href="&#123;&#123; url_for('static',filename='index.css') &#125;&#125;" rel="stylesheet" type="text/css"/&gt;&#123;% endblock %&#125;&#123;% block body %&#125; &lt;div class="container"&gt; &lt;div class="header clearfix"&gt; &lt;h3 class="text-muted"&gt;Todo&lt;/h3&gt; &lt;/div&gt; &lt;div class="jumbotron"&gt; &#123;% block content %&#125; &#123;% endblock %&#125; &lt;/div&gt; &#123;% block footer %&#125; &lt;footer class="footer"&gt; &lt;p&gt;&amp;copy; yangshuai 2017&lt;/p&gt; &lt;/footer&gt; &#123;% endblock %&#125; &lt;/div&gt; &lt;!-- /container --&gt; &#123;% endblock %&#125;&#123;% endraw %&#125; Flask-Nav生成导航栏 注册Nav对象 注册navbar元素 在模板需要的地方引用 nav.top.render() top是nav对象的id 12345678from flask_nav import Navfrom flask_nav.elements import *nav = Nav(app)nav.register_element('top', Navbar(u'记事本', View(u'主页', 'index'), View(u'关于', 'index'), View(u'项目', 'index'))) Bootstrap CDN选择Bootstrap的样式，引入css文件 www.bootstrapcdn.com 123456789&#123;% raw %&#125;&#123;% block styles %&#125; &#123;&#123; super() &#125;&#125; &lt;link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/darkly/bootstrap.min.css" rel="stylesheet" integrity="sha384-S7YMK1xjUjSpEnF4P8hPUcgjXYLZKK3fQW1j5ObLSl787II9p8RO9XUGehRmKsxd" crossorigin="anonymous"&gt;&#123;% endblock %&#125;&#123;% endraw %&#125; 使用Bootstrap wtf宏渲染表单12345&#123;% raw %&#125;&lt;form action="/login" method="post"&gt; &#123;&#123; wtf.quick_form(form) &#125;&#125; &lt;/form&gt;&#123;% endraw %&#125;]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python Web框架]]></title>
    <url>%2F2017%2F09%2F07%2F%E6%8A%80%E6%9C%AF%2FPython%20Web%E6%A1%86%E6%9E%B6%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Python Web框架简介Web开发框架Web框架简介框架 Web框架 Web框架中的概念 MVC (Model，View，Controller) ORM（Object-Relational Mapping） URL Route Template 常用Web框架种类 Django Flask]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python CGI编程]]></title>
    <url>%2F2017%2F09%2F06%2F%E6%8A%80%E6%9C%AF%2Fpython%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%9A%84CGI%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[python CGI编程简介 CGI介绍cgi允许web服务器执行外部脚本或程序，并将执行结果通过web服务器发送给浏览器。 对一个 CGI 程序，做的工作其实只有：从环境变量(environment variables)和标准输入(standard input)中读取数据、处理数据、向标准输出(standard output)输出数据。 GET和POST 常见的web服务器Python内置的 ApacheNginxCGI hello world12345678#! /usr/bin/env python# cgi-bin/main.pyimport cgi, cgitbform = cgi.FieldStorage()name = form.getvalue('name')print "Content-type:text/html \n\n"print "&lt;h1&gt;Hello %s&lt;/h1&gt;" % name python -m &quot;CGIHTTPServer&quot; 8881,启动服务器。 访问 http://127.0.0.1:8881/cgi-bin/main.py?name=Susan,就会得到“Hello Susan”的输出。]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python web 数据库连接]]></title>
    <url>%2F2017%2F09%2F05%2F%E6%8A%80%E6%9C%AF%2FPython%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[Python数据库连接数据库基础数据库概念 数据库分类SQL和NoSQL SQL SQLite 单文件，小型数据库 MySQL 开源，性能优异，使用广泛 PostgreSQL 开源，性能优异，使用广泛 Oracle Microsoft SQL Server NoSQL MongoDB 性能好 neo4j elasticsearch 索引搜索功能强大 InfluxDB BigTable LevelDB 单机 分布式 单机：Mysql和PostgreSQL常用于单机 分布式：Apache HIVE 和 cloudera IMPALA 文件型和内存性 文件型，数据放在磁盘，索引放在内存 MySQL、MongoDB 内存性：数据存放在内存，多用于缓存系统 Redis(支持复杂数据类型)、Memcached 批处理和交互式 批处理：将SQL分解成MR(map &amp;&amp; reduce) Apache Hive 交互式：分级之后查询汇总 cloudera IMPALA、Apache Hbase、Amazon DynamoDB AWS Amazon web services，亚马逊云服务，提供了免运维的数据库服务，支持MySQL、PostgreSQL、Oracle、SQL Server，注册后有个免费一年的套餐。 安装数据库及管理工具 mysql server 管理工具 Navicat RoboMongo RedisDesktop ​ 数据库查询语言SQL 不同数据库之间的SQL语句不完全相同 增删查改 python进行SQL操作Mysqlpip install mysql-python 建立数据库连接 连接也可以使用URI，mysql:\\user:passs@host:port/db_name 创建游标 通过游标执行SQl语句 执行插入、删除、修改后，需要提交到数据库以保持数据一致性 最后关闭数据库 1234567891011121314151617181920212223242526import MySQLdb# connect to dbconn = MySQLdb.connect("db_host","user","passwd","db_name", charset='utf-8')# create cursorcur = conn.cursor()# sql sql ="select * from student"insert_sql="insert into student (name) values ('%s')" %('xxx')# executecur.execute(insert_sql)conn.commit() # 执行插入、提交# 执行查询，并遍历查询结果# get the resultcur.execute(sql)result=cur.fetchall()for row in result: print row[0] print row[1]# 关闭数据库conn.close() Mongodb 操作 使用pymongo 123456789101112import pymongo# 建立连接client = pymongo.MongoClient(host='127.0.0.1', port='27017')# 选择数据库db = client['test']# 选着collectionuser_collection = db['user']# 向collection插入一条文档user_collection.insert(&#123;'id': '1', 'name': 'Lisa'&#125;)# 文档查找all_user = user_collection.find() Flask中使用数据库 首先创建表 字段：id, user, password, 使用单独的py文件定义全部sql操作。 1234567891011121314151617# db.pyimport MySQLdbconn = MySQLdb.connect("localhost","root","","test")cur = conn.cursor()def insert(username,password): sql = "insert into user (username,password) values ('%s','%s')" %(username,password) cur.execute(sql) conn.commit() conn.close()def isExisted(username,password): sql="select * from user where username ='%s' and password ='%s'" %(username,password) cur.execute(sql) result = cur.fetchall() return True if result else False 视图函数中使用SQL操作 注册用户时，新建用户插入数据库表中 用户登陆时，检查登陆信息是否正确 12345678910111213141516171819from db import *@app.route("/register",methods=['GET','POST'])def register(): myForm=LoginForm(request.form) if request.method=='POST': insert(myForm.username.data,myForm.password.data) return redirect("http://www.jikexueyuan.com") return render_template('index.html',form=myForm) @app.route("/login",methods=['GET','POST'])def login(): myForm=LoginForm(request.form) if request.method =='POST': if (isExisted(myForm.username.data,myForm.password.data)): return redirect("http://www.jikexueyuan.com") else: return "Login Failed" return render_template('index.html',form=myForm) ORM 对象关系映射ORM概念数据库和对象之间的桥梁 SQLAlchemy 著名的Python ORM框架 建立连接和创建表 创建有外键关联的表对象 插入和查询数据 高级查询 与或逻辑，多表查询，多级查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899# coding:utf-8import sqlalchemyfrom sqlalchemy import create_enginefrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import Column, Integer, String, DateTime, TEXTfrom sqlalchemy.orm import sessionmakerfrom datetime import datetime# 创建数据库连接引擎, echo显示执行的sqlengine = create_engine('sqlite:///test.db', echo=False)# 创建数据库会话，用于事务提交Session = sessionmaker(bind=engine)session = Session()# 声明基类Base = declarative_base()class User(Base): __tablename__ = 'users' id = Column(Integer, primary_key=True) passwd = Column(Integer) name = Column(String, default='Noname') full_name = Column(String(50)) create_time = Column(DateTime, default= datetime.now) def __repr__(self): return str(self.id) + " &lt;User(name='%s', fullname='%s')&gt;" % ( self.name, self.full_name)# 包含外键的对象from sqlalchemy import ForeignKeyfrom sqlalchemy.orm import relationship, backrefclass Address(Base): __tablename__ = 'addresses' id = Column(Integer, primary_key=True) email_address = Column(String, nullable=False) user_id = Column(Integer, ForeignKey('users.id')) user = relationship("User", backref=backref('addresses', order_by=id)) def __repr__(self): return "&lt;Address(email_address='%s')&gt;" % self.email_address# 创建数据库表Base.metadata.create_all(engine)if __name__ == "__main__": u = User(name='sqwe', full_name='aassd', passwd=1) # 保存一条数据并提交 session.add(User(name='sqwe', full_name='aassd', passwd=1)) session.commit() # 保存多条数据并提交 session.add_all([ User(name='wendy1', full_name='aassd1', passwd=1), User(name='jack', full_name='aassd2', passwd=1), User(name='wendy3', full_name='aassd3', passwd=1), User(name='jack4', full_name='aassd4', passwd=1), ]) session.commit() # 查询操作，SELECT * FROM users WHERE name="ed" LIMIT 1; print session.query(User).filter_by(name='sqwe2').first() # 所有的User对象的list print(session.query(User).all()) # 按id排序，降序 for row in session.query(User).order_by(-User.id): print(row) # 精确查找，完全匹配给定值 for row in session.query(User).filter(User.name.in_(['sqwe', 'wendy', 'jack'])): print(row) # 模糊查找，子串匹配即可 for row in session.query(User).filter(~User.name.in_(['sqwe2', 'wendy', 'jack'])): print(row) # count计数 print(session.query(User).filter(User.name == 'sqwe').count()) # 高级查询条件，与或逻辑 from sqlalchemy import and_, or_ for row in session.query(User).filter(and_(User.name == 'ed', User.fullname == 'Ed Jones')): print(row) for row in session.query(User).filter(or_(User.name == 'ed', User.name == 'wendy')): print(row) jack = User(name='jack', full_name='Jack Bean', passwd='gjffdd') # 一对多的关系，一个用户可以有多个地址 jack.addresses = [ Address(email_address='jack@google.com'), Address(email_address='j25@yahoo.com')] session.add(jack) session.commit() # 多表条件查询 for u, a in session.query(User, Address).\ filter(User.id==Address.user_id).\ filter(Address.email_address=='jack@google.com').\ all(): print u, a Flask-SQLAlchemy ORM相关操作 ORM使用示例 使用ORM实现用户注册、登陆 db.create_all()命令可以生成所有的表 先创建一个SQLAlchemy对象 继承db.Model创建Model对象，实现对应的SQL操作 model对象要定义与数据库表的列名一致的属性 数据库的表可以用__tablename__定义 model.py 12345678910111213141516171819202122232425262728293031# model.pyfrom flask import Flaskfrom flask.ext.sqlalchemy import SQLAlchemyapp = Flask(__name__)app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://root@localhost/test'db = SQLAlchemy(app)class User(db.Model): id = db.Column(db.Integer,primary_key=True) username = db.Column(db.String(32),unique=True) password = db.Column(db.String(32)) def __init__(self,username,password): self.username = username self.password = password def add(self): try: db.session.add(self) db.session.commit() return self.id except Exception,e: db.session.rollback() return e finally: return 0 def isExisted(self): temUser=User.query.filter_by(username=self.username,password=self.password).first() if temUser is None: return 0 else: return 1 在view中实例化Model对象，并调用其SQL操作方法 123456789101112131415161718192021from model import User@app.route("/register",methods=['GET','POST'])def register(): myForm=LoginForm(request.form) if request.method=='POST': u=User(myForm.username.data,myForm.password.data) u.add() return redirect("http://www.jikexueyuan.com") return render_template('index.html',form=myForm) @app.route("/login",methods=['GET','POST'])def login(): myForm=LoginForm(request.form) if request.method =='POST': u=User(myForm.username.data,myForm.password.data) if (u.isExisted()): return redirect("http://www.jikexueyuan.com") else: return "Login Failed" return render_template('index.html',form=myForm) 多表关联，主表中设置外键，foreignkey指向从表。从表中设置relationship指向主表 1234567891011121314151617181920db = SQLAlchemy(app)# 主表class User(db.Model): __tablename__ = 'users' id = db.Column(db.Integer,primary_key=True) username = db.Column(db.String(32),unique=True) password = db.Column(db.String(32)) address_id = db.Column(db.Integer, db.ForeignKey('address.id')) # 从表class Address(db.model): __tablename__ = 'address' id = db.Column(db.Integer,primary_key=True) address = db.Column(db.String(32),unique=True) users = db.relationship('User', backref='adress_x') # 相当于为User增加了一个属性address_x # 使用 User(id=1, name='xx', address_x=Address()) ​ 实现一个留言板 SQLAlchemy Model对象有query方法，实现了一系列SQL查询操作 1234567891011121314151617class Entry(db.Model): id = db.Column(db.Integer,primary_key=True) content = db.Column(db.Text) sender = db.Column(db.String(32)) def __init__(self,content,sender): self.content = content self.sender = sender def add(self): try: db.session.add(self) db.session.commit() return self.id except Exception,e: db.session.rollback() return e finally: return 0 Entry.query.filter_by().all()可以从数据库获取全部Entry对象的记录 每个表单都用wtform实现一个类 POST提交数据，GET展示数据 12345678910111213class PublishForm(Form): content = TextField("content",[validators.Required()]) sender = TextField("sender",[validators.Required()])@app.route("/show",methods=['GET','POST'])def show(): myEntryForm = PublishForm(request.form) l = Entry.query.filter_by().all() if request.method =='POST': e = Entry(myEntryForm.content.data,myEntryForm.sender.data) e.add() return render_template("show.html",entries=l,form=myEntryForm) return render_template("show.html",entries=l,form=myEntryForm) Flask应用的外部脚本 flask-script pip install flask-script ，类似Django中的manage命令 一个manage.py示例 不带参数的函数用manager.command装饰器 带参数的函数manager.option 使用livereload启动app，可以在开发时，修改代码，页面自动刷新。 12345678910111213141516171819202122from adder import app # flask appfrom flask_script import Managermanager = Manager(app)@manager.commanddef hello_world(): print 'hello world!'@manager.option('-n', '--name', dest='name', default='my friend')def hello(name): print 'hello &#123;&#125;!'.format(name) @manager.commanddef run(): from livereload import Server live_server = Server(app.wsgi_app) live_server.watch('**/*.*') live_server.serve(open_url=True) if __name__ == '__main__': manager.run() 执行 python manage hello -n Alice就会运行hello函数。 我们可以通过外部脚本与flask应用交互。 SQLlite3操作 model对象中定义SQL存储查询等操作 123456789101112131415161718192021222324252627282930313233import sqlite3def get_conn(): return sqlite3.Connection('test.db') class User(object): def __init__(self, id, name): self.id = id self.name = name def save(self): conn = get_conn() cursor = conn.cursor() sql = 'insert into user VALUES (?, ?)' cursor.execute(sql, (self.id, self.name)) conn.commit() cursor.close() conn.close() @staticmethod def query(): sql = 'select * from user' conn = get_conn() cursor = conn.cursor() cursor.execute(sql) rows = cursor.fetchall() users = [User(row[0], row[1]) for row in rows] cursor.close() conn.close() return users def __str__(self): return 'id:&#123;&#125; ----- name:&#123;&#125;'.format(self.id, self.name) ​ 在manage中可以执行SQL 12345678910111213141516171819@manager.commanddef init_db(): conn = sqlite3.Connection('test.db') cursor = conn.cursor() sql = 'create table user(id INT, NAME VARCHAR )' cursor.execute(sql) conn.commit() cursor.close() conn.close()@manager.commanddef all_user(): for x in User.query(): print x @manager.option('-i', dest='id')@manager.option('-n', dest='name', default='sss')def save_user(id, name): User(id, name).save() Mysql操作 model对象中定义SQL存储查询等操作 Mysqldb建立连接时无法使用URI格式，接收关键字参数 user passwd db host port 12345678910111213141516171819202122232425262728293031import MySQLdbdef get_conn(): return MySQLdb.Connection(user='root', passwd='123456', db='test' )# ''mysql://root:123456@localhost:3306/test''class User(object): def __init__(self, id, name): self.id = id self.name = name def save(self): conn = get_conn() cursor = conn.cursor() sql = 'insert into user(user_id, user_name) VALUES (%s, %s)' cursor.execute(sql, (self.id, self.name)) conn.commit() cursor.close() conn.close() @staticmethod def query(): sql = 'select * from user' conn = get_conn() cursor = conn.cursor() cursor.execute(sql) rows = cursor.fetchall() users = [User(row[0], row[1]) for row in rows] cursor.close() conn.close() return users 在manage中可以执行SQL 调用对象sql方法即可 Mysql ORM使用 使用 flask_sqlalchemy的Model对象定义Model 代码简洁，且无需自己实现SQL操作 会有一定的资源消耗 1234# your appapp = Flask(__name__)app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://root:123456@localhost:3306/test'db = SQLAlchemy(app) 数据库配置 app.config中配置数据库URI app.config[&#39;SQLALCHEMY_DATABASE_URI&#39;] = &#39;mysql://root:123456@localhost:3306/test&#39; 创建db对象 db = SQLAlchemy(app) 创建完APP后，就应该创建对应的db对象，然后在models.py中引用db 定义Model model的column属性名必须和数据表中字段一致 变量名一致 12345678910111213# model.pyfrom adder import dbclass User(db.Model): user_id = db.Column(db.Integer, primary_key=True) user_name = db.Column(db.String) def __init__(self, id, name): self.user_id = id self.user_name = name def __str__(self): return 'id:&#123;&#125; ----- name:&#123;&#125;'.format(self.id, self.name) Model相关操作 对于数据库的SQL操作都是使用的db对象 一个数据库的warning FSADeprecationWarning: SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and will be disabled by default in the future.解决办法， app.config[&#39;SQLALCHEMY_TRACK_MODIFICATIONS&#39;]=True 一个Mysql数据库的bug Warning: Incorrect string value: ‘\xD6\xD0\xB9\xFA\xB1\xEA…’ for column ‘VARIABLE_VALUE’ at row 480 ​ 1234567891011121314151617# manage.pyfrom models import Userfrom adder import app, dbfrom flask_script import Managermanager = Manager(app)@manager.commanddef all_user(): for x in User.query.all(): print x@manager.option('-i', dest='id')@manager.option('-n', dest='name', default='sss')def save_user(id, name): u = User(int(id), str(name)) db.session.add(u) db.session.commit() Mobgodb ORM使用Mongodb是文档型的，Document，准确的讲应该是Document-Relational Mapping. flask_mongoengine 事务要求高的系统不适合Mongodb 使用flask_Mongengine扩展来实现ORM app.config中配置数据库URI app.config[&#39;MONGODB_SETTINGS&#39;] = {&#39;db&#39;: &#39;test&#39;} 数据库配置三种方式可选 app.config[&#39;MONGODB_SETTINGS&#39;] = { &#39;db&#39;: &#39;project1&#39;, &#39;username&#39;:&#39;webapp&#39;, &#39;password&#39;:&#39;pwd123&#39;} 1234app.config['MONGODB_SETTINGS'] = &#123; 'db': 'project1', 'host': 'mongodb://localhost/database_name'&#125; 12345app.config['MONGODB_DB'] = 'project1'app.config['MONGODB_HOST'] = '192.168.1.35'app.config['MONGODB_PORT'] = 12345app.config['MONGODB_USERNAME'] = 'webapp'app.config['MONGODB_PASSWORD'] = 'pwd123' 创建db对象 db = MongoEngine(app) 123from flask_mongoengine import MongoEngineapp.config[&apos;MONGODB_SETTINGS&apos;] = &#123;&apos;db&apos;: &apos;test&apos;&#125;db = MongoEngine(app) 创建完APP后，就应该创建对应的db对象，然后在models.py中引用db 定义Document对象 document对象和mongodb中的数据结构要保持一致。 变量命名一致 不需要实现init 12345678910111213141516# model.pyfrom adder import dbclass User(db.Document): meta = &#123; 'collection': 'todo', 'ordering': ['-create_at'], 'strict': False, &#125; user_id = db.StringField() user_name = db.StringField() create_at = db.DateTimeField(default=datetime.now) is_completed = db.BooleanField(default=False) def __str__(self): return "id: &#123;&#125;---name:&#123;&#125;".format(self.user_id, self.user_name) Model相关操作 对于数据库的SQL操作都是使用的db对象 保存用document对象的save方法 查询所有用XXX.objects.all() ​ 12345678910111213141516# manage.pyfrom models import Userfrom adder import app, dbfrom flask_script import Managermanager = Manager(app)@manager.commanddef all_user(): for x in User.objects.all(): print x@manager.option('-i', dest='id')@manager.option('-n', dest='name', default='sss')def save_user(id, name): u = User(str(id), str(name)) u.save() 条件查询 123task = 'cooking'todo = Todo.objects(task=task).first() 排序 用Todo.objects().order_by(&#39;create_at&#39;) 更新、删除数据时，先找到，在更新 1234todo = Todo.objects(task='1').first() # 先查找if todo: todo.update(is_completed=True) # 再更新 todo.delete() 还可以实现分页 事件驱动 触发器1234567891011121314151617181920class Post(db.Model): __tablename__ = 'posts' id = db.Column(db.Integer, primary_key=True) title = db.Column(db.String) body = db.Column(db.String) body_html = db.Column(db.String) created = db.Column(db.DateTime, index=True, default=datetime.utcnow) comments = db.relationship('Comment', backref='post') author_id = db.Column(db.Integer, db.ForeignKey('users.id')) @staticmethod def on_body_changed(target, value, oldvalue, initiator): if value is None or (value is ''): target.body_html = '' else: target.body_html = markdown(value)db.event.listen(Post.body, 'set', Post.on_body_changed)]]></content>
      <categories>
        <category>技术</category>
        <category>数据库</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RESTful API]]></title>
    <url>%2F2017%2F09%2F04%2F%E6%8A%80%E6%9C%AF%2FRESTful%20API%20%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[RESTful API 设计Chrome开发工具 F12 审查元素，查看network，resource，element Console使用 postman插件 RESTful简介比如微博开发平台API接口 RESTful API 不会保存会话和cookie 验证身份可以服务器生成token，请求API时携带Token即为合法请求 表现层状态转化 表现层：html、纯文本、xml、或json格式的数据。资源呈现的具体形式即为表现层。 每一个URI代表一种资源； 客户端和服务器之间，传递这种资源的某种表现形式； 状态转移 客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化” 设计REST架构的6大原则 Uniform Interface Stateless Cacheable Client-Server Layered System 分层 Code on Demond 不限制编程语言 实例 restful api使用json或xml传递数据，为了方便生成json，定义一个jsonobject对象 123456789101112import jsonclass JsonObject(): def __init__(self): self.dic=&#123;&#125; def put(self,key,value): self.dic[key]=value def get(self,key): return self.dic[key] def getJson(self): return json.dumps(self.dic,ensure_ascii=False) def getDic(self): return self.dic 使用flask-restful扩展 定义API对象 reqparse.RequestParser对象， add_argument绑定要解析哪些参数，提交的参数不满足条件，可以给出提示 auth.parse_args 解析请求参数 可插拔视图机制 12345678910111213141516171819202122232425#coding:utf-8from flask.ext.restful import reqparse, Resourcefrom JsonObject import JsonObjectfrom models import Userauth=reqparse.RequestParser()class Authentication(Resource): def get(self): pass def post(self): auth.add_argument('username',required=True,help="Username is Required") auth.add_argument('password',required=True,help="Password is Required") args=auth.parse_args() username = args['username'] password = args['password'] u = User(username,password) jsobj = JsonObject() if u.isExisted(): jsobj.put("code",1) jsobj.put("desc","User Existed") else: jsobj.put("code",2) jsobj.put("desc","User Not Existed") return jsobj.getJson(),200 ​ 在app主文件内注册API 12345678910111213141516#coding:utf-8from flask import Flaskfrom flask.ext.restful import Apifrom userAPI import Authenticationapp = Flask(__name__)api = Api(app)import sysreload(sys)sys.setdefaultencoding('utf-8')api.add_resource(Authentication,'/auth')if __name__=='__main__': app.run(port=8080) ​]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy中使用BloomFilter优化URL去重]]></title>
    <url>%2F2017%2F09%2F03%2F%E6%8A%80%E6%9C%AF%2FScrapy%E4%BD%BF%E7%94%A8bloom%20filter%E4%BC%98%E5%8C%96URL%E5%8E%BB%E9%87%8D%2F</url>
    <content type="text"><![CDATA[scrapy使用BloomFilter优化URL去重 scrapy原有的url去重算法是sha1 Hash算法，去重队列存放在内存中。 当URL记录上亿时，内存开销会非常大，因此需要更加节省内存的算法来实现URL过滤。 参考文章 scrapy_redis去重优化 基于redis的BloomFilter实现 demo 源码解读 当scrapy的scheduler将url request任务入队时，首先要判断URL是否重复 在scrapy_redis/scheduler下有一个enqueue_request方法 if not request.dont_filter and self.df.request_seen(request) 如果设置了不过滤url，或者url任务已执行过，则不入队。 12345678def enqueue_request(self, request): if not request.dont_filter and self.df.request_seen(request): self.df.log(request, self.spider) return False if self.stats: self.stats.inc_value('scheduler/enqueued/redis', spider=self.spider) self.queue.push(request) return True self.df = load_object(self.dupefilter_cls) df是dupefilter对象。 df.request_seen是去重函数 使用的是redis的集合添加操作sadd，如果存在sadd会返回0 redis中存放的是request的指纹，request_fingerprint(request) 12345def request_seen(self, request): fp = self.request_fingerprint(request) # This returns the number of values added, zero if already exists. added = self.server.sadd(self.key, fp) return added == 0 查看request_fingerprint(request)函数 fp是hashlib.sha1对象，也就是说存放在redis数据库中的是request的sha1摘要。 123456789101112131415161718def request_fingerprint(request, include_headers=None): if include_headers: include_headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers)) cache = _fingerprint_cache.setdefault(request, &#123;&#125;) if include_headers not in cache: fp = hashlib.sha1() fp.update(to_bytes(request.method)) fp.update(to_bytes(canonicalize_url(request.url))) fp.update(request.body or b'') if include_headers: for hdr in include_headers: if hdr in request.headers: fp.update(hdr) for v in request.headers.getlist(hdr): fp.update(v) cache[include_headers] = fp.hexdigest() return cache[include_headers] 如果要改进去重机制实现节省内存的话，则需要修改request_fingerprint算法生成更小的指纹，或者改进redis存储查找指纹的算法，节省空间，而不损失太多性能。 如果改变指纹算法函数的话，那么之前爬取过得任务指纹都无法使用了，sha1算法不可逆。而且找到sha1的替代也很难。 使用BloomFilter算法，改进指纹的存储和检索算法。只需改动df.request_seen 1234567def request_seen(self, request): fp = request_fingerprint(request) if self.bf.isContains(fp): # 如果已经存在 return True else: self.bf.insert(fp) return False bf为BloomFilter对象，实现了插入和查找(判断是否存在) BloomFilter算法 123456789101112131415161718192021222324252627282930313233343536373839class BloomFilter(object): def __init__(self, host='localhost', port=6379, db=0, blockNum=1, key='bloomfilter'): """ :param host: the host of Redis :param port: the port of Redis :param db: witch db in Redis :param blockNum: one blockNum for about 90,000,000; if you have more strings for filtering, increase it. :param key: the key's name in Redis """ self.server = redis.Redis(host=host, port=port, db=db) self.bit_size = 1 &lt;&lt; 31 # Redis的String类型最大容量为512M，现使用256M self.seeds = [5, 7, 11, 13, 31, 37, 61] self.key = key self.blockNum = blockNum self.hashfunc = [] for seed in self.seeds: self.hashfunc.append(SimpleHash(self.bit_size, seed)) def isContains(self, str_input): if not str_input: return False m5 = md5() m5.update(str_input) str_input = m5.hexdigest() ret = True name = self.key + str(int(str_input[0:2], 16) % self.blockNum) for f in self.hashfunc: loc = f.hash(str_input) ret = ret &amp; self.server.getbit(name, loc) return ret def insert(self, str_input): m5 = md5() m5.update(str_input) str_input = m5.hexdigest() name = self.key + str(int(str_input[0:2], 16) % self.blockNum) for f in self.hashfunc: loc = f.hash(str_input) self.server.setbit(name, loc, 1) ​]]></content>
      <categories>
        <category>技术</category>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[screen Cannot open your terminal]]></title>
    <url>%2F2017%2F09%2F02%2F%E6%8A%80%E6%9C%AF%2Fscreen%20%E8%BF%9E%E6%8E%A5%E5%87%BA%E9%94%99%2F</url>
    <content type="text"><![CDATA[解决screen Cannot open your terminal ‘/dev/pts/1’问题 问题描述: userA首先登录系统，使用screen开启了一个session，然后detach这个窗口。 userB然后登录系统，通过su - userA 变成userA，然后使用screen -r 恢复之前detached窗口，这时系统报如下错误: Cannot open your terminal ‘/dev/pts/1’ - please check. 解决方法: userB在 su - userA以后，执行如下命令即可: script /dev/null 原理 注意: 有人提到 chmod 777 /dev/pts/1，这么干的人真是误人子弟，虽然这么做的确能解决这个问题，但是会带来极大的安全问题！！！ 为什么这条命令能解决问题? 一般人看到上面这里估计就马上回去试验了，但是，等等，你不想知道为什么这个命令会有作用吗？它是怎么起作用的呢？ 我们来过一遍整个的操作步骤: 首先，usera登录到系统中，我们使用tty命令查看一下分配给他的tty，然后看一下这个tty的权限，然后用户执行screen命令。 usera@localhost ~ $ ssh usera@remotehostusera@remotehost ~ $ tty/dev/pts/1usera@remotehost ~ $ ls -l /dev/pts/1crw–w—- 1 usera tty 136, 1 2011-01-09 20:14 /dev/pts/1usera@remotehost ~ $ screen 我们观察上边的输出，发现usera对于/dev/pts/1具有读写权限，它所在组成员对这个tty具有写权限，其他用户不能访问这个tty。 然后，userb也登录到系统中，同样我们使用tty命令查看一下分配给他的tty，然后看一下这个tty的权限 userb@localhost ~ $ ssh userb@remotehostuserb@remotehost ~ $ tty/dev/pts/2userb@remotehost ~ $ ls -l /dev/pts/2crw–w—- 1 userb tty 136, 2 2011-01-09 20:20 /dev/pts/2 观察输出，userb被分配了/dev/pts/2，也是对于/dev/pts/2具有读写权限，它所在组成员对这个tty具有写权限，其他用户不能访问这个tty。 然后userb通过su - usera命令变成usera，同样我们使用tty命令查看一下分配给他的tty，然后看一下这个tty的权限 userb@remotehost ~ $ sudo su - usera[sudo] password for userb:usera@remotehost ~ $ tty/dev/pts/2usera@remotehost ~ $ ls -l /dev/pts/2crw–w—- 1 userb tty 136, 2 2011-01-09 20:20 /dev/pts/2 AHA!! 注意了，我们看到虽然userb已经变成了usera，但是他所使用的tty并没有改变，仍然是/dev/pts/2。这就是为什么执行screen命令会报错的原因了，因为所有命令此时是使用usera帐户执行的，但是/dev/pts/2的读写权限属于userb，所以所有试图控制/dev/pts/2的访问都被拒绝了！ 那么我们接下来看一下 script /dev/null做了些什么，使得screen命令能执行呢？ usera@remotehost ~ $ script /dev/nullScript started, file is /dev/nullusera@remotehost ~ $ tty/dev/pts/3usera@remotehost ~ $ ls -l /dev/pts/3crw–w—- 1 usera tty 136, 3 2011-01-09 20:36 /dev/pts/3 AHA!!! 看到了吗？我们实际上是得到了一个新的tty —&gt; /dev/pts/3，因此screen命令能够执行了，因为 /dev/pts/3这个tty的所有者是usera！]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web开发基础]]></title>
    <url>%2F2017%2F09%2F01%2F%E6%8A%80%E6%9C%AF%2Fweb%E5%BC%80%E5%8F%91%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[python web开发基础 Web开发概述CS和BS架构QQ客户端属于C/S架构。 b/s浏览器服务器架构，不需要安装/升级客户端，只要浏览器，可以跨平台。 动态/静态网站区别：数据全部来自html还是有从数据库获取加工。 MVC设计web应用 CGI程序 HTML和JavaScripthtml页面结构和元素 head ，包含meta，title，js， css body 页面主体 footer 底部 img标签 form表单 input输入框 ​ 12345678910111213141516171819&lt;head&gt; &lt;title&gt;Calculator&lt;/title&gt; &lt;script src="add.js" type="text/javascript"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div align="center" style="margin-top:40px;"&gt; &lt;img src="add.png"&gt; &lt;/div&gt; &lt;div align="center" style="margin-top:60px;"&gt; &lt;form name="form1"&gt; &lt;input type="text" placeholder="adder" name="adder1"&gt;+ &lt;input type="text" placeholder="adder-2" name="adder2"&gt;= &lt;input type="text" readonly="readonly" placeholder="result" name="result"&gt; &lt;input type="button" value="计算" onclick="add()"&gt; &lt;/form&gt; &lt;/div&gt;&lt;/body&gt;&lt;footer&gt;&lt;/footer&gt; JavaScript document定位元素对象 数值转换 1234567function add()&#123; var adder1=Number(document.form1.adder1.value); var adder2=Number(document.form1.adder2.value); var result=adder1+adder2; document.form1.result.value=result;&#125;]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Web</tag>
        <tag>Html</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask基础 hello world]]></title>
    <url>%2F2017%2F08%2F30%2F%E6%8A%80%E6%9C%AF%2Fflask%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Hello World1234567891011from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run() Flask路由 装饰器@app.route(&#39;/login&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) 可以使用多个装饰器，同一视图绑定多个url路由 反向路由url_for(&#39;index&#39;)，参数为视图函数 从URL中获取参数 get参数request.args.get(&#39;key&#39;) 路由中的参数 /user/&lt;user_id&gt; 参数转化器/user/&lt;int：user_id&gt; 自定义参数转化器 from werkzeug.routing import BaseConverter /url/&lt;regex(&quot;[a-z0-9]{3}&quot;):user_id&gt; 12345678910111213141516171819202122232425262728from flask import Flask,request,url_for,request, session, g, redirect, url_for, abort, render_template, flashapp = Flask(__name__)# 指定请求方法，默认为GET@app.route('/login/', methods=['GET', 'POST'])def login(): error = None if request.method == 'POST': # 判断请求方法 # 从POST表单中提取数据 form = request.form if form.get('username') != app.config['USERNAME']: error = 'Invalid username' elif form.get('password') != app.config['PASSWORD']: error = 'Invalid password' else: session['logged_in'] = True flash('You were logged in') # 消息提示 重定向，反向路由 return redirect(url_for('index')) 渲染模板页面 return render_template('login.html', error=error)# 获取参数@app.route('/user/&lt;int: user_id&gt;')def show_user(user_id): pass /about/和/about的区别 访问/about/等同于访问/about/index.html, 在浏览器中不加最后的斜线也能正常访问 如果路由是/about，那么访问/about/会404出错。 request request上下文可以获取cookie、form(POST)、args(GET)参数等信息 request.cookie[‘token’] 设置cookie 1234from flask import make_responseresponse = make_response(render_template('index.html', **locals()))response.set_cookie('key', 'value')return response request.files[&#39;form-key&#39;]可以 获取post提交的文件 1from werkzeug.utils import secure_filename form表单必须指明enctype参数 模板语法 使用jinja2模板，块语句、表达式、变量赋值和with域、 赋值123456789&#123;% raw %&#125;&#123;% with %&#125;&#123;% set links=[ ('Hpme', url_for('.index')), ('About', url_for('.about')), ('Projects', url_for('.projects')),]%&#125;&#123;% endwith %&#125;&#123;% endraw %&#125; 条件语句if if elif else endif 123456789&#123;% raw %&#125;&#123;% if session.logged_in %&#125; xxx&#123;% elif session.user %&#125; ooo&#123;% else %&#125; xxx&#123;% endif %&#125;&#123;% endraw%&#125; 循环语句for语句支持else，当遍历对象为空时，进入else逻辑 1234567&#123;% raw %&#125; &#123;% for entry in entries %&#125; &lt;li&gt;&lt;h2&gt;&#123;&#123; entry.title &#125;&#125;&lt;/h2&gt;&#123;&#123; entry.text|safe &#125;&#125; &#123;% else %&#125; &lt;li&gt;&lt;em&gt;Unbelievable. No entries here so far&lt;/em&gt; &#123;% endfor %&#125;&#123;% endraw%&#125; 过滤器 {{ text | safe }} {% for comment in post.comments|sort(attribute='created',reverse=True ) %} 自定义过滤器 实现markdown渲染的例子 1234@app.template_filter('md')def markdown_to_html(text): from markdown import markdown return markdown(text) 可以调用服务器的函数 {{ read_mdfile('readme.md') | md | safe }} 12345678def read_mdfile(filename): with open(filename) as md: content = reduce(lambda x, y: x + y, md.readlines()) return content.decode('utf-8')@app.context_processordef inject_methods(): return dict(read_mdfile=read_mdfile) 模板继承 同一个block如何引用多次？ {{ self.title() }} 如果只是在父类block基础上部分增加内容，而不是覆盖，又不想重复写一遍代码？ {% block footer %} 增加内容{{ super() }} 增加内容{% endblock %} block内如何引用block外的变量 ？scoped参数 {% block title scoped %} {{ item }} {% endblock %} 模板继承，引入文件，重写覆盖block块 include参数可以是数组，一次引入多个文件 1234567&#123;% raw %&#125;&#123;% extend 'base.html' %&#125;&#123;% block body %&#125; &#123;% endblock body %&#125;重写base里的block&#123;% include 'footer.html' %&#125; //从html文档里导入代码&#123;% endraw%&#125; base.html 12345678910111213141516171819&#123;% raw %&#125;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&#123;&#123; title &#125;&#125;&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt; &#123;% block header %&#125;&#123;% endblock %&#125;&lt;/div&gt;&#123;% block content %&#125;&#123;% endblock %&#125;&lt;div&gt; &#123;% block footer %&#125; &#123;% endblock %&#125;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&#123;% endraw %&#125; ​ extend_base.html 123456789101112&#123;% raw %&#125;&#123;% extend 'base.html' %&#125;&#123;% block header %&#125; &#123;% include 'header.html' %&#125;&#123;% endblock %&#125;&#123;% block content %&#125; &lt;h2&gt;这是页面内容&lt;/h2&gt;&#123;% endblock %&#125;&#123;% block footer %&#125; &#123;% include 'footer.html' %&#125;&#123;% endblock %&#125;&#123;% endraw %&#125; header.html 1234567891011121314151617181920&#123;% raw %&#125;&#123;% if session.logged_in %&#125; &lt;form action="&#123;&#123; url_for('add_entry') &#125;&#125;" method=post class=add-entry&gt; &lt;dl&gt; &lt;dt&gt;Title: &lt;dd&gt;&lt;input type=text size=30 name=title&gt; &lt;dt&gt;Text: &lt;dd&gt;&lt;textarea name=text rows=5 cols=40&gt;&lt;/textarea&gt; &lt;dd&gt;&lt;input type=submit value=Share&gt; &lt;/dl&gt; &lt;/form&gt; &#123;% endif %&#125; &lt;ul class=entries&gt; &#123;% for entry in entries %&#125; &lt;li&gt;&lt;h2&gt;&#123;&#123; entry.title &#125;&#125;&lt;/h2&gt;&#123;&#123; entry.text|safe &#125;&#125; &#123;% else %&#125; &lt;li&gt;&lt;em&gt;Unbelievable. No entries here so far&lt;/em&gt; &#123;% endfor %&#125; &lt;/ul&gt;&#123;% endraw %&#125; ​ 模板宏 宏类似于python的函数，引用 {{ input('password', type='password') }} 当宏很多时，可以写在单独的文件里，然后在需要的地方引用 {% import '_marcos.html' as ui %} 然后调用 {{ ui.input('password', type='password') }} 消息提示与异常处理消息提示 消息闪现 flash函数, 需要定义app.secret_key加密消息 在视图函数中使用flash(&quot;login success&quot;) 模板中使用get_flashed_messages 12345&#123;% raw %&#125; &#123;% for message in get_flashed_messages() %&#125; &lt;div class=flash&gt;&#123;&#123; message &#125;&#125;&lt;/div&gt; &#123;% endfor %&#125;&#123;% endraw %&#125; 异常处理 @app.httphandler(404)装饰器 abort(404) 主动抛出404异常 12345678910@app.errorhandler(404)def not_found(e): return render_template("404.html")@app.route('/find/&lt;user_id&gt;')def find(user_id): if int(user_id): return render_template("user.html") else: abort(404) # 抛出异常 视图蓝图可以根据功能将一个项目拆分成多个子app模块。 url_for]]></content>
      <categories>
        <category>技术</category>
        <category>flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pip requirements]]></title>
    <url>%2F2017%2F05%2F03%2F%E6%8A%80%E6%9C%AF%2FPython-pip-requirement%2F</url>
    <content type="text"><![CDATA[requirements.txt的作用在很多Python项目中都包含一个requirements.txt文件，里面写的是一些包的名称和版本信息。 描述运行这个项目所需要的环境，包括一些库。 可以使用pip install -r requirements.txt安装这些库。 查找python项目依赖并生成requirements.txt 将整个python环境的依赖包list出来， pip freeze &gt; requirements.txt list某个项目用到的依赖包,使用工具pipreqs ，但有的时候结果会有偏差(源码分析的不准确) pip install pipreqs pipreqs ./]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海豚笔试]]></title>
    <url>%2F2017%2F04%2F13%2F%E6%8A%80%E6%9C%AF%2Fdolphin-exam%2F</url>
    <content type="text"><![CDATA[今天参加了一个实习生招聘的笔试，准备的不充分，错的惨不忍睹。整理一下遇到的题目。 1、Java部分试题有java和c++两种，以前上课学的C和Java早还给老师了，python的BIF用多了，发现连个排序算法都写不好了。 考察java的类继承和静态方法 通过类名来调用子类中的静态变量和静态方法，当父类与子类相同时是，子类会隐藏父类中与其相同的静态变量和静态方法，如果子类中没有与其父类相同的静态变量和静态方法，子类从其父类调用过来的静态变量和静态方法就会表现出来。 通过子类创建对象来用对象名调用子类中的静态变量和静态方法，除非是父类没有的静态变量和静态方法，会显示其子类的静态变量和静态方法。否则，最后显示一定是从父类哪里引用来的静态变量和静态方法。 考察Java String变量的比较及值比较 1234567891011121314151617181920//"=="操作符:用于基本数据类型的比较,判断引用是否指向同一内存块//如果String缓冲池内不存在与其指定值相同的String对象，那么此时虚拟机将为此创建新的String对象，并存放在String缓冲池内。import java.io.*;class test &#123; public static void main (String[] args) throws java.lang.Exception &#123; String s1 = "helloworld"; String s2 = "hello" + "world"; String s0 = "helloworld"; String s3 = new String("helloworld"); System.out.println(s1==s0); // true System.out.println(s2==s0); // true System.out.println(s1==s2); // true System.out.println(s1.equals(s0)); // true System.out.println(s1.equals(s3)); // true System.out.println(s1==s3); //false &#125;&#125;//如果String缓冲池内存在与其指定值相同的String对象，那么此时虚拟机将不为此创建新的String对象，而直接返回已存在的String对象的引用。 ​ 简单的算法，一个32位整数的数组，返回有序排列的2个相邻元素之差的最大值。整数是32位的，时间复杂度o(n)有加分。LeetCode 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115// 写了个最蠢的方法，冒泡排序，然后遍历数组，求最大的差值。时间复杂度在排序，o(n^2);// 数据结构书里只记得o(n*logn)的排序算法，快速排序，插入排序，堆排序，选择排序o(n^2);// 在整数取值范围有限的情况下，计数排序、基数排序和桶排序可以实现空间复杂度O(k),时间复杂度O(n)的排序/* 思路：使用桶排序的原理，但桶的取值设为n-1，而不是整数的取值范围。桶内数据无序，桶间，用后一个桶的min 减去 前一个桶的max即可。*/public class MaxGap&#123; public int maximumGap(int[] num) &#123; int maxGap = 0; // edge case if (num.length &lt; 2) &#123; return maxGap; &#125; // get maximum and minimum int min = num[0]; int max = num[0]; for (int i = 0; i &lt; num.length; i++) &#123; if (num[i] &lt; min) min = num[i]; if (num[i] &gt; max) max = num[i]; &#125; // divide into identical gaps Gap[] gaps = new Gap[num.length - 1]; boolean[] Engaged = new boolean[num.length - 1]; double gap = (double) (max - min) / (double) (num.length - 1); for (int i = 0; i &lt; gaps.length; i++) Engaged[Math.min((int) Math.floor((double) (num[i] - min) / gap), gaps.length - 1)] = true; // assign maximum and minimum for each gap for (int i = 0; i &lt; gaps.length; i++) gaps[i] = new Gap(); for (int i = 0; i &lt; num.length; i++) &#123; int index = (int) Math.floor((double) (num[i] - min) / gap); index = Math.min(index, gaps.length - 1); // lower bound if (gaps[index].low == -1) gaps[index].low = num[i]; else gaps[index].low = Math.min(gaps[index].low, num[i]); // upper bound if (gaps[index].high == -1) gaps[index].high = num[i]; else gaps[index].high = Math.max(gaps[index].high, num[i]); &#125; // find maximum gap for (int i = 0; i &lt; gaps.length; i++) &#123; if (Engaged[i]) &#123; // check its inner gap maxGap = Math.max(gaps[i].high - gaps[i].low, maxGap); // lower all the way int j = i; while (--j &gt;= 0) &#123; if (Engaged[j]) break; &#125; if (j &gt;= 0) maxGap = Math.max(gaps[i].low - gaps[j].high, maxGap); // upper all the way j = i; while (++j &lt; num.length - 2) &#123; if (Engaged[j]) break; &#125; if (j &lt; gaps.length) maxGap = Math.max(gaps[j].low - gaps[i].high, maxGap); &#125; &#125; return maxGap; &#125; class Gap &#123; int low; int high; boolean hasItem; Gap() &#123; low = -1; high = -1; &#125; Gap(int x, int y) &#123; low = x; high = y; &#125; &#125; public static void main(String[] args) &#123; int[] num = &#123;1, 2, 3, 5, 7, 9&#125;; System.out.println((new MaxGap()).maximumGap(num)); &#125;&#125; ​ 2、Python部分 下列不能创建字典的语句是 dict1 = {[1,2,3]: &#39;use&#39;} 12345678910a = &#123;[1,2 ,3]: "user"&#125;Traceback (most recent call last): Python Shell, prompt 6, line 1TypeError: unhashable type: 'list'a = &#123;4:5&#125; a = &#123;&#125;a = &#123;(1,2,3): 'use'&#125;print a&#123;(1, 2, 3): 'use'&#125; Numpy数组的切片操作 3、机器学习 研究发现，买尿布的顾客中80%的也会同时购买啤酒，这属于数据挖掘的哪种问题？关联规则。 为了防止过拟合可以采取的方法，正则化，early stopping，数据集扩增，Dropout(神经网络)。 分类和回归的区别，应用场景，常见的算法 分类和回归的区别在于输出变量的类型。 定量输出称为回归，或者说是连续变量预测；预测明天的气温是多少度，这是一个回归任务；定性输出称为分类，或者说是离散变量预测。预测明天是阴、晴还是雨，就是一个分类任务。 常见算法？好像很多算法思想都既能用于回归，也能用于分类。分类和回归应该有内在联系， 标记一下，以后深入学习后补充 逻辑回归中的常用激励函数 记得神经网络里有sigmod函数做激励函数，逻辑回归？ logistic回归此处留疑，后面再补充 描述你熟悉的神经网络和它们的特征 重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation）， Hopfield网络，自组织映射（Self-Organizing Map, SOM）。学习矢量量化（Learning Vector Quantization， LVQ） 都不熟悉，先去看书了:cry: ​]]></content>
      <categories>
        <category>技术</category>
        <category>其他</category>
      </categories>
      <tags>
        <tag>Exam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy-redis 分布式爬虫]]></title>
    <url>%2F2017%2F03%2F02%2F%E6%8A%80%E6%9C%AF%2Fscrapy-redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[scrapy-redis 分布式爬虫简介安装和配置 安装redis数据库 pip install scrapy redis-py scrapy-redis 作用和特点 scrapy-redis是为Scrapy提供redis支持以实现分布式爬虫的组件 多个爬虫共享一个redis队列（分配request） 分布式的post处理。将爬到的items也放入redis队列，因而可以实现items的分布式处理。 scrapy-redis仅仅为scrapy提供了部分基于redis的组件，可以查看源码。 pipeline scheluder redis队列替换原有的scrapy队列 过滤器 Duplication 初步使用settings参数123456789101112131415161718192021222324252627282930313233343536# Enables scheduling storing requests queue in redis.SCHEDULER = "scrapy_redis.scheduler.Scheduler"# Ensure all spiders share same duplicates filter through redis.DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"# Don't cleanup redis queues, allows to pause/resume crawls.SCHEDULER_PERSIST = True# Schedule requests using a priority queue. (default)SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'# Alternative queues.#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'# Max idle time to prevent the spider from being closed when distributed crawling.# This only works if queue class is SpiderQueue or SpiderStack,# and may also block the same time when your spider start at the first time (because the queue is empty).#SCHEDULER_IDLE_BEFORE_CLOSE = 10# Store scraped item in redis for post-processing.ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 300&#125;# Specify the host and port to use when connecting to Redis (optional).#REDIS_HOST = 'localhost'#REDIS_PORT = 6379# Specify the full Redis URL for connecting (optional).# If set, this takes precedence over the REDIS_HOST and REDIS_PORT settings.REDIS_URL = 'redis://user:pass@hostname:9001'# Use other encoding than utf-8 for redis.默认utf-8REDIS_ENCODING = 'latin1' scrapy-redis使用方法 首先用scrapy实现一个爬虫，然后在替换其中的组件为scrapy-redis setting里修改： 123456789101112131415# setting.pyBOT_NAME = 'moko1'SPIDER_MODULES = ['moko1.spiders']NEWSPIDER_MODULE = 'moko1.spiders'# 使用scrapy-redis的去重和调度器组件DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"SCHEDULER = "scrapy_redis.scheduler.Scheduler"SCHEDULER_PERSIST = TrueITEM_PIPELINES = &#123;# 会将items放入redis队列中'scrapy_redis.pipelines.RedisPipeline': 400,&#125; spiders里修改： spider类从scrapy_redis.spiders导入，有RedisSpider和RedisCrawlSpider，对应scrapy原来的Spider和CrawlSpider。 start_urls改为从redis中某个key获取，因此redis_key = ‘moko_spider:start_urls’，然后向该key push数据。 直接给出start_urls也是可行的，但是不太符合redis队列及分布式的逻辑，而且不能手动动态添加。 123456789101112131415161718192021# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import Rulefrom ..items import Moko1Itemfrom scrapy_redis.spiders import RedisCrawlSpiderclass MokoSpiderSpider(RedisCrawlSpider): # 修改此处 name = 'moko_spider' allowed_domains = ['moko.cc'] start_urls = ['http://www.moko.cc/moko/post/1.html'] # 修改此处 # redis_key = 'moko_spider:start_urls' rules = ( Rule(LinkExtractor(allow=r'http://www\.moko\.cc/post/\d+\.html'), callback='parse_item', follow=True), ) def parse_item(self, response): item = Moko1Item() item['name'] = response.css("#workNickName::text").extract()[0] return item 确保redis数据库运行，清空数据库flushdb, “moko_spider:dupefilter”保存了我上次执行时已经爬取过的url信息。再次执行会被过滤掉，scrapy的去重机制。 然后启动scrapy爬虫，然后向redis_key = ‘moko_spider:start_urls’中push数据，在redis-cli客户端中执行lpush moko_spider:start_urls https://moko.cc/1.html 如果启用了scrapy_redis.pipelines.RedisPipeline，items会存储在moko_spider:items中。可以将items不断的pop出来，并进行其他处理，如存储等。(感觉这种活应该交给一个pipeline干) 分布式爬取直接运行多个爬虫 上面的单个爬虫默认从localhost的Redis数据库中存取request和爬到的items， 而实现分布式爬虫只需要为爬虫指定Redis数据库的网络位置，所有的爬虫都去redis队列里存取。 12345# settings.pyREDIS_URL = 'redis://user:mima@localhost:6379' # 或者不带密码的REDIS_HOST = 'localhost'REDIS_PORT = 6379 配置redis允许远程访问 修改配置文件/etc/redis.conf 12# bind 127.0.0.1requirepass mima # 设置密码 ​ 关于主从模式 分布式架构一般分为主从模式和P2P模式。有的人认为scrapy-redis中安装有redis数据库的节点就是master，错的！ scrapy-redis中的每只爬虫都是平级的，没有主从之分。每只爬虫都是主动请求任务，执行任务，爬到的数据也可以提交给redis。redis的request队列为空时，爬虫处于饥饿状态。 scrapy-redis仅仅是把scrapy原来得本地队列放入redis数据库中，从通信和数据传输的角度来看，redis像是一个master，而实际上redis对爬虫没做任何控制和操作，只是被动的为它们提供数据。 利用docker部署爬虫 创建一个docker镜像并配置scrapy环境，这样下次就能恢复环境直接使用了 首先在daocloud申请一台胶囊主机，然后ssh登录上去，配置scrapy环境 123456789101112# 使用ubuntu的docker镜像docker run -it daocloud.io/ubuntu:14.04 /bin/bashapt-get update apt-get install lrzszapt-get install python2.7 python-pip python-devapt-get install libxml2-dev libxslt-dev python-lxml # 安装lxmlapt-get install build-essential libssl-dev libffi-devpip install six --upgradepython -m pip install pyparsing appdirspip install cryptographypip install pymongo redis twisted scrapy scrapy-redisexit # 退出docker，记住id，docker id root@ea0d832b19bb 打包上传镜像 123456789101112131415161718# 打包镜像，docker容器的id ea0d832b19bb~$ docker commit -m "ubuntu with scrapy_redis" -a "author——info" ea0d832b19bb scrapy-redissha256:53a605ccc92ab29bba70f9f026c002a9c4afa43fb9b14a9819d5859f51b0d586~$ docker images # 查看镜像# 为镜像打上tagdocker tag scrapy-redis syy2358/scrapy-redis:latest# 上传至dockerhub托管docker login # 先注册并登录dockerhub，创建一个托管仓库scrapy-redisdocker push syy2358/scrapy-redis:lastest # 将镜像上传至dockerhub# 胶囊主机只有2小时，hub上传速度又慢，只好打包镜像文件下载到我的电脑上。docker save ea0d832b19bb &gt; /home/ubuntu/scrapy-redis.tar# 可以在有docker的电脑上恢复该容器，docker load &lt; scrapy-redis.tar# 或者直接拉dockerhub/daocloud上的镜像用就行了docker pull syy2358/scrapy-redisdocker run -it xx.xx# 导出 export 和save的区别- 导入 import# save保存了容器的运行状态，支持回滚，但是数据较大。 ​自己配置环境各种报错，主要是下载链接超时，用daocloud就很顺利 首先自己编译docker镜像容易遇到各种错误，而且dockerhub的镜像push、pull的速度巨慢，估计是被墙了，所以决定改用daocloud在线编译发布镜像，编译和pull的速度都很快。 镜像制作过程： 在自己 GitHub 创建新的 repository 。 将爬虫的代码，包含Dockerfile push 到自己刚创建的 repository。 到 https://dashboard.daocloud.io/build-flows/new ，项目名称 scrapy，选择自己刚在 GitHub 创建的 repository同步代码，开始创建，选择分支：master，手动执行。如果失败，可以先看下流程定义里的构建阶段，修改任务，选择云端Dockerfile。 到 https://dashboard.daocloud.io/packages 选择 scrapy，设置 -&gt; 镜像访问控制 -&gt; 公开,设置tag为latest。 https://dashboard.daocloud.io/packages/选择scrapy后，版本 -&gt; latest 。然后可以部署到已经接入的docker或者云测试环境(右上角打开控制台，能进入web版的终端，在里面执行scrapy crawl spider即可)。 或者在自己的docker环境下，使用docker run -it daocloud.io/blue_whale/scrapy,然后就能看到爬虫在运行了 镜像地址 daocloud.io/blue_whale/scrapy : daocloud上的，速度很快 syy2358/scrapy-redis: dockerhub上的，巨慢 ​ 运行爬虫 上传源码，并从Dockerfile build镜像，然后运行爬虫 1234567891011121314151617181920212223242526272829303132333435# 项目结构.├── docker-compose.yml├── Dockerfile├── moko1│ ├── __init__.py│ ├── items.py│ ├── pipelines.py│ ├── settings.py│ └── spiders│ ├── __init__.py│ ├── moko_spider.py├── requirements.txt└── scrapy.cfg---------------------------# docker-compose.ymlversion: '2'services: spider: build: . volumes: - .:/code------------------------# DockerfileFROM syy2358/scrapy-redisENV PATH /usr/local/bin:$PATHADD . /codeWORKDIR /codeRUN pip install -r requirements.txt# COPY spiders.py /usr/local/lib/python3.5/site-packages/scrapy_redisCMD scrapy crawl moko_spider 我的redis服务器是在腾讯云上的，没有使用docker。 如果redis在docker中运行的话，需要在docker-compose.yml中定义redis的container，将spider和redis link起来，同时redis需要映射端口6379，这样不同的container之间才能相互通信。 使用docker-compose创建container 12345pip install docker-compose rz -E # 上传爬虫源码 docker-compose up #从 docker-compose.yml 中创建 `container` docker-compose scale spider=4 #将 spider 这一个服务扩展到4个container # 会有4个scrapy爬虫运行，处于饥饿状态，因为刚开始start_urls为空，直到我们pushurl去feed爬虫，爬虫才会开始抓取工作。 ​ 方法二，使用已经build好的docker镜像(爬虫代码也已经copy进去了) 123456docker run -it daocloud.io/blue_whale/scrapy### 或者docker run -it syy2358/scrapy-redis### Ctrl+P+Q 将当前container放入后台，回到docker界面docker ps -a ## 查看正在运行的containerdocker attach id # 连接入正在执行的container 退出attach的docker container 用1执行爬虫时真惨，attach上container退不出去，scrapy不停地输出log内容，按啥键都不好使，只好退出ssh重连T.T，重连后发现原来的container仍在运行中。 正常attach上一个container，可以Ctrl+P+Q退出再后台执行，或exit终止运行并退出container。 使用Docker attach命名进入docker容器后： 【场景一】如果要正常退出不关闭容器，请按Ctrl+P+Q进行退出容器。 【场景二】如果使用exit退出，那么在退出容器后会关闭容器，如下图所示。 总结 只需要配置一次scrapy的docker运行环境，上传代码，然后将container打包成镜像，就可以在任何有docker的地方pull下镜像运行。 docker挺有意思的，项目部署非常方便，不过我这个新手对docker只是一知半解。 导出redis中的items linux下使用redis-dump redis-dump -u 127.0.0.1:6379 &gt; db_full.json 将数据导入mongodb中 123456789101112131415161718192021222324#!/usr/bin/env python# -*- coding: utf-8 -*-import jsonimport redisimport pymongodef main(): r = redis.Redis(host='192.168.1.112',port=6379,db=0) client = pymongo.MongoClient(host='localhost', port=27017) db = client['dmoz'] sheet = db['sheet'] while True: # 将队列里的数据逐条pop出来，并插入mongodb中 # process queue as FIFO, change `blpop` to `brpop` to process as LIFO source, data = r.blpop(["dmoz:items"]) item = json.loads(data) sheet.insert(item) try: print u"Processing: %(name)s &lt;%(link)s&gt;" % item except KeyError: print u"Error procesing: %r" % itemif __name__ == '__main__': main() ​]]></content>
      <categories>
        <category>技术</category>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Scrapy</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis学习笔记]]></title>
    <url>%2F2017%2F03%2F01%2F%E6%8A%80%E6%9C%AF%2Fredis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[redis学习笔记no sql：redis，mongodb，Memcached redis：Remote Directory Server，远程字典服务器。 键值数据类型支持： ●字符串String类型 set、get、getset。。。 ●散列Hash类型 类似python的字典 hset、hget、hdel、hgetall、hkeys、hvals ●列表List类型 lpush、rpush、lpop、rpop、llen ●集合Set类型 sadd、smembers、srem、spop、sdiff(差集)、sinter(交集)、sunion(并集)、scard(长度)、 ●有序集合Zset类型 ZADD key [NX|XX][CH] [INCR] score member [score member ...] zscore、zcard、zrank、 一个键最多存储512MB 官网 1、下载安装linux下安装apt-get install redis-server,启动service redis-server start redis-server ：服务器 redis-benchmark：性能测试工具 redis-cli：命令行客户端 redis-check-dump：RDB文件检测工具 redis-check-aof：AOF文件修复工具 启动命令 redis-server redis.windows.conf 设置Redis服务 由于上面虽然启动了redis，但是只要一关闭cmd窗口，redis就会消失。所以要把redis设置成windows下的服务。 redis-server --service-install redis.windows-service.conf --loglevel verbose 卸载服务：redis-server –service-uninstall 开启服务：redis-server –service-start 停止服务：redis-server –service-stop 测试： redis-cli -h 127.0.0.1 -p 6379 参数配置 启动参数 redis-server redis.windows.conf –port xxx –loglevel notice 命令行客户端中动态修改 127.0.0.1:6379&gt; CONFIG get loglevel1) “loglevel”2) “notice”127.0.0.1:6379&gt; CONFIG GET port1) “port”2) “6379”127.0.0.1:6379&gt; CONFIG SET loglevel warningOK127.0.0.1:6379&gt; CONFIG GET loglevel1) “loglevel”2) “warning” 配置文件redis.conf port 6379 默认端口 bind 127.0.0.1，默认绑定的主机地址 timeout 0，当客户端闲置多久之后关闭连接，0代表没有启动这个选项 loglevel notice，日志的记录级别 # debug：很详细的信息，适合开发和测试 # verbose ：包含很多不太有用的信息 # notice ：比较适合生产环境 # warning ：警告信息 logfile stdout代表日志的记录方式，默认为标准输出 databases 16代表默认数据库的数量16个,默认的数据库编号从0开始，select 1 选择数据库1 save 300 10 –300秒内将10个更改同步到磁盘 save 900 1 dbfilename dump.rdb，指定本地数据库文件名，默认为dump.rdb dir ./,指定本地数据库的存放目录，默认是当前目录 requirepass password 设置认证 ​ ​ 2、客户端命令行2.1、命令返回消息类型： 状态信息 127.0.0.1:6379&gt; set test “test_value” OK 127.0.0.1:6379&gt; ping PONG 错误回复 127.0.0.1:6379&gt; xx (error) ERR unknown command ‘xx’ 整数 127.0.0.1:6379&gt; DBSIZE (integer) 1 字符串 127.0.0.1:6379&gt; get a “1” 127.0.0.1:6379&gt; get test “test_value” 127.0.0.1:6379&gt; get aa (nil) nil表示空的结果 多行字符串 127.0.0.1:6379&gt; keys * 1) “b” 2) “test” 3) “a” 2.2、常用命令‘redis-cli -h 127.0.0.1 -p 6379 -a passwd’ select 0 选择0号数据库 exit quit 断开连接 shutdown 同时关闭服务器 参考文档http://doc.redisfans.com/ 2.3、使用python redis-py连接Redispip install redis redis-py没有实现select 1234r = redis.Redis(host='localhost', port=6379, db=0, password=None)print r.set('a', 'a_value', ex=None, px=None, nx=False, xx=False)print r.get('a')print r.config_get('loglevel') 使用pipeline提高执行效率 1234567r = redis.Redis(host='localhost', port=6379, db=1)p = r.pipeline()p.set('k1', 'v1')p.set('qqq', 2)p.incr('num1')p.execute()print r.keys('*') 3、事务事务可以理解为一些列操作的集合，或一个过程。要么成功（全部执行），要么失败（全部都不被执行）。 开启事务 EXEC 执行事务 事务的执行 监视key WATCH counter1 counter2 如果在执行事务之前key如果被其它命令改动，事务就被打断了，不会执行。 UNWATCH:取消WATCH命令对所有key的监视 取消事务 DISCARD 错误处理 语法错误 错误的语法不会被添加到队列中，自然也不会被执行。 运行出错 事务中QUEUED的语句，在运行中出错，其他的命令也会成功执行。Redis事务不支持回滚。 4、缓存和Redis生存时间redis的键值可以设置生存时间，到期后自动删除。 EXPIRE（设置过期的秒数）/EXPIREAT（在指定的时间戳进行删除） PEXPIRE（设置过期的微秒数）/PEXPIREAT（在指定的时间戳进行删除） PERSIST（永不过期，持久化） TTL （得到某个键的生存时间） PTTL （得到某个键的生存时间） 将近期访问过得数据保存到redis中。如果数据不在Redis中，再去数据库中取。 如果大量的使用这种缓存键而且生存时间设置的过长，Redis会占用大量内存。 如果缓存键的生存时间设置太短的话，可能会导致缓存的命中率过低，内存利用率低。 5、消息队列5.1、生产者-消费者模式123456789101112131415161718# coding: utf-8import redisclass Task: def __init__(self): self.rcon = redis.Redis(db=5) self.queue = 'task:prodcons:queue' def process_task(self): while True: task = self.rcon.blpop(self.queue, 0)[1] # blpop阻塞式的pop print 'Task: %s' % taskTask().process_task()# 向'task:prodcons:queue'里lpush数据后，会有数据打印出来 5.2、发布-订阅模式12345678910111213141516171819# coding: utf-8import redisclass Task: def __init__(self): self.rcon = redis.Redis(db=5) self.ps = self.rcon.pubsub() self.ps.subscribe('task:pubsub:channel') # 订阅频道 def process_task(self): for i in self.ps.listen(): # 是个生成器 if i['type'] == 'message': print 'Task: %s' % i['data']Task().process_task()# 向'task:pubsub:channel'里publish数据后，每个订阅该频道的程序都会会打印打印相同的消息出来]]></content>
      <categories>
        <category>技术</category>
        <category>redis</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy_redis源码阅读]]></title>
    <url>%2F2017%2F03%2F01%2F%E6%8A%80%E6%9C%AF%2Fscapy-redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[scapy-redis 源码阅读connection.py 创建redis连接实例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import sixfrom scrapy.utils.misc import load_object # 加载对象from . import defaults# Shortcut maps 'setting name' -&gt; 'parmater name'.SETTINGS_PARAMS_MAP = &#123; 'REDIS_URL': 'url', 'REDIS_HOST': 'host', 'REDIS_PORT': 'port', 'REDIS_ENCODING': 'encoding',&#125;def get_redis_from_settings(settings): """Returns a redis client instance from given Scrapy settings object. This function uses ``get_client`` to instantiate the client and uses ``defaults.REDIS_PARAMS`` global as defaults values for the parameters. You can override them using the ``REDIS_PARAMS`` setting. Parameters ---------- settings : Settings A scrapy settings object. See the supported settings below. Returns ------- server Redis client instance. Other Parameters ---------------- REDIS_URL : str, optional Server connection URL. REDIS_HOST : str, optional Server host. REDIS_PORT : str, optional Server port. REDIS_ENCODING : str, optional Data encoding. REDIS_PARAMS : dict, optional Additional client parameters. """ # 获取Redis的设置参数 params = defaults.REDIS_PARAMS.copy() params.update(settings.getdict('REDIS_PARAMS')) for source, dest in SETTINGS_PARAMS_MAP.items(): val = settings.get(source) if val: params[dest] = val # ``redis_cls``是redis类的路径 if isinstance(params.get('redis_cls'), six.string_types): params['redis_cls'] = load_object(params['redis_cls']) # 通过'redis_cls'得到redis对象，如果if为False，get_redis函数中会加载默认的redis类StrictRedis return get_redis(**params)# Backwards compatible alias.from_settings = get_redis_from_settingsdef get_redis(**kwargs): """Returns a redis client instance. Parameters ---------- redis_cls : class, optional Defaults to ``redis.StrictRedis``. url : str, optional If given, ``redis_cls.from_url`` is used to instantiate the class. **kwargs Extra parameters to be passed to the ``redis_cls`` class. Returns ------- server Redis client instance. """ redis_cls = kwargs.pop('redis_cls', defaults.REDIS_CLS) # defaults.REDIS_CLS 默认值redis.StrictRedis url = kwargs.pop('url', None) if url: return redis_cls.from_url(url, **kwargs) # url不为None，则通过url创建一个Redis连接客户端实例 else: return redis_cls(**kwargs) dupefilter.py 实现去重机制 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131import loggingimport timefrom scrapy.dupefilters import BaseDupeFilterfrom scrapy.utils.request import request_fingerprint # 计算url的指纹from . import defaultsfrom .connection import get_redis_from_settingslogger = logging.getLogger(__name__)# TODO: Rename class to RedisDupeFilter.class RFPDupeFilter(BaseDupeFilter): """Redis-based request duplicates filter. This class can also be used with default Scrapy's scheduler. """ logger = logger def __init__(self, server, key, debug=False): """Initialize the duplicates filter. Parameters ---------- server : redis.StrictRedis The redis server instance. key : str Redis key Where to store fingerprints. debug : bool, optional Whether to log filtered requests. """ self.server = server self.key = key self.debug = debug self.logdupes = True @classmethod def from_settings(cls, settings): """Returns an instance from given settings. This uses by default the key ``dupefilter:&lt;timestamp&gt;``. When using the ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as it needs to pass the spider name in the key. Parameters ---------- settings : scrapy.settings.Settings Returns ------- RFPDupeFilter A RFPDupeFilter instance. """ server = get_redis_from_settings(settings) # XXX: This creates one-time key. needed to support to use this # class as standalone dupefilter with scrapy's default scheduler # if scrapy passes spider on open() method this wouldn't be needed # TODO: Use SCRAPY_JOB env as default and fallback to timestamp. key = defaults.DUPEFILTER_KEY % &#123;'timestamp': int(time.time())&#125; debug = settings.getbool('DUPEFILTER_DEBUG') return cls(server, key=key, debug=debug) @classmethod def from_crawler(cls, crawler): """Returns instance from crawler. Parameters ---------- crawler : scrapy.crawler.Crawler Returns ------- RFPDupeFilter Instance of RFPDupeFilter. """ return cls.from_settings(crawler.settings) def request_seen(self, request): """ 去重判断 如果request已经请求过了，则返回false """ fp = self.request_fingerprint(request) # This returns the number of values added, zero if already exists. # redis客户端的sadd操作，即向集合里添加元素 added = self.server.sadd(self.key, fp) return added == 0 def request_fingerprint(self, request): return request_fingerprint(request) def close(self, reason=''): """Delete data on close. Called by Scrapy's scheduler. 退出时清除URL指纹数据 Parameters ---------- reason : str, optional """ self.clear() def clear(self): """Clears fingerprints data.""" self.server.delete(self.key) def log(self, request, spider): """Logs given request. Parameters ---------- request : scrapy.http.Request spider : scrapy.spiders.Spider """ if self.debug: msg = "Filtered duplicate request: %(request)s" self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) elif self.logdupes: msg = ("Filtered duplicate request %(request)s" " - no more duplicates will be shown" " (see DUPEFILTER_DEBUG to show all duplicates)") self.logger.debug(msg, &#123;'request': request&#125;, extra=&#123;'spider': spider&#125;) self.logdupes = False picklecompat.py 实现序列化， 参考廖雪峰-序列化 12345678910111213141516"""A pickle wrapper module with protocol=-1 by default.redis数据格式有整数和字符串，其他的Python复杂的数据类型需要序列化成字符串后存入redis把变量从内存中变成可存储或传输的过程称之为序列化，"""try: import cPickle as pickle # PY2except ImportError: import pickledef loads(s): return pickle.loads(s)def dumps(obj): return pickle.dumps(obj, protocol=-1) pipelines.py&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869from scrapy.utils.misc import load_objectfrom scrapy.utils.serialize import ScrapyJSONEncoderfrom twisted.internet.threads import deferToThreadfrom . import connection, defaultsdefault_serialize = ScrapyJSONEncoder().encodeclass RedisPipeline(object): """Pushes serialized item into a redis list/queue Settings -------- REDIS_ITEMS_KEY : str Redis key where to store items. REDIS_ITEMS_SERIALIZER : str Object path to serializer function. """ def __init__(self, server, key=defaults.PIPELINE_KEY, serialize_func=default_serialize): """Initialize pipeline. ---------- server : StrictRedis,Redis client instance. key : str,Redis key where to store items. serialize_func : callable,Items serializer function. """ self.server = server self.key = key # defaults.PIPELINE_KEY = '%(spider)s:items',不同的爬虫用不同的key self.serialize = serialize_func @classmethod def from_settings(cls, settings): params = &#123; 'server': connection.from_settings(settings), &#125; if settings.get('REDIS_ITEMS_KEY'): params['key'] = settings['REDIS_ITEMS_KEY'] if settings.get('REDIS_ITEMS_SERIALIZER'): params['serialize_func'] = load_object( settings['REDIS_ITEMS_SERIALIZER'] ) return cls(**params) @classmethod def from_crawler(cls, crawler): return cls.from_settings(crawler.settings) def process_item(self, item, spider): return deferToThread(self._process_item, item, spider) def _process_item(self, item, spider): key = self.item_key(item, spider) data = self.serialize(item) self.server.rpush(key, data) return item def item_key(self, item, spider): """Returns redis key based on given spider. Override this function to use a different key depending on the item and/or spider. """ return self.key % &#123;'spider': spider.name&#125; Queue.py 实现消息队列 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139from scrapy.utils.reqser import request_to_dict, request_from_dictfrom . import picklecompatclass Base(object): """Per-spider base queue class""" def __init__(self, server, spider, key, serializer=None): """Initialize per-spider redis queue. ---------- server : StrictRedis,Redis client instance. spider : Spider,Scrapy spider instance. key: str, Redis key where to put and get messages. serializer : object, Serializer object with ``loads`` and ``dumps`` methods. """ if serializer is None: # Backward compatibility. # TODO: deprecate pickle. serializer = picklecompat if not hasattr(serializer, 'loads'): raise TypeError("serializer does not implement 'loads' function: %r" % serializer) if not hasattr(serializer, 'dumps'): raise TypeError("serializer '%s' does not implement 'dumps' function: %r" % serializer) self.server = server self.spider = spider self.key = key % &#123;'spider': spider.name&#125; self.serializer = serializer def _encode_request(self, request): """Encode a request object""" obj = request_to_dict(request, self.spider) return self.serializer.dumps(obj) def _decode_request(self, encoded_request): """Decode an request previously encoded""" obj = self.serializer.loads(encoded_request) return request_from_dict(obj, self.spider) def __len__(self): """Return the length of the queue""" raise NotImplementedError def push(self, request): """Push a request""" raise NotImplementedError def pop(self, timeout=0): """Pop a request""" raise NotImplementedError def clear(self): """Clear queue/stack""" self.server.delete(self.key)class FifoQueue(Base): """Per-spider FIFO queue""" def __len__(self): """Return the length of the queue""" return self.server.llen(self.key) def push(self, request): """Push a request""" self.server.lpush(self.key, self._encode_request(request)) def pop(self, timeout=0): """Pop a request""" if timeout &gt; 0: data = self.server.brpop(self.key, timeout) if isinstance(data, tuple): data = data[1] else: data = self.server.rpop(self.key) if data: return self._decode_request(data)class PriorityQueue(Base): """Per-spider priority queue abstraction using redis' sorted set""" def __len__(self): """Return the length of the queue""" return self.server.zcard(self.key) def push(self, request): """Push a request""" data = self._encode_request(request) score = -request.priority # We don't use zadd method as the order of arguments change depending on # whether the class is Redis or StrictRedis, and the option of using # kwargs only accepts strings, not bytes. self.server.execute_command('ZADD', self.key, score, data) def pop(self, timeout=0): """ Pop a request timeout not support in this queue class """ # use atomic range/remove using multi/exec pipe = self.server.pipeline() pipe.multi() pipe.zrange(self.key, 0, 0).zremrangebyrank(self.key, 0, 0) results, count = pipe.execute() if results: return self._decode_request(results[0])class LifoQueue(Base): """Per-spider LIFO queue.""" def __len__(self): """Return the length of the stack""" return self.server.llen(self.key) def push(self, request): """Push a request""" self.server.lpush(self.key, self._encode_request(request)) def pop(self, timeout=0): """Pop a request""" if timeout &gt; 0: data = self.server.blpop(self.key, timeout) if isinstance(data, tuple): data = data[1] else: data = self.server.lpop(self.key) if data: return self._decode_request(data)# TODO: Deprecate the use of these names.SpiderQueue = FifoQueue # 先进先出SpiderStack = LifoQueue # 后进先出SpiderPriorityQueue = PriorityQueue # 优先级队列 scheduler.py 调度，对request查重，为spider分配request任务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180import importlibimport sixfrom scrapy.utils.misc import load_objectfrom . import connection, defaults# TODO: add SCRAPY_JOB support.class Scheduler(object): """Redis-based scheduler Settings -------- SCHEDULER_PERSIST : bool (default: False) Whether to persist or clear redis queue. SCHEDULER_FLUSH_ON_START : bool (default: False) Whether to flush redis queue on start. SCHEDULER_IDLE_BEFORE_CLOSE : int (default: 0) How many seconds to wait before closing if no message is received. SCHEDULER_QUEUE_KEY : str Scheduler redis key. SCHEDULER_QUEUE_CLASS : str Scheduler queue class. SCHEDULER_DUPEFILTER_KEY : str Scheduler dupefilter redis key. SCHEDULER_DUPEFILTER_CLASS : str Scheduler dupefilter class. SCHEDULER_SERIALIZER : str Scheduler serializer. """ def __init__(self, server, persist=False, flush_on_start=False, queue_key=defaults.SCHEDULER_QUEUE_KEY, queue_cls=defaults.SCHEDULER_QUEUE_CLASS, dupefilter_key=defaults.SCHEDULER_DUPEFILTER_KEY, dupefilter_cls=defaults.SCHEDULER_DUPEFILTER_CLASS, idle_before_close=0, serializer=None): """Initialize scheduler. Parameters ---------- server : Redis The redis server instance. persist : bool Whether to flush requests when closing. Default is False. flush_on_start : bool Whether to flush requests on start. Default is False. queue_key : str Requests queue key. queue_cls : str Importable path to the queue class. dupefilter_key : str Duplicates filter key. dupefilter_cls : str Importable path to the dupefilter class. idle_before_close : int Timeout before giving up. """ if idle_before_close &lt; 0: raise TypeError("idle_before_close cannot be negative") self.server = server self.persist = persist self.flush_on_start = flush_on_start self.queue_key = queue_key self.queue_cls = queue_cls self.dupefilter_cls = dupefilter_cls self.dupefilter_key = dupefilter_key self.idle_before_close = idle_before_close self.serializer = serializer self.stats = None def __len__(self): return len(self.queue) @classmethod def from_settings(cls, settings): kwargs = &#123; 'persist': settings.getbool('SCHEDULER_PERSIST'), 'flush_on_start': settings.getbool('SCHEDULER_FLUSH_ON_START'), 'idle_before_close': settings.getint('SCHEDULER_IDLE_BEFORE_CLOSE'), &#125; # If these values are missing, it means we want to use the defaults. optional = &#123; # TODO: Use custom prefixes for this settings to note that are # specific to scrapy-redis. 'queue_key': 'SCHEDULER_QUEUE_KEY', 'queue_cls': 'SCHEDULER_QUEUE_CLASS', 'dupefilter_key': 'SCHEDULER_DUPEFILTER_KEY', # We use the default setting name to keep compatibility. 'dupefilter_cls': 'DUPEFILTER_CLASS', 'serializer': 'SCHEDULER_SERIALIZER', &#125; for name, setting_name in optional.items(): val = settings.get(setting_name) if val: kwargs[name] = val # Support serializer as a path to a module. if isinstance(kwargs.get('serializer'), six.string_types): kwargs['serializer'] = importlib.import_module(kwargs['serializer']) server = connection.from_settings(settings) # Ensure the connection is working. server.ping() return cls(server=server, **kwargs) @classmethod def from_crawler(cls, crawler): instance = cls.from_settings(crawler.settings) # FIXME: for now, stats are only supported from this constructor instance.stats = crawler.stats return instance def open(self, spider): self.spider = spider try: # 根据对象名字创建一个对象 self.queue = load_object(self.queue_cls)( server=self.server, spider=spider, key=self.queue_key % &#123;'spider': spider.name&#125;, serializer=self.serializer, ) except TypeError as e: raise ValueError("Failed to instantiate queue class '%s': %s", self.queue_cls, e) try: self.df = load_object(self.dupefilter_cls)( server=self.server, key=self.dupefilter_key % &#123;'spider': spider.name&#125;, debug=spider.settings.getbool('DUPEFILTER_DEBUG'), ) except TypeError as e: raise ValueError("Failed to instantiate dupefilter class '%s': %s", self.dupefilter_cls, e) if self.flush_on_start: self.flush() # notice if there are requests already in the queue to resume the crawl if len(self.queue): spider.log("Resuming crawl (%d requests scheduled)" % len(self.queue)) def close(self, reason): if not self.persist: self.flush() def flush(self): self.df.clear() self.queue.clear() def enqueue_request(self, request): # 如果需要检测去重，且检测到有重复，返回False if not request.dont_filter and self.df.request_seen(request): self.df.log(request, self.spider) return False if self.stats: self.stats.inc_value('scheduler/enqueued/redis', spider=self.spider) # 将request入队，返回True self.queue.push(request) return True def next_request(self): block_pop_timeout = self.idle_before_close # 分发request request = self.queue.pop(block_pop_timeout) if request and self.stats: self.stats.inc_value('scheduler/dequeued/redis', spider=self.spider) return request def has_pending_requests(self): return len(self) &gt; 0 spiders.py 爬虫类， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188from scrapy import signalsfrom scrapy.exceptions import DontCloseSpiderfrom scrapy.spiders import Spider, CrawlSpiderfrom . import connection, defaultsfrom .utils import bytes_to_str# Mixin 混合类型，利用多重继承来简洁的实现组合模式class RedisMixin(object): """Mixin class to implement reading urls from a redis queue.""" redis_key = None redis_batch_size = None redis_encoding = None # Redis client placeholder. server = None def start_requests(self): """Returns a batch of start requests from redis.""" return self.next_requests() def setup_redis(self, crawler=None): """Setup redis connection and idle signal. This should be called after the spider has set its crawler object. """ if self.server is not None: return if crawler is None: # We allow optional crawler argument to keep backwards # compatibility. # XXX: Raise a deprecation warning. crawler = getattr(self, 'crawler', None) if crawler is None: raise ValueError("crawler is required") settings = crawler.settings if self.redis_key is None: self.redis_key = settings.get( 'REDIS_START_URLS_KEY', defaults.START_URLS_KEY, ) self.redis_key = self.redis_key % &#123;'name': self.name&#125; if not self.redis_key.strip(): raise ValueError("redis_key must not be empty") if self.redis_batch_size is None: # TODO: Deprecate this setting (REDIS_START_URLS_BATCH_SIZE). self.redis_batch_size = settings.getint( 'REDIS_START_URLS_BATCH_SIZE', settings.getint('CONCURRENT_REQUESTS'), ) try: self.redis_batch_size = int(self.redis_batch_size) except (TypeError, ValueError): raise ValueError("redis_batch_size must be an integer") if self.redis_encoding is None: self.redis_encoding = settings.get('REDIS_ENCODING', defaults.REDIS_ENCODING) self.logger.info("Reading start URLs from redis key '%(redis_key)s' " "(batch size: %(redis_batch_size)s, encoding: %(redis_encoding)s", self.__dict__) self.server = connection.from_settings(crawler.settings) # The idle signal is called when the spider has no requests left, # that's when we will schedule new requests from redis queue crawler.signals.connect(self.spider_idle, signal=signals.spider_idle) def next_requests(self): """Returns a request to be scheduled or none.""" use_set = self.settings.getbool('REDIS_START_URLS_AS_SET', defaults.START_URLS_AS_SET) fetch_one = self.server.spop if use_set else self.server.lpop # XXX: Do we need to use a timeout here? found = 0 # TODO: Use redis pipeline execution. while found &lt; self.redis_batch_size: data = fetch_one(self.redis_key) # 取出一条数据 if not data: # Queue empty. break req = self.make_request_from_data(data) if req: yield req found += 1 else: self.logger.debug("Request not made from data: %r", data) if found: self.logger.debug("Read %s requests from '%s'", found, self.redis_key) def make_request_from_data(self, data): """Returns a Request instance from data coming from Redis. By default, ``data`` is an encoded URL. You can override this method to provide your own message decoding. Parameters ---------- data : bytes Message from redis. """ url = bytes_to_str(data, self.redis_encoding) return self.make_requests_from_url(url) def schedule_next_requests(self): """Schedules a request if available""" # TODO: While there is capacity, schedule a batch of redis requests. for req in self.next_requests(): self.crawler.engine.crawl(req, spider=self) # 当爬虫空闲时 def spider_idle(self): """Schedules a request if available, otherwise waits.""" # XXX: Handle a sentinel to close the spider. self.schedule_next_requests() raise DontCloseSpiderclass RedisSpider(RedisMixin, Spider): """Spider that reads urls from redis queue when idle. Attributes ---------- redis_key : str (default: REDIS_START_URLS_KEY) Redis key where to fetch start URLs from.. redis_batch_size : int (default: CONCURRENT_REQUESTS) Number of messages to fetch from redis on each attempt. redis_encoding : str (default: REDIS_ENCODING) Encoding to use when decoding messages from redis queue. Settings -------- REDIS_START_URLS_KEY : str (default: "&lt;spider.name&gt;:start_urls") Default Redis key where to fetch start URLs from.. REDIS_START_URLS_BATCH_SIZE : int (deprecated by CONCURRENT_REQUESTS) Default number of messages to fetch from redis on each attempt. REDIS_START_URLS_AS_SET : bool (default: False) Use SET operations to retrieve messages from the redis queue. If False, the messages are retrieve using the LPOP command. REDIS_ENCODING : str (default: "utf-8") Default encoding to use when decoding messages from redis queue. """ @classmethod def from_crawler(self, crawler, *args, **kwargs): obj = super(RedisSpider, self).from_crawler(crawler, *args, **kwargs) obj.setup_redis(crawler) return objclass RedisCrawlSpider(RedisMixin, CrawlSpider): """Spider that reads urls from redis queue when idle. Attributes ---------- redis_key : str (default: REDIS_START_URLS_KEY) Redis key where to fetch start URLs from.. redis_batch_size : int (default: CONCURRENT_REQUESTS) Number of messages to fetch from redis on each attempt. redis_encoding : str (default: REDIS_ENCODING) Encoding to use when decoding messages from redis queue. Settings -------- REDIS_START_URLS_KEY : str (default: "&lt;spider.name&gt;:start_urls") Default Redis key where to fetch start URLs from.. REDIS_START_URLS_BATCH_SIZE : int (deprecated by CONCURRENT_REQUESTS) Default number of messages to fetch from redis on each attempt. REDIS_START_URLS_AS_SET : bool (default: True) Use SET operations to retrieve messages from the redis queue. REDIS_ENCODING : str (default: "utf-8") Default encoding to use when decoding messages from redis queue. """ @classmethod def from_crawler(self, crawler, *args, **kwargs): obj = super(RedisCrawlSpider, self).from_crawler(crawler, *args, **kwargs) obj.setup_redis(crawler) return obj]]></content>
      <categories>
        <category>技术</category>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy线性运算]]></title>
    <url>%2F2017%2F02%2F28%2F%E6%8A%80%E6%9C%AF%2Fnumpy-liner-skills%2F</url>
    <content type="text"><![CDATA[线性代数 解方程组 Numpy和Scipy模块 Numpy进行简单的描述性统计计算 Numpy进行线性代数运算 Numpy计算特征值和特征向量 Numpy随机数 创建掩码式Numpy数组 描述性统计计算12345678910# 计算numpy数组的平均值，中位数，最大值，最小值和标准差import numpy as npfrom scipy.stats import scoreatpercentile # 就算第N%的数值data = np.loadtxt('ex1.csv', delimiter=',', usecols=(1,), skiprows=1, unpack=True)print dataprint "Max:", data.max(), np.max(data)print "Min:", data.min(), np.min(data)print "Mean:", data.mean(), np.mean(data)print "Std方差:", data.std(), np.std(data)print "Median:", np.median(data), scoreatpercentile(data, 50) [ 2. 6. 10.] Max: 10.0 10.0 Min: 2.0 2.0 Mean: 6.0 6.0 Std方差: 3.26598632371 3.26598632371 Median: 6.0 6.0 线性代数运算 numpy.linalg 或 scipy.linalg 矩阵求逆运算，转置，计算特征值，求解线性方程组或计算行列式矩阵可用numpy.ndarray的一个子类表示。 1234567891011121314# 创建一个矩阵A = np.mat("2 4 6;4 2 6;10 -4 18")print A# 矩阵求逆A_inverse = np.linalg.inv(A)print A_inverseprint "如果矩阵是不可逆的，可以用pinv求伪逆矩阵"A_inverse = np.linalg.pinv(A)print A_inverse# A*A- = IX = A * A_inverseprint X# 计算误差print X - np.eye(3) [[ 2 4 6] [ 4 2 6] [10 -4 18]] [[-0.41666667 0.66666667 -0.08333333] [ 0.08333333 0.16666667 -0.08333333] [ 0.25 -0.33333333 0.08333333]] 如果矩阵是不可逆的，可以用pinv求伪逆矩阵 [[-0.41666667 0.66666667 -0.08333333] [ 0.08333333 0.16666667 -0.08333333] [ 0.25 -0.33333333 0.08333333]] [[ 1.00000000e+00 -1.66533454e-15 4.44089210e-16] [ 6.66133815e-16 1.00000000e+00 3.88578059e-16] [ 1.11022302e-15 -1.88737914e-15 1.00000000e+00]] [[ 8.88178420e-16 -1.66533454e-15 4.44089210e-16] [ 6.66133815e-16 -1.22124533e-15 3.88578059e-16] [ 1.11022302e-15 -1.88737914e-15 8.88178420e-16]] 求解线性方程组， solve() 和 dot() 12345678A = np.mat("1 -2 1;0 2 -8;-4 5 9")b = np.array([0, 8, 9])# Ax=b，求x，用np.linalg.solvex = np.linalg.solve(A, b)print x# Ax=b,求解b，用np.dotb1 = np.dot(A, x)print b1 [ 155. 88. 21.] [[ 0. 8. 9.]] 若存在常数λ及n维非零向量X，使得Ax=λx，则称λ是矩阵A的特征值，x是A属于特征值λ的特征向量.λ = np.linalg.eigvals(A)(λ, x) = np.linalg.eig(A) 12345678A = np.mat("3,-2;1,0")a, x = np.linalg.eig(A)b = np.linalg.eigvals(A)print a, b, x# 验证Ax=λxfor i in range(len(x)): print 'A*x&#123;&#125;'.format(i), np.dot(A, x[:, i]) print 'a*x&#123;&#125;'.format(i), a[i] * x[:, i] [ 2. 1.] [ 2. 1.] [[ 0.89442719 0.70710678] [ 0.4472136 0.70710678]] A*x0 [[ 1.78885438] [ 0.89442719]] a*x0 [[ 1.78885438] [ 0.89442719]] A*x1 [[ 0.70710678] [ 0.70710678]] a*x1 [[ 0.70710678] [ 0.70710678]] numpy随机数，随机相关函数位于np.random模块。连续分布：正态分布和对数正态分布离散分布：几何分布，超几何分布和二项分布 123456789101112131415161718192021222324# 用np.random.binomial()模拟二项分布,提供相同的种子，则产生的随机结果相同from matplotlib import pyplot as plt%matplotlib inline# B(0.5, 9) 生成执行10000次的博弈结果序列 outcome = np.random.binomial(9, 0.5, size=10000)plt.subplot(121)plt.plot(np.arange(10000), outcome)money = np.zeros(10000)my_money = 1000i = 0for x in outcome: if x &lt; 5: money[i] = my_money - 1 elif x &lt; 10: money[i] = my_money + 1 my_money = money[i] i = i + 1 print moneyplt.subplot(122)plt.plot(np.arange(10000), money)plt.show() [ 999. 1000. 999. ..., 972. 971. 970.] ​ 正态分布采样np.random模块提供了很多表示连续随机分布的函数 Draw random samples from a normal (Gaussian) distribution. np.random.normal(loc, scale, size)loc : float,Mean (“centre”) of the distribution.scale : float,Standard deviation (spread or “width”) of the distribution.size : int or tuple of ints, optional,Output shape. 12345678N = 10000normal_values = np.random.normal(size=N)dummy, bins, dummy = plt.hist(normal_values, np.sqrt(N), normed=True, lw=1) # dunmmy中文蠢货，假的；bins箱子，也就是直方图的条条sigma = 1mu = 0fx = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp(- (bins - mu) ** 2 / (2 * sigma ** 2))plt.plot(bins, fx, lw=2) ## lw: linewidthplt.show() 正态检测：检查样本是否符合正态分布scipy.stats里实现了一些正态分布的检测方法。 创建掩码式数组掩码式数组可以忽略无效的、残缺的数据。如忽略负值和极值]]></content>
      <categories>
        <category>技术</category>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib笔记]]></title>
    <url>%2F2017%2F02%2F26%2F%E6%8A%80%E6%9C%AF%2Fuse-matplotlib%2F</url>
    <content type="text"><![CDATA[matplotlib matplotlib API pandas中绘图函数 1%pylab Using matplotlib backend: Qt5Agg Populating the interactive namespace from numpy and matplotlib 123import matplotlib.pyplot as pltimport numpy as npimport pandas as pd 1%matplotlib inline Figure 和Subplot创建一个figure后，还需要在上面添加子图，然后调用子图的绘图方法绘图。直接使用plot绘图，图像绘制在最后一次选定的子图 1234567891011from numpy.random import randnfig = plt.figure()sp1 = fig.add_subplot(2, 2, 1) # 2*2布局的第一个子图sp1.plot(randn(50).cumsum(), 'k--') # k--黑色虚线图sp2 = fig.add_subplot(2, 2, 2)sp2.hist(randn(100), bins=20, color='k', alpha=0.3) ## 直方图，20个箱子sp3 = fig.add_subplot(2, 2, 3)sp3.scatter(np.arange(30), np.arange(30) + randn(30) * 3, color = 'b')# sp4 = fig.add_subplot(2, 2, 4) 12345# 使用plt.subplot()创建子图fig, axes = plt.subplots(2, 2) ## 返回画布和子图对象的数组axes[0][0].plot(range(10), range(10))## 调整子图间的距离, wspace和hspace 子图间百分比fig.subplots_adjust(left=1, right=2, wspace=0, hspace=0) 颜色、标记、线型plot函数接受一组X和y坐标，还有格式字符串。表示颜色和线型的字符串,如“g–”,表示绿色的折线“go–”, 绿色O点折线，marker=’o’也可以在plot中指定参数 linestyle=’–’ color=’g’ color值可以用缩写词或RGB值，“#CECECE” 线形图的点可以加上标记marker，更容易看出点的位置 label 图线的label drawstyle 绘图方式 fig的方法 fig.xlabel(‘xxxx’) fig.ylabel(‘y’) fig.xlim() 1234567plt.plot(randn(30).cumsum(), 'go--', label='yyy')plt.plot(randn(30).cumsum(), color='b', marker='*', linestyle='--', label='xxx')plt.plot(randn(30).cumsum(), color='r', drawstyle='steps-post', label='zzz')plt.legend(loc='best') # 图例，label的位置为bestplt.xlabel('X')plt.ylabel('random')plt.show() 12plt.xlim() # 当前x轴的范围plt.xlim([-10,10]) # 调整x轴的范围 (-10, 10) 123456#### 注解以及在subplot上绘图ax = plt.subplot(1, 1, 1)ax.plot(range(10), range(3, 13), 'g*--', label='line')ax.legend('best')ax.text(2, 5, 'Here!', family='monospace', fontsize=10)ax.annotate('Look Here!', xy=(3, 6), arrowprops=dict(facecolor='black')) #,family='monospace', fontsize=10) pandas中的绘图函数线型图Series和Dataframe的plot方法默认生成的是线型图 柱状图添加参数kind=bar或barh，生成柱状图和水平方向的柱状图 直方图和密度图直方图是一种可以对值频率进行离散化显示的柱状图，plot.hist()密度图：计算“可能会产生观测数据的连续发布的估计”，一般是将该分布近似为一组核（如高斯正态分布）分布。 散布图观察两组一维数据之间关系的有效方法 123from pandas import Series, DataFrames = Series(randn(10).cumsum(), index=range(0, 100, 10))s.plot() 12345678fig, axes = plt.subplots(2, 2)s.plot(ax=axes[0][0])frame = DataFrame(randn(10, 4).cumsum(0), columns=list('ABCD'), index=range(0, 100, 10))frame.plot(ax=axes[0][1])frame.plot(ax=axes[0][1])### 柱状图s.plot(ax=axes[1][0], kind='barh')frame.plot(ax=axes[1][1], kind='bar') 123456fig, axes = plt.subplots(2, 2)## 堆积柱状图s.plot(ax=axes[0][0], kind='bar', stacked=True, alpha=0.3) frame.plot(ax=axes[0][1], kind='bar', stacked=True, alpha=0.5)s.value_counts().plot(ax=axes[1][0], kind='bar', stacked=True, alpha=0.3)frame.ix[:, 2:8].plot(ax=axes[1][1], kind='bar', stacked=True, alpha=0.5) 123456fig, axes = plt.subplots(1, 2)s = Series(randn(100).cumsum(), index=range(0, 1000, 10))# 直方图s.hist(ax=axes[0], bins=40)# 密度图s.plot(ax=axes[1], kind='kde', color='k') 12s.hist(bins=40, normed=True)s.plot(kind='kde', style='g--') 12### 散点图plt.scatter(frame['A'], frame['D']) 1pd.scatter_matrix(frame, diagonal='kde') array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021169438&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x00000000213EB4A8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x00000000214B22B0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021559710&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021667208&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000002174F978&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000002181B5F8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x00000000218DDCC0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021A2D9B0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021AA00B8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021B9DE80&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021BBE278&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021D61400&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021E70E10&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000021F677F0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000000022036240&gt;]], dtype=object)]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas学习笔记]]></title>
    <url>%2F2017%2F02%2F25%2F%E6%8A%80%E6%9C%AF%2FPandas-skills%2F</url>
    <content type="text"><![CDATA[Pandas数据结构基本功能汇总和计算描述统计处理缺失数据层次化索引其他 123from pandas import DataFrame, Seriesimport pandas as pdimport numpy as np Series12345678910# series类似一维数组，由一组数据和数据的标签（索引）组成s = Series([11, 32, -223, 114])print sprint s.indexprint s.values# 索引可以直接赋值修改s.index = ['a', 'b', 'c', 'd']print ss2 = Series([11, 32, -223, 114], index=['a', 'b', 'c', 'd'])print s2['a'], '\n', s2[['a', 'd']] 0 11 1 32 2 -223 3 114 dtype: int64 RangeIndex(start=0, stop=4, step=1) [ 11 32 -223 114] a 11 b 32 c -223 d 114 dtype: int64 11 a 11 d 114 dtype: int64 Numpy数组运算的结果会保留索引和值之间的链接 根据布尔型数组进行过滤，标量乘法，数学函数等 1234567891011121314print s2 &gt; 11 ## 得到布尔型数组print s2[s2 &gt; 11] ## 根据布尔型数组筛选对应为true的元素## 可以将series理解为定长字典，因为是key-value的对应关系，series也可以由字典对象创建d = &#123;'Alice': 98, 'Bob': 87, 'Cc': 67&#125;score1 = Series(d)print score1print 'Alice' in score1## 同时传入字典和index索引student = ['Alice', 'Cc', 'Sam', 'Wandou']score2 = Series(d, index=student) # 会根据index查找字典中相应的数据，找不到的对应index的值为NaN，表示缺失或非数字print score2print score2.isnull()print score2.notnull() a False b True c False d True dtype: bool b 32 d 114 dtype: int64 Alice 98 Bob 87 Cc 67 dtype: int64 True Alice 98.0 Cc 67.0 Sam NaN Wandou NaN dtype: float64 Alice False Cc False Sam True Wandou True dtype: bool Alice True Cc True Sam False Wandou False dtype: bool 12345678## 在算术运算中，会自动对其不同索引的数据print score1, score2print score1 + score2 ## 索引和数据都有一个name属性score2.name = 'score'score2.index.name = 'name'print score2 Alice 98 Bob 87 Cc 67 dtype: int64 Alice 98.0 Cc 67.0 Sam NaN Wandou NaN dtype: float64 Alice 196.0 Bob NaN Cc 134.0 Sam NaN Wandou NaN dtype: float64 name Alice 98.0 Cc 67.0 Sam NaN Wandou NaN Name: score, dtype: float64 DataFramedataframe是一种表格型的数据结构，含有一组有序的列，每列的数值类型可以不同。dataframe既有行索引，又有列索引，可以看做Series组成的字典。 1234567891011121314151617181920212223242526## 构建DataFrame，传入等长的列表或Numpy数组province = ['Beijing', 'Shandong', 'Shanghai', 'Hubei', 'Hunan']area = [50, 156, 300, 250, 260]pop = [5000, 16000, 9000, 10000, 8000]data = &#123;'province': province, 'people': pop, 'area': area&#125;frame1 = DataFrame(data)print frame1# 指定列的排列顺序frame2 = DataFrame(data, columns=['province', 'area', 'people'])print frame2# 加入自定义索引frame3 = DataFrame(data, columns=['province', 'area', 'people'], index=data['province'])print frame3# 获取列print frame3['area']print frame3.area# 获取行print frame3.ix['Shandong']# 添加一列或修改一列，frame3['GDP_Unit'] = '亿元' # 一列所有数据都改为该值frame3['GDP'] = [150, 160, 170, 180, 190] print frame3frame3.GDP = Series(&#123;'Beijing': 0, 'Shandong': -1, 'Shanghai': 2, 'hah': 90&#125;)print frame3# 如果值是列表，要等长。# 如果是Series，则严格按照Series的索引赋值该列，series中无相应index的，frame中该值改为Na area people province 0 50 5000 Beijing 1 156 16000 Shandong 2 300 9000 Shanghai 3 250 10000 Hubei 4 260 8000 Hunan province area people 0 Beijing 50 5000 1 Shandong 156 16000 2 Shanghai 300 9000 3 Hubei 250 10000 4 Hunan 260 8000 province area people Beijing Beijing 50 5000 Shandong Shandong 156 16000 Shanghai Shanghai 300 9000 Hubei Hubei 250 10000 Hunan Hunan 260 8000 Beijing 50 Shandong 156 Shanghai 300 Hubei 250 Hunan 260 Name: area, dtype: int64 Beijing 50 Shandong 156 Shanghai 300 Hubei 250 Hunan 260 Name: area, dtype: int64 province Shandong area 156 people 16000 Name: Shandong, dtype: object province area people GDP_Unit GDP Beijing Beijing 50 5000 亿元 150 Shandong Shandong 156 16000 亿元 160 Shanghai Shanghai 300 9000 亿元 170 Hubei Hubei 250 10000 亿元 180 Hunan Hunan 260 8000 亿元 190 province area people GDP_Unit GDP Beijing Beijing 50 5000 亿元 0.0 Shandong Shandong 156 16000 亿元 -1.0 Shanghai Shanghai 300 9000 亿元 2.0 Hubei Hubei 250 10000 亿元 NaN Hunan Hunan 260 8000 亿元 NaN 1frame3.GDP.isnull() Beijing False Shandong False Shanghai False Hubei True Hunan True Name: GDP, dtype: bool 123## 删除一列del frame3['GDP']print frame3.columns Index([u&apos;province&apos;, u&apos;area&apos;, u&apos;people&apos;, u&apos;GDP_Unit&apos;], dtype=&apos;object&apos;) ​ 使用嵌套字典创建dataframe 外层的字典key作为列索引，内层的字典key作为行索引。frame可以转置。内层的字典key会被合并、排序最终成为行索引，显示指定了index的除外 123456789data = &#123;'province': &#123;'Shandong': 'Shangdong', 'Beijing': 'Beijing', 'Hubei': 'Hubei'&#125;, 'GDP': &#123;'Shanghai': 15000, 'Shandong': 20000, 'Hubei': 7000&#125;, 'people': &#123;'Shandong': 100000, 'Hubei': 20000&#125; &#125;frame = DataFrame(data) ## 缺省的值设为NaNprint frameframe = DataFrame(data, index=['Guangzhou', 'Beijing', 'Shanghai', 'Shandong'])print frameprint frame.T GDP people province Beijing NaN NaN Beijing Hubei 7000.0 20000.0 Hubei Shandong 20000.0 100000.0 Shangdong Shanghai 15000.0 NaN NaN GDP people province Guangzhou NaN NaN NaN Beijing NaN NaN Beijing Shanghai 15000.0 NaN NaN Shandong 20000.0 100000.0 Shangdong Guangzhou Beijing Shanghai Shandong GDP NaN NaN 15000 20000 people NaN NaN NaN 100000 province NaN Beijing NaN Shangdong 可以用于构造DataFrame的数据 二维ndarray 值为数组，列表或元组的字典， 每个数据序列必须等长，作为dataframe的一列 值为Series的字典， 每个series成为frame的一列，如果没有指定index索引，则series内的索引会成为行索引 嵌套字典 。。。 1frame3.values ## frame的values是ndarray， ndarray的数据类型会选择能兼容各列数据的类型 array([[&apos;Beijing&apos;, 50L, 5000L, &apos;\xe4\xba\xbf\xe5\x85\x83&apos;], [&apos;Shandong&apos;, 156L, 16000L, &apos;\xe4\xba\xbf\xe5\x85\x83&apos;], [&apos;Shanghai&apos;, 300L, 9000L, &apos;\xe4\xba\xbf\xe5\x85\x83&apos;], [&apos;Hubei&apos;, 250L, 10000L, &apos;\xe4\xba\xbf\xe5\x85\x83&apos;], [&apos;Hunan&apos;, 260L, 8000L, &apos;\xe4\xba\xbf\xe5\x85\x83&apos;]], dtype=object) 索引对象Index对象的值是不能不被修改的包含的方法有 append 连接一个index对象 diff 计算差集，并得到一个index intersect 计算交集 union 计算并集 isin 计算index各值是否在给定的集合内，返回一个布尔型数组 delete 删除索引i处的元素，并得到新的index drop insert is_monotonic 是否单调 is_unique 是否有重复值 unique 计算index中唯一值得数组 123456## 重新索引对象, 生成新的series或frameprint s.indexq = s.reindex(['e', 'd', 'c', 'b', 'a']) # 另外一个可选参数，fill_value=0设置默认值或method='填充方法'，ffill、pad向前填充，bfill或backfill向后填充print qp = frame3.reindex(index=[], columns=[], method='ffill') # 可以同时重新index和column，值填充只能按行应用 Index([u&apos;a&apos;, u&apos;b&apos;, u&apos;c&apos;, u&apos;d&apos;], dtype=&apos;object&apos;) e NaN d 114.0 c -223.0 b 32.0 a 11.0 dtype: float64 索引、选取和过滤 行列索引 布尔型dataframe索引 1234print frame3['area']print frame3[:4] # 选取的列print frame3[frame3['area'] &gt; 160] # 根据布尔型选取frame3.ix[:2, ['area', 'people']] # 同时选取行和列 Beijing 50 Shandong 156 Shanghai 300 Hubei 250 Hunan 260 Name: area, dtype: int64 province area people GDP_Unit Beijing Beijing 50 5000 亿元 Shandong Shandong 156 16000 亿元 Shanghai Shanghai 300 9000 亿元 Hubei Hubei 250 10000 亿元 province area people GDP_Unit Shanghai Shanghai 300 9000 亿元 Hubei Hubei 250 10000 亿元 Hunan Hunan 260 8000 亿元 area people Beijing 50 5000 Shandong 156 16000 12print frame3.iloc[1,:2] # 根据位置整数选取print frame3.loc[:, ['area', 'people']] # 同时根据行列lable选取 province Shandong area 156 Name: Shandong, dtype: object area people Beijing 50 5000 Shandong 156 16000 Shanghai 300 9000 Hubei 250 10000 Hunan 260 8000 算数运算和数据对齐计算是在两个对象索引的并集上运算，相同索引的值进行运算，不重叠的索引处引入NaN值。缺省值NaN在运算中会传播。在运算add、sub、div、mul中传入参数fill_value=0可以指定一个填充值。DataFrame和Series进行运算时，会产生广播。 函数应用和映射Numpy的元素级函数可用于操作Pandas对象 np.abs(frame) 和其他函数 frame.apply(funcs) frame.applymap(funcs) series.map() 123456789101112131415frame = DataFrame(np.random.randn(4, 3), columns=list('abc'), index=range(1, 5))frame2 = np.abs(frame)print frameprint frame2f1 = lambda x: x.max()print frame.apply(f1) # 将函数应用到frame的列print frame.max() ## 很多统计函数已被实现，没必要用applyprint frame.apply(f1, axis=1) # 将函数应用于frame的行## apply的函数除了返回值外，也可返回多个值组成的series，framef2 = lambda x: Series([x.min(), x.max()], index=['min', 'max'])print frame.apply(f2)## 如果要在元素级上应用函数，frame使用applymap(),series使用map()f3 = lambda x: '%.2f' %xprint frame.applymap(f3)print frame.a.map(f3) a b c 1 0.811815 0.501822 0.225176 2 0.171434 -1.446678 0.148284 3 -0.187686 -1.017413 1.145296 4 -0.596069 -0.376504 -0.298326 a b c 1 0.811815 0.501822 0.225176 2 0.171434 1.446678 0.148284 3 0.187686 1.017413 1.145296 4 0.596069 0.376504 0.298326 a 0.811815 b 0.501822 c 1.145296 dtype: float64 a 0.811815 b 0.501822 c 1.145296 dtype: float64 1 0.811815 2 0.171434 3 1.145296 4 -0.298326 dtype: float64 a b c min -0.596069 -1.446678 -0.298326 max 0.811815 0.501822 1.145296 a b c 1 0.81 0.50 0.23 2 0.17 -1.45 0.15 3 -0.19 -1.02 1.15 4 -0.60 -0.38 -0.30 1 0.81 2 0.17 3 -0.19 4 -0.60 Name: a, dtype: object 排序和排名123456s = Series(range(5), index=list('qwert'))print s.sort_index(ascending=False)print s.sort_values(ascending=False) # 缺省值会排序放到最后print frame.sort_index(axis=1, ascending=False)print frame.sort_values('a')print frame.rank(method='first') w 1 t 4 r 3 q 0 e 2 dtype: int64 t 4 r 3 e 2 w 1 q 0 dtype: int64 c b a 1 0.225176 0.501822 0.811815 2 0.148284 -1.446678 0.171434 3 1.145296 -1.017413 -0.187686 4 -0.298326 -0.376504 -0.596069 a b c 4 -0.596069 -0.376504 -0.298326 3 -0.187686 -1.017413 1.145296 2 0.171434 -1.446678 0.148284 1 0.811815 0.501822 0.225176 a b c 1 4.0 4.0 3.0 2 3.0 1.0 2.0 3 2.0 2.0 4.0 4 1.0 3.0 1.0 Index索引可以是重复的 选取了重复index的数据，返回的时series，非重复的直接返回其值frame的index也是可以重复的 123s = Series(range(5), index=list('qaqaz'))print s.aprint s.z a 1 a 3 dtype: int64 4]]></content>
      <categories>
        <category>技术</category>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy ImagePipeline下载gif图片和自定义图片文件名称]]></title>
    <url>%2F2017%2F02%2F16%2F%E6%8A%80%E6%9C%AF%2Fscrapy-image-download%2F</url>
    <content type="text"><![CDATA[scrapy图片下载保留gif格式和自定义图片名称继承ImagesPipeline并重写部分函数 get_media_requests 返回下载图片的Request file_path 返回文件名，文件名是sha1哈希image url生成的image_guid convert_image 该函数将图片转化成rgb 模式的jpg格式，避免重新下载近期下载过的图片 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130######################## pipelines.py ######################################## -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.htmlimport pymongofrom scrapy.pipelines.images import ImagesPipelinefrom scrapy.http import Request# from scrapy.exceptions import DropItemfrom PIL import Imagetry: from cStringIO import StringIO as BytesIOexcept ImportError: from io import BytesIOclass JokerPipeline(object): collection_name = 'joke' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get("MONGO_DATABASE", "items") ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): # data = self.db[self.collection_name].find(&#123;&#125;, &#123;'_id': 0, 'url': 1&#125;) # url_finished = set(x['url'] for x in data) # if item['pic-url']: # # raise DropItem("%s has been crawled!" % item) # pic_name = item['data-id'] + '.' + item['pic-url'].plite('.')[-1] # else: self.db[self.collection_name].insert(dict(item)) return item class ImageDownloadPipeline(ImagesPipeline): default_headers = &#123; 'accept': 'image/webp,image/*,*/*;q=0.8', 'accept-encoding': 'gzip, deflate, sdch, br', 'accept-language': 'zh-CN,zh;q=0.8,en;q=0.6', 'cookie': 'bid=yQdC/AzTaCw', 'referer': 'https://www.douban.com/photos/photo/2370443040/', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36', &#125; def get_media_requests(self, item, info): # 下载图片 for image_url in item['pic_url']: self.default_headers['referer'] = image_url yield Request(image_url, headers=self.default_headers, meta=&#123;'item': item&#125;) # # 使用meta是我想用item里的某个字段做图片的名字， def file_path(self, request, response=None, info=None): item = request.meta['item'] # 通过上面的meta传递过来item # 图片文件名，request.url.split('/')[-1].split('.')[-1]得到图片后缀jpg,png,gif image_guid = item['data-id'] + '-' + request.url.split('/')[-1] # 使用有中文的item字段做目录时，用path=''.join(item['no1']),否则路径会变成\u97e9\u56fd\u6c7d\u8f66\u6807\u5fd7\xxx.jpg filename = u'full/&#123;0&#125;'.format(image_guid) return filename # 图片格式转换，如果是gif，则不转换，可能会出问题 def convert_image(self, image, size=None): # 自己添加的 if image.format == 'GIF': buf = BytesIO() image.save(buf, 'GIF') return image, buf # 原始代码 if image.format == 'PNG' and image.mode == 'RGBA': background = Image.new('RGBA', image.size, (255, 255, 255)) background.paste(image, image) image = background.convert('RGB') elif image.mode != 'RGB': image = image.convert('RGB') if size: image = image.copy() image.thumbnail(size, Image.ANTIALIAS) buf = BytesIO() image.save(buf, 'JPEG') return image, buf # 重写下载完图片的处理方法image_downloaded def check_gif(self, image): if image.format == 'GIF': return True # The library reads GIF87a and GIF89a versions of the GIF file format. return image.info.get('version') in ['GIF89a', 'GIF87a'] def persist_gif(self, key, data, info): root, ext = os.path.splitext(key) if not key.endswith(('.gif', '.GIF')): key = key + '.gif' absolute_path = self.store._get_filesystem_path(key) self.store._mkdir(os.path.dirname(absolute_path), info) f = open(absolute_path, 'wb') # use 'b' to write binary data. f.write(data) def image_downloaded(self, response, request, info): checksum = None for key, image, buf in self.get_images(response, request, info): if checksum is None: buf.seek(0) checksum = md5sum(buf) if key.startswith('full') and self.check_gif(image): # Save gif from response directly. self.persist_gif(key, response.body, info) else: width, height = image.size self.store.persist_file( key, buf, info, meta=&#123;'width': width, 'height': height&#125;, headers=&#123;'Content-Type': 'image/jpeg'&#125;) # self.store.persist_image(key, image, buf, info) return checksum]]></content>
      <categories>
        <category>技术</category>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何优雅的使用Linux]]></title>
    <url>%2F2017%2F02%2F12%2F%E6%8A%80%E6%9C%AF%2Fhow-to-use-Linux%2F</url>
    <content type="text"><![CDATA[如何优雅的使用Linux 作为学习/开发环境使用，而不是娱乐。写这边文章是为了下次重装系统快速恢复，系统使用以为简洁、优雅、高效为目标。 1、选择 发行版选择Debian 之前一直使用Ubuntu，大概过那么一两个月就会出现啥问题，然后不得不重装系统，我严重Ubuntu16.04的桌面有bug，开2个软件窗口就卡的爹妈不认了。现在决定再也不用Ubuntu了，改用Debian8后稳定多了，图形桌面很少崩溃。关于centos和openSUSE，只在搭建服务器使用过，个人使用还是更习惯Debian系的。 桌面选择 Ubuntu的unity真tm丑，紫色的主题我也是忍了好久才适应下来。现在改用gnome3。 2、卸载liboffice事实上我连wps也没装，以前是装机必备，最后发现一共使用的次数不超过10次，office那套还是交给windows吧。文档记录使用txt和markdown。 完全卸载sudo apt-get purge libreoffice? 然后安装Typora markdown工具。参考官网, 1234567sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAE# 添加密钥和源在 /etc/apt/sources.list中添加一条'deb https://typora.io ./linux/'sudo apt-get update# 安装 typorasudo apt-get install typora 3、更改主目录文件夹名为英文打开终端，在终端中输入命令: $ export LANG=en_US $ xdg-user-dirs-gtk-update 在弹出的窗口中询问是否将目录转化为英文路径,勾上不再提示并同意更改， $ export LANG=zh_CN $ xdg-user-dirs-gtk-update 关闭终端,并注销或重启.下次进入系统.主目录的中文转英文就完成了~ 4、美化Debian刚装上以后真是丑啊，尤其是白底黑字的shell终端，让人抓狂。 这里美化仅仅是设置gtk主题，gnome-shell主题，改变terminal的颜色等，以美观实用为主，花里胡哨的东西请去windows娱乐。相关文件下载 安装gnome-tweak-tool apt-get install gnome-tweak-tool 这个工具不能用root运行。按下win，搜索tweak，点优化工具打开。 在扩展中启用user-themes，然后gnome shell主题才可用。下面几个插件推荐安装 安装gnome主题 主题可以去网上下载，我直接从Kali Linux上copy出来的。将themes文件夹内容放入/usr/share/themes/下即可，操作之前先备份。 安装gnome-shell主题 备份替换/usr/share/gnome-shell/内容。 字体 备份替换 /usr/share/fonts 内容。如果使用主题后，登陆桌面崩溃，则很有可能缺少字体。 改变壁纸和锁屏、登陆背景 壁纸锁屏可以在设置中修改。 登陆背景，替换gnome-shell主题内的图片，位置 /usr/share/gnome-shell/theme/KaliLogin.png 安装配置terminator 修改终端内的文字高亮，自动代码补全。shell环境变量配置文件有 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# /etc/profile 用户登陆时加载，全局配置，# /etc/bash.bashrc 打开shell时配置shell变量，如果~/.bashrc文件存在则会使用其配置，# ~/.bashrc，仅对该用户有效，user和root下的建议都改# .bashrc文件修改# If not running interactively, don't do anythingcase $- in *i*) ;; *) return;;esac# don't put duplicate lines or lines starting with space in the history.# See bash(1) for more optionsHISTCONTROL=ignoreboth# append to the history file, don't overwrite itshopt -s histappend# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)HISTSIZE=1000HISTFILESIZE=2000# check the window size after each command and, if necessary,# update the values of LINES and COLUMNS.shopt -s checkwinsize# If set, the pattern "**" used in a pathname expansion context will# match all files and zero or more directories and subdirectories.#shopt -s globstar# make less more friendly for non-text input files, see lesspipe(1)#[ -x /usr/bin/lesspipe ] &amp;&amp; eval "$(SHELL=/bin/sh lesspipe)"# set variable identifying the chroot you work in (used in the prompt below)if [ -z "$&#123;debian_chroot:-&#125;" ] &amp;&amp; [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot)fi# set a fancy prompt (non-color, unless we know we "want" color)case "$TERM" in xterm-color) color_prompt=yes;;esac# uncomment for a colored prompt, if the terminal has the capability; turned# off by default to not distract the user: the focus in a terminal window# should be on the output of commands, not on the promptforce_color_prompt=yesif [ -n "$force_color_prompt" ]; then if [ -x /usr/bin/tput ] &amp;&amp; tput setaf 1 &gt;&amp;/dev/null; then # We have color support; assume it's compliant with Ecma-48 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fifiif [ "$color_prompt" = yes ]; then PS1='$&#123;debian_chroot:+($debian_chroot)&#125;\[\033[01;31m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ 'else PS1='$&#123;debian_chroot:+($debian_chroot)&#125;\u@\h:\w\$ 'fiunset color_prompt force_color_prompt# If this is an xterm set the title to user@host:dircase "$TERM" inxterm*|rxvt*) PS1="\[\e]0;$&#123;debian_chroot:+($debian_chroot)&#125;\u@\h: \w\a\]$PS1" ;;*) ;;esac# enable color support of ls and also add handy aliasesif [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors &amp;&amp; eval "$(dircolors -b ~/.dircolors)" || eval "$(dircolors -b)" alias ls='ls --color=auto' #alias dir='dir --color=auto' #alias vdir='vdir --color=auto' #alias grep='grep --color=auto' #alias fgrep='fgrep --color=auto' #alias egrep='egrep --color=auto'fi# some more ls aliases#alias ll='ls -l'#alias la='ls -A'#alias l='ls -CF'# Alias definitions.# You may want to put all your additions into a separate file like# ~/.bash_aliases, instead of adding them here directly.# See /usr/share/doc/bash-doc/examples in the bash-doc package.if [ -f ~/.bash_aliases ]; then . ~/.bash_aliasesfi# enable programmable completion features (you don't need to enable# this, if it's already enabled in /etc/bash.bashrc and /etc/profile# sources /etc/bash.bashrc).if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi 然后右键终端，修改配置文件中背景为透明。背景透明简直太爽了，可以在敲命令时，透过终端看文档或浏览器，而不用将终端窗口挪来挪去。 ​ 5、python开发环境 安装pyenv 自动安装,不推荐 12$ curl -L https://raw.githubusercontent.com/yyuu/pyenv-installer/master/bin/pyenv-installer | bash$ pyenv update 手动git 123456789101112# 安装依赖# sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utilsgit clone git://github.com/yyuu/pyenv.git ~/.pyenv# 用户和root的我都改了,使用了绝对路径，没用$HOMEvim ~/.bashrcexport PYENV_ROOT="/home/syy/.pyenv"export PATH="/home/syy/.pyenv/bin:$PATH"eval "$(pyenv init -)"eval "$(pyenv virtualenv-init -)"source ~/.bashrc# exec $SHELL 安装virtualenv插件 1git clone https://github.com/yyuu/pyenv-virtualenv.git ~/.pyenv/plugins/pyenv-virtualenv 使用 12345678910# 安装# pyenv install --list# pyenv install -v XXX 安装位置/home/syy/.pyenv/versions/# 卸载# pyenv uninstall XXX# pyenv versions (查看所有版本)# pyenv global 3.3.5 切换版本# pyenv virtualenv 2.7.1 newvir 创建虚拟环境# pyenv activate newvir 切换进入newvir# pyenv deactivate 退出当前虚拟环境]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy笔记]]></title>
    <url>%2F2017%2F02%2F11%2F%E6%8A%80%E6%9C%AF%2Fuse-Numpy%2F</url>
    <content type="text"><![CDATA[Numpy数组入门1 Numpy数组对象Numpy数组 数组中的元素类型必须一致 数组的空间大小可以确定，可以运用向量化运算，底层c实现，速度快 数组下标0开始 1234567import numpy as npa = np.arange(5) # 创建0-4的一维数组b = np.array([0, 1, 2, 3, 4])print aprint a.dtype # a 的数据类型print a.dtype.itemsize # 对象数据类型占的字节数a.shape # a的形状，输出元组为每一维的长度 [0 1 2 3 4] int32 4 (5L,) 2 创建多维数组1234m = np.array([a, a, a]) # array的参数为list或list的嵌套listprint mprint m.dtypem.shape [[0 1 2 3 4] [0 1 2 3 4] [0 1 2 3 4]] int32 (3L, 5L) 3 选择numpy数组元素1print m[2, 4], m[0, 0], m[1] # a[m, n]确定第m行n列的元素 4 0 [0 1 2 3 4] 4 Numpy的数值类型123456print m.dtype.typen = np.array(m, dtype='uint32') # 定义数据类型为无符号32位print n.dtypeprint float(62) # 数据类型转换print np.sctypeDict.keys()[-10:] # 部分dtype的字符码print n.dtype.char # n的类型的字符码 &lt;type &apos;numpy.int32&apos;&gt; uint32 62.0 [&apos;a&apos;, &apos;short&apos;, &apos;e&apos;, &apos;i&apos;, &apos;clongfloat&apos;, &apos;m&apos;, &apos;Object0&apos;, &apos;int64&apos;, &apos;i2&apos;, &apos;int0&apos;] L 5 一维数组的切片与索引1print a[1:3], a[2::2], a[::-1] [1 2] [2 4] [4 3 2 1 0] 6 处理数组型状 x.shape x.shape = (m, n) x.rashape(s, m, n) x.resize(s, m, n) x.ravel() x.flatten() x.transpose() 1234567891011121314c = np.arange(24)print c.shapeb = c.reshape(2, 3, 4) # 生成2个3x4的新数组， c不变b.resize(2, 3, 4) # b会改变print be.shape = (3, 8) # 1维数组变成形状为3x8的数组print e, type(e)e = b.ravel() # 将多维数组变为一维数组，返回的是原数组的视图d = b.flatten() # 将多维数组变为一维数组，生成真实的数组，分配存储内存print b, d, eprint b.shape, d.shape, c.shape# 数组转置t = b.transpose()print t (24L,) [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] [[ 0 1 2 3 4 5 6 7] [ 8 9 10 11 12 13 14 15] [16 17 18 19 20 21 22 23]] &lt;type &apos;numpy.ndarray&apos;&gt; [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] (2L, 3L, 4L) (24L,) (24L,) [[[ 0 12] [ 4 16] [ 8 20]] [[ 1 13] [ 5 17] [ 9 21]] [[ 2 14] [ 6 18] [10 22]] [[ 3 15] [ 7 19] [11 23]]] 数组堆叠 x.vstack() x.dstack() x.hstack() x.column_stack() x.row_stack() x.concatenate() 12345678910111213141516171819202122232425262728a = np.arange(9).reshape(3, 3)b = a * 2print a, b# 水平叠加x1 = np.hstack((a, b))x2 = np.concatenate((a, b), axis=1) # axis=1,水平连接；axis=0，垂直连接;默认为0print x1print x2# 垂直叠加x3 = np.vstack((a, b))x4 = np.concatenate((a, b), axis=0)print x3, x4# 深度叠加， 沿着第三个坐标轴方向叠加一组数据x5 = np.dstack((a, b))print x5# 列式堆叠 堆叠一维数组时，按列码处理进行水平叠加，处理二维数组同水平叠加hstack()one1 = np.arange(3)one2 = one1 * 2x6 = np.column_stack((one1, one2))x7 = np.column_stack((a, b))print x6, x7# 行式堆叠 堆叠一维数组时，按行码进行垂直叠加，处理二维数组时同垂直叠加vstack()x8 = np.row_stack((one1, one2))x9 = np.row_stack((a, b))print x8, x9 [[0 1 2] [3 4 5] [6 7 8]] [[ 0 2 4] [ 6 8 10] [12 14 16]] [[ 0 1 2 0 2 4] [ 3 4 5 6 8 10] [ 6 7 8 12 14 16]] [[ 0 1 2 0 2 4] [ 3 4 5 6 8 10] [ 6 7 8 12 14 16]] [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 0 2 4] [ 6 8 10] [12 14 16]] [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 0 2 4] [ 6 8 10] [12 14 16]] [[[ 0 0] [ 1 2] [ 2 4]] [[ 3 6] [ 4 8] [ 5 10]] [[ 6 12] [ 7 14] [ 8 16]]] [[0 0] [1 2] [2 4]] [[ 0 1 2 0 2 4] [ 3 4 5 6 8 10] [ 6 7 8 12 14 16]] [[0 1 2] [0 2 4]] [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 0 2 4] [ 6 8 10] [12 14 16]] 数组拆分，横向，纵向，深度方向 hsplit() vsplit() dsplit() split() 123456789101112a = np.arange(9).reshape(3,3)print a# 沿着横向拆分成三个部分，也就是3列b = np.hsplit(a, 3)print bprint np.split(a,3, axis=1) # axis=1沿着横向拆分，axis=0，沿着纵向拆分，默认为0# 沿着纵向拆分print np.vsplit(a, 3)print np.split(a, 3, axis=0)# 沿着深度方向分割，要有一个三维数组,可以理解为面包片叠一起切条c = np.arange(36).reshape(3, 2, 6)print np.dsplit(c, 6) [[0 1 2] [3 4 5] [6 7 8]] [array([[0], [3], [6]]), array([[1], [4], [7]]), array([[2], [5], [8]])] [array([[0], [3], [6]]), array([[1], [4], [7]]), array([[2], [5], [8]])] [array([[0, 1, 2]]), array([[3, 4, 5]]), array([[6, 7, 8]])] [array([[0, 1, 2]]), array([[3, 4, 5]]), array([[6, 7, 8]])] [array([[[ 0], [ 6]], [[12], [18]], [[24], [30]]]), array([[[ 1], [ 7]], [[13], [19]], [[25], [31]]]), array([[[ 2], [ 8]], [[14], [20]], [[26], [32]]]), array([[[ 3], [ 9]], [[15], [21]], [[27], [33]]]), array([[[ 4], [10]], [[16], [22]], [[28], [34]]]), array([[[ 5], [11]], [[17], [23]], [[29], [35]]])] Numpy的属性 1234567891011print a.ndim, a.size, a.itemsize, a.nbytes, a.size * a.itemsize# 维度，元素个数，元素占字节数，数组占字节数print a.shape, a.dtypeprint a.T # a的转置数组，如果数组是一维，那么得到的是一个数组的视图b = np.array([1+2j, 3-4j])print b.dtype, b.real, b.imag # 复数的实部和虚部,数组的数据类型为复数a.flat # 把a转成一维数组的一个迭代器对象，可以用for遍历print a.flat[2:5]a.flat = 0 # a中所有数据都变成0print a 2 9 4 36 36 (3L, 3L) int32 [[0 0 0] [0 0 0] [0 0 0]] complex128 [ 1. 3.] [ 2. -4.] [0 0 0] [[0 0 0] [0 0 0] [0 0 0]] 数组的转换 tolist() astype() 12print a.tolist() # 把np数组转化为py的列表print a.astype('float') # 改变a的元素类型 [[0, 1, 2], [3, 4, 5], [6, 7, 8]] [[ 0. 1. 2.] [ 3. 4. 5.] [ 6. 7. 8.]] 7 创建数组的视图和拷贝 copy() view() 1234567891011121314151617import scipy.miscimport matplotlib.pyplot as plt%matplotlib inline# 显示图片face = scipy.misc.face()acopy = face.copy()aview = face.view()plt.subplot(221)plt.imshow(face)plt.subplot(222)plt.imshow(acopy)plt.subplot(223)plt.imshow(aview)aview.flat = 0plt.subplot(224)plt.imshow(aview)# 改变view视图后，原数组同样被改变，copy不受影响 8 花式索引花式索引，通过坐标x y的迭代器序列，索引一系列的元素 1234567face = scipy.misc.face()xmax = face.shape[0] ymax = face.shape[1]print xmax, ymaxface[range(xmax), range(xmax)] = 0 # 对角线上的像素点值改为0，sorry，非n*n方阵face[range(xmax-1, -1, -1), range(xmax)] = 0 # range(start, stop, step) 不包含stop，因此是n-1，-1plt.imshow(face) 768 1024 9 基于位置变量的索引方法1234567# 通过位置列表索引x = np.arange(xmax)y = np.arange(ymax)np.random.shuffle(x)np.random.shuffle(y)plt.imshow(face[np.ix_(x, y)]) # np.ix_() 输入n个1维数组，返回n个n维数组 # a[np.ix_([1,3],[2,5])] 的结果 [[a[1,2] a[1,5]], [a[3,2] a[3,5]]] 10 用bool型变量索引Numpy数组用布尔型变量索引数组 123456789x = (np.arange(xmax)%4 == 0)face = scipy.misc.face()f2 = face.copy()face[x, x] = 0 # 对角线上索引整除4的元素值设为0plt.subplot(121)plt.imshow(face)plt.subplot(122)f2[(face &gt; face.max()/4) &amp; (face &lt; 3*face.max()/4)] = 0 # 数组中值大于最大值1/4小于3/4的置为0plt.imshow(f2) 11 numpy数组的广播数组要和标量相乘的话，标量要根据数组的形状进行相应的扩展]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ipython使用技巧]]></title>
    <url>%2F2017%2F02%2F10%2F%E6%8A%80%E6%9C%AF%2Fuse-ipython-noteboke%2F</url>
    <content type="text"><![CDATA[ipython使用技巧 ipython –pylab pylab开关，ipython启动时自动导入np，sci，和matplotlib ipython –pylab inline 进入pylab模式，且图片内嵌到网页中 %logstart 保存会话，记录日志； %logstop关闭日志记录功能 !date !+shell命令，执行系统shell命令 c = !date %hist 显示历史命令 %hist -g print 在输入的历史命令中搜索print % xx magic function，单独一行时可以省略% jupyter notebook使用技巧 jupyter notebook 启动nootbook jupyter nbconvert –to markdown xxxx.ipynb ​ 快捷键命令模式 (按键 Esc 开启) Enter : 转入编辑模式 Shift-Enter : 运行本单元，选中下个单元 Ctrl-Enter : 运行本单元 Alt-Enter : 运行本单元，在其下插入新单元 Y : 单元转入代码状态 M :单元转入markdown状态 R : 单元转入raw状态 1 : 设定 1 级标题 2 : 设定 2 级标题 3 : 设定 3 级标题 4 : 设定 4 级标题 5 : 设定 5 级标题 6 : 设定 6 级标题 Up : 选中上方单元 K : 选中上方单元 Down : 选中下方单元 J : 选中下方单元 Shift-K : 扩大选中上方单元 Shift-J : 扩大选中下方单元 A : 在上方插入新单元 B : 在下方插入新单元 X : 剪切选中的单元 C : 复制选中的单元 Shift-V : 粘贴到上方单元 V : 粘贴到下方单元 Z : 恢复删除的最后一个单元 D,D : 删除选中的单元 Shift-M : 合并选中的单元 Ctrl-S : 文件存盘 S : 文件存盘 L : 转换行号 O : 转换输出 Shift-O : 转换输出滚动 Esc : 关闭页面 Q : 关闭页面 H : 显示快捷键帮助 I,I : 中断Notebook内核 0,0 : 重启Notebook内核 Shift : 忽略 Shift-Space : 向上滚动 Space : 向下滚动 编辑模式 ( Enter 键启动) Tab : 代码补全或缩进 Shift-Tab : 提示 Ctrl-] : 缩进 Ctrl-[ : 解除缩进 Ctrl-A : 全选 Ctrl-Z : 复原 Ctrl-Shift-Z : 再做 Ctrl-Y : 再做 Ctrl-Home : 跳到单元开头 Ctrl-Up : 跳到单元开头 Ctrl-End : 跳到单元末尾 Ctrl-Down : 跳到单元末尾 Ctrl-Left : 跳到左边一个字首 Ctrl-Right : 跳到右边一个字首 Ctrl-Backspace : 删除前面一个字 Ctrl-Delete : 删除后面一个字 Esc : 进入命令模式 Ctrl-M : 进入命令模式 Shift-Enter : 运行本单元，选中下一单元 Ctrl-Enter : 运行本单元 Alt-Enter : 运行本单元，在下面插入一单元 Ctrl-Shift– : 分割单元 Ctrl-Shift-Subtract : 分割单元 Ctrl-S : 文件存盘 Shift : 忽略 Up : 光标上移或转入上一单元 Down :光标下移或转入下一单元 ​]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Notebook</tag>
        <tag>Ipython</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy 入门学习]]></title>
    <url>%2F2016%2F12%2F19%2F%E6%8A%80%E6%9C%AF%2FScrapy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Scrapy学习笔记1、安装scrapy windows下安装 如果按照网上的方法自行安装scrapy，会出现各种错误，折腾很长时间。这里安装Anaconda2程序，会自带scrapy；如果默认没带的话，打开Anaconda Prompt，执行命令：conda listconda install scrapy安装完后，使用scrapy时如果提示openssl错误，去下载对应的openssl，安装即可。 Linux下安装 比较简单，# apt-get install python-dev# pip install scrapy 2、初步使用2.1、命令行工具scrapy12345678910111213141516171819202122232425# scrapy startproject my_spider # 创建scrapy项目.目录结构├── scrapy.cfg # 项目配置文件└── my_spider # 项目python模块, 之后将在此加入代码 ├── __init__.py ├── items.py ├── pipelines.py ├── settings.py └── spiders # 放置spider的目录 └── __init__.py# cd my_spider# scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt; # 使用模板生成spider文件，位于spiders文件夹下,默认使用basic模板# scrapy genspider -l 列出所有模板# scrapy genspider -d basic 查看basic模板# scrapy list 列出所有的爬虫# scrapy 显示scrapy可用的命令# scrapy &lt;command&gt; -h 查看command命令的详细信息# scrapy version -v Scrapy : 1.1.1 lxml : 3.6.0.0 libxml2 : 2.9.3 Twisted : 16.5.0 ...# scrapy bench 运行基准测试,可以测试scrapy是否正常# scrapy runspider spider_file.py 2.2、基本流程 startproject和genspider 首先分析要抓取的目标网站，在items.py中定义要抓取的数据对象 123456789101112import scrapyclass DmozItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() def __init__(self): # ？如果想初始化某些字段 scrapy.Item.__init__(self) self.title = '' def __str__(self): # ？默认控制台会输出所有的数据，包括抓取的页面源码 return 'crawling %s' % self.title 编写spider，解析网页(xpath/css/re)，提取数据到item对象 12345678910111213141516171819import scrapyfrom tutorial.items import DmozItemclass DmozSpider(scrapy.spider.Spider): name = "dmoz" #唯一标识，启动spider时即指定该名称 allowed_domains = ["dmoz.org"] start_urls = [ "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/", "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/" ] # 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。 def parse(self, response): for sel in response.xpath('//ul/li'): # 使用response.selector的css或xpath解析网页 item = DmozItem() # 类字典对象 item['title'] = sel.xpath('a/text()').extract() item['link'] = sel.xpath('a/@href').extract() item['desc'] = sel.xpath('text()').extract() yield item ## 返回item数据对象 ''' 每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给parse()函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。 ''' 保存数据 在执行scrapy crawl 命令时加入-o xxx.csv参数可以将item保存到csv文件中。也可在settings中设置FEED_URI和FEED_FORMAT 12FEED_URI = 'file:///tmp/export.csv'FEED_FORMAT = 'CSV' 将spider爬取到的item进一步处理并存储到数据库，需要在pipelines.py中实现自己的pipeline对象。 process_item(item, spider) # 数据经过pipeline时，都要调用该方法，最后返回一个item。 open_spider(spider) #当spider被开启时，这个方法被调用。 close_spider(spider) #当spider被关闭时，这个方法被调用，可以再爬虫关闭后进行相应的数据处理。 下面是存储数据到mongodb的一个实例： 12345678910111213141516171819202122232425class GushiciPipeline(object): collection_name = 'gushi' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get("MONGO_DATABASE", "items") ) # 在settings.py中定义 MONGO_URI = 'mongodb://localhost:27017' MONGO_DATABASE = 'test2' def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].insert(dict(item)) return item ​ 最后在settings.py中添加我们定义的pipeline：ITEM_PIPELINES = {&#39;blog_crawl.pipelines.SQLiteStorePipeline&#39;: 1} 1为优先级，越小级别越高 执行爬虫任务 12# scrapy list 查看所有的spider# scrapy crawl dmoz [-o xxx.csv|json|xml] 执行dmoz爬虫,-o指定结果输出到文件 通过python调用cmd命令执行爬虫 123# main.py 根目录下from scrapy import cmdlinecmdline.execute("scrapy crawl mindjet_muban".split()) 这样方便用ide调试 3、如何解析网页 scrapy.Selector有四个基本的方法： xpath()：参数是xpath表达式 css()：输入css表达式 extract()：序列化该节点为unicode字符串并返回list列表； re()：输入正则表达式，返回unicode字符串list列表；注意 正则效率低，可读性差 这四个方法返回的都是包含所有匹配节点的list列表，可嵌套使用css和xpath 3.1、xpathxpath w3school 12345678910111213141516171819202122/html/head/title: 选择HTML文档中 &lt;head&gt; 标签内的 &lt;title&gt; 元素/html/head/title/text(): 选择上面提到的 &lt;title&gt; 元素的文字//td: 选择所有的 &lt;td&gt; 元素//div[@class="mine"]: 选择所有具有 class="mine" 属性的 div 元素/x 是选取子节点x，//x 是递归选取全部的x.//x 从当前节点开始匹配，@是选择属性----- 路径表达式 -----/div/p[1] #选择第一个p元素/div/p[last()] #选择最后一个p元素/div/p[last()-2] #选择倒数第三个p元素/div/p[position()&lt;3] #选择前2个元素/div/p[x&gt;35.00]/name #选择x属性大于35.00的p元素下的name元素/div/p[x&gt;35.00]/@aaa #匹配所有属性aaa的值/div[@class] #选择含有class属性的div//div[@class="wp-pagenavi"]/a[not(@title)] #不含title属性的a标签/div[@class='lang'] #选择class值为lang的div/node() #根元素下所有的节点（包括文本节点，注释节点等）/text() #查找文档根节点下的所有文本节点----- 通配符 -----* #匹配任意元素@* #匹配任意属性exp1 | exp2 #返回2个表达式匹配结果的合集 3.2、css123456.con1 #选择class为con1的全部标签div.con1 #选择class为con1的全部div标签div p #递归选择div下的全部p标签div &gt; p #选择div的全部子标签pa::text #选择a标签的文本img::attr(href) #选择img标签的href属性的值 3.3、re返回unicode字符串的list，该对象无法继续使用css或xpath 3.4、extractresponse.xpath(&quot;//div[@class=&#39;son5&#39;]/p/a&quot;).css(&quot;::attr(href)&quot;).extract()[0] 该语句执行后得到的结果是字符串‘/view_12390.aspx’ 4、多级页面的爬取 主要是spider中的回调函数，下面分析爬古诗词网站为例，爬取诗文的标题、作者、朝代、url、原文，翻译注释 4.1、爬取前分析 [a] start_url：http://so.gushiwen.org/type.aspx [b] 不同朝代的诗文列表url：http://so.gushiwen.org/type.aspx?p=1&amp;c=先秦 ，参数p是页码，c是朝代，一共12个朝代，页码最多的有200页 [c] 诗文url：http://so.gushiwen.org/view_1.aspx [d] 翻译和注释url：http://so.gushiwen.org/fanyi_1.aspx 最后，abcd是递进关系，我们需要的item在bcd中都能找到一部分信息。注意b有很多页要抓，思路是“下一页”url。 4.2、代码示例 basic爬虫继承scrapy.Spider, crawl爬虫继承scrapy.CrawlSpider 基于basic模板的爬虫 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# -*- coding: utf-8 -*-import scrapyfrom gushici.items import GushiciItemclass TangsiSpider(scrapy.Spider): name = "tangsi" allowed_domains = ["gushiwen.org"] start_urls = ( 'http://so.gushiwen.org/type.aspx', ) def parse(self, response): # 获取不同朝代诗词的列表入口,利用meta字典传参到下一个回调函数，并调用get_lists函数 destinies = response.css("div.cont&gt;a") for destiny in destinies: # item = GushiciItem() # item['destiny'] = destiny.css("::text").extract() url = destiny.css("::attr(href)").extract()[0] destiny_url = response.urljoin(url) # 此处是分类列表的url # item['url'] = full_url destiny_text = destiny.css("::text").extract()[0] yield scrapy.Request(destiny_url, meta=&#123;"destiny": destiny_text&#125;, callback=self.get_lists) def get_lists(self, response): # 爬当前页的所有诗词的url，title，作者，并进入下一级调爬诗文内容；爬到“下一页”url后交给get_lists处理 destiny = response.meta['destiny'] divs = response.css("div.sons") for div in divs: title = div.css('p &gt; a[target]::text').extract()[0] full_url = response.urljoin(div.css('p &gt; a[target]::attr(href)').extract()[0]) author = div.xpath("p[2]/text()").extract()[0] # score = response.re(u'(\d+\.\d+)')[0] yield scrapy.Request(full_url, meta=&#123;'title': title, 'url': full_url, 'author': author, 'destiny': destiny&#125;, callback=self.get_contents) # 递归爬下一页的诗词url，入口是网页中的下一页标签url next_urls = response.xpath('//a[@style="width:60px;"]').css("::attr(href)").extract() for next_url in next_urls: next_page = response.urljoin(next_url) yield scrapy.Request(next_page, meta=&#123;"destiny": destiny&#125;, callback=self.get_lists) # 获取诗词的评分，文本，翻译页的url，接收父级页面解析出来的标题，作者，朝代，创建item def get_contents(self, response): item = GushiciItem() item['title'] = response.meta['title'] item['url'] = response.meta['url'] item['destiny'] = response.meta['destiny'] item['author'] = response.meta['author'] try: item['score'] = response.xpath('//div[@class="pingfen"]//div[@class="line1"]/span/text()').extract()[0] except Exception: item['score'] = u'评分不足10人' ps = response.xpath("//div[@class='son2']/p") text = "" if len(ps) &gt; 3: for p in ps[3:]: text += ''.join(p.css("::text").extract()) + '\n' else: text += ''.join(response.xpath("//div[@class='son2']/text()").extract()).replace('\n', '') item['text'] = text try: fanyi_url = response.xpath("//div[@class='son5']/p/a").css("::attr(href)").extract()[0] fanyi_url = response.urljoin(fanyi_url) yield scrapy.Request(fanyi_url, callback=self.get_fanyi, meta=&#123;'item': item&#125;) except Exception: item['translate'] = '' yield item # 获取翻译文本 def get_fanyi(self, response): # 前面是利用meta字典传递已抓到的内容，这里是将item对象装入meta传递 item = response.meta['item'] ps = response.xpath('//div[@class="shangxicont"]/p') fanyi = '' for p in ps[:-1]: fanyi += ''.join(p.css("::text").extract()) + '\n' item['translate'] = fanyi yield item ​ 网站没有反扒措施，但这个爬虫最终抓到7795条文章，像宋词至少有1万+，但在该分类下200页后的内容就是空的，也就是说web只给我们提供了200页的宋词。 基于crawl模板的爬虫 crawl类的爬虫更强大一点，Rule方法可以根据正则匹配所有满足条件的url，并调用相应回调函数处理 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom gushici.items import GushiciItemclass SongciSpider(CrawlSpider): name = 'songci' allowed_domains = ['so.gushiwen.org'] start_urls = ['http://so.gushiwen.org/type.aspx'] rules = ( # 这是分类页的url，全部加入到抓取列表中 Rule(LinkExtractor(allow=r"http://so\.gushiwen\.org/type\.aspx\?p=\d+&amp;c=.+?")), # 这是诗文详情页url，全部调用parse_item解析 Rule(LinkExtractor(allow=r'http://so\.gushiwen\.org/view_\d+\.aspx'), callback='parse_item', follow=True), ) def parse_item(self, response): item = GushiciItem() try: item['title'] = response.xpath('//h1/text()').extract()[0] item['url'] = response.url item['destiny'] = response.xpath('//div[@class="son2"]/p[1]//text()').extract()[-1] item['author'] = response.xpath('//div[@class="son2"]/p[2]//text()').extract()[-1] except Exception: pass try: item['score'] = response.xpath('//div[@class="pingfen"]//div[@class="line1"]/span/text()').extract()[0] except Exception: item['score'] = u'评分不足10人' ps = response.xpath("//div[@class='son2']/p") text = "" if len(ps) &gt; 3: for p in ps[3:]: text += ''.join(p.css("::text").extract()) + '\n' else: text += ''.join(response.xpath("//div[@class='son2']/text()").extract()).replace('\n', '') item['text'] = text try: fanyi_url = response.xpath("//div[@class='son5']/p/a").css("::attr(href)").extract()[0] fanyi_url = response.urljoin(fanyi_url) yield scrapy.Request(fanyi_url, callback=self.get_fanyi, meta=&#123;'item': item&#125;) except Exception: item['translate'] = '' yield item # 获取翻译文本 def get_fanyi(self, response): item = response.meta['item'] ps = response.xpath('//div[@class="shangxicont"]/p') fanyi = '' for p in ps[:-1]: fanyi += ''.join(p.css("::text").extract()) + '\n' item['translate'] = fanyi yield item basic爬虫是循环查找下一页的url来抓取，这里crawl爬虫我们直接用正则匹配列表页url和诗文详情页url，另外“标题文本朝代作者”也都改为在详情页获取了，“翻译”仍需从下一级网页获取。crawl爬了8700条数据，我在settings设置了download_delay为0.5秒，爬取效果是100秒约90条数据，一共用了2h48m。 pipelines的Dropitem 我先把basic抓到的放入mongodb了，再用crawl爬虫时，我改写了pipelines，如果item在数据库中，就丢弃，否则存入monggodb，避免数据重复。 123456789from scrapy.exceptions import DropItemdef process_item(self, item, spider): data = self.db[self.collection_name].find(&#123;&#125;, &#123;'_id': 0, 'url': 1&#125;) url_finished = set(x['url'] for x in data) # 获取已爬url的集合 if item['url'] in url_finished: raise DropItem("%s has been crawled!" % item) else: self.db[self.collection_name].insert(dict(item)) return item ​ 5、Scrapy框架解读 Scrapy基于Twisted异步网络库处理网络通信 5.1、整体框架 各组件及功能： Scrapy Engine: 用来处理整个系统的数据流处理, 触发事务(框架核心) Scheduler: 调度器，接收引擎发过来的Request, 压入队列中, 由引擎调度，将Request发送给Downloader。 可以理解为要爬取URL的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址 Downloader: 根据Scheduler给出的Request，去下载网页内容, 并将网页内容返回给Spiders解析。Scrapy下载器是基于twisted的异步模型，因此网页下载的结果并不是有序的。 Spiders: 爬虫有2个作用：从网页中提取自己需要的信息实体(Item)，并将item发送到item pipeline进行后续处理；从中网页提取出进一步爬取的URL,并发送给Scheduler。 Pipeline: 负责处理爬虫从网页中抽取的实体：数据清洗(整理、查重、验证有效性)、数据保存等。 Downloader Middlewares: 位于Scrapy引擎和下载器之间的中间件，主要是处理Scrapy引擎与下载器之间的请求及响应。可以在此处设置请求的代理、cookie？ Spider Middlewares: 介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。 Scheduler Middewares: 介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。 ​ 5.2、主要对象 Spider Selector Items &amp; Item Pipeline &amp; Feed exports Requests Responses Logging 日志模块，使用python内置的logging模块，logging.log(logging.WARNING, &quot;This is a warning&quot;),可以在setting中配置logging属性。 同时Spider对象含有logger对象，spider内也可使用self.logger.info(&#39;msg&#39;) self.logger.warning(&#39;msg&#39;) statscollectors MailSender 12345678910111213141516from scrapy.mail import MailSenderfrom xxx import settings#------ spider文件，在spider对象close时发邮件，重写close方法 def close(self, spider, reason): mailer = MailSender.from_settings(settings) mailer.send(to=['xx@xx.com'], subject=u'爬虫结束', body='test!'+str(reason))#-----settings文件# 下面是在settings文件中配置邮件服务器# 发送邮件MAIL_FROM = 'xxx@xxx.com' #邮件中的fromMAIL_HOST = 'smtp.xxx.com' #邮件服务器地址，端口MAIL_PORT = 25MAIL_USER = '登录名'MAIL_PASS = '密码'MAIL_TLS = False # 默认邮件服务器不开启TLS SSL安全连接MAIL_SSL = False 6、下载图片和文件6.1、激活Media pipeline在settings中设置如下： 1234567891011121314# 启用pipelineITEM_PIPELINES = &#123;'scrapy.pipelines.images.ImagesPipeline': 1, 'scrapy.pipelines.files.FilesPipeline': 1&#125;# 1.2版默认存放在./full，路径是相对.cfg配置文件# 即使指定存储位置，也会在该目录下生成full子文件夹IMAGES_STORE = '/path/to/valid/dir'# 定义图片url的item域IMAGES_URLS_FIELD = 'field_name_for_your_images_urls'# 定义图片下载结果存放的item域IMAGES_RESULT_FIELD = 'field_name_for_your_processed_images'# 文件和图片定义类似FILES_STORE = '/path/to/valid/dir'FILES_URLS_FIELD = 'field_name_for_your_files_urls'FILES_RESULT_FIELD = 'field_name_for_your_processed_files' 6.2、items中定义123import scrapyimage_url = scrapy.Field()download_success = scrapy.Field() 注意：image_url 的接收对象是image url的列表。 download_success:包含字典的列表，由ImagePipeline自动填充，包含image的url，本地存储位置，文件校验值： [{&#39;url&#39;: &#39;http://img.tupianzj.com/uploads/allimg/161226/9-161226154443.jpg&#39;, &#39;path&#39;: &#39;full/46ae5ec4f4c0905e48a41d569abe8e1aaa832f29.jpg&#39;, &#39;checksum&#39;: &#39;c35693197b1217ac8cafd7f7f318ef33&#39;}] 6.3、spiders编写 只需要给item[‘image_url’]传url的列表，并将item返回 这是爬图片之家QQ表情图的一个例子。 123456789101112131415161718192021222324252627282930313233# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom gaoxiao_pic.items import GaoxiaoPicItemclass FunPicSpider(CrawlSpider): name = 'fun_pic' allowed_domains = ['tupianzj.com'] start_urls = ['http://www.tupianzj.com/gaoxiao/biaoqing/', ] rules = ( Rule(LinkExtractor(allow=r'http://www\.tupianzj\.com/gaoxiao/gx/biaoqing\.html')), Rule(LinkExtractor(allow=r'http://www\.tupianzj\.com/gaoxiao/biaoqing/list[_\d]+\.html')), Rule(LinkExtractor(allow=r'http://img\.tupianzj\.com/uploads/allimg/\d+/.+\.(?:gif|jpg|png|bmp)'), callback='parse_item'), Rule(LinkExtractor(allow=r'http://www\.tupianzj\.com/gaoxiao/biaoqing/\d+/[_\d]+.html'), callback='parse_item', follow=True), ) def parse_item(self, response): i = GaoxiaoPicItem() if str(response.url).split('.')[-1].lower() in ['jpg', 'gif', 'png', 'jpeg', 'bmp']: i['image_url'] = [response.url] else: try: url = response.xpath("//div[@id='bigpic']/a/img/@src").extract()[0] i['image_url'] = [response.urljoin(url)] # 这里每个页面只有一张图片, print i['image_url'] except Exception, e: print e.message return i items.py: 123class GaoxiaoPicItem(scrapy.Item): image_url = scrapy.Field() download_success = scrapy.Field() settings.py: 12345678910# 启用pipelineITEM_PIPELINES = &#123;'scrapy.pipelines.images.ImagesPipeline': 1, # 'scrapy.pipelines.files.FilesPipeline': 1 &#125;# 定义图片存储位置（必须）IMAGES_STORE = '.'# 定义图片url的item域IMAGES_URLS_FIELD = 'image_url'# 存储下载结果IMAGES_RESULT_FIELD = 'download_success' 7、设置headers7.1、设置默认headers1234567891011# settings中可以设置浏览器默认headers，DEFAULT_REQUEST_HEADERS = &#123; 'accept': 'image/webp,/;q=0.8', 'accept-language': 'zh-CN,zh;q=0.8', 'referer': 'https://www.baidu.com/', 'user-agent': 'Mozilla/5.0 (Windows NT 6.3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36',&#125; 7.2、为每个download 分配随机UA和代理 UA 1234567891011121314# --------------settings.py--------------------DOWNLOADER_MIDDLEWARES = &#123; 'xxx.middlewares.RandomUserAgentMiddleware': 400, 'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,&#125;# ------创建middlewares.py ----------------------import fakerf = faker.Factory().create()# user_agent = f.user_agent() # 随机生成的浏览器UA，都次调用返回结果都不同class RandomUserAgentMiddleware(object): def process_request(self, request, spider): request.headers.setdefault('User-Agent', f.user_agent()) #log.msg('&gt;&gt;&gt;&gt; UA %s'%request.headers) 代理 123456789101112131415161718192021222324252627282930# -------------------settings.py--------# 启用代理DOWNLOADER_MIDDLEWARES = &#123; 'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 110, 'xxx.middlewares.ProxyMiddleware': 100,&#125;# 在setting中写死了代理列表，实际环境在需要代理时可以从数据库代理池获取PROXY_LIST = [ &#123;'ip_port': '111.11.228.75:80', 'user_pass': ''&#125;, &#123;'ip_port': '120.198.243.22:80', 'user_pass': ''&#125;, &#123;'ip_port': '111.8.60.9:8123', 'user_pass': ''&#125;, &#123;'ip_port': '101.71.27.120:80', 'user_pass': ''&#125;, &#123;'ip_port': '122.96.59.104:80', 'user_pass': ''&#125;, &#123;'ip_port': '122.224.249.122:8088', 'user_pass': ''&#125;,]# -----创建middlewares.py, 配置代理-----------------from xxx.settings import PROXY_LISTimport randomimport base64class ProxyMiddleware(object): def process_request(self, request, spider): proxy = random.choice(PROXIES_LIST) if proxy['user_pass'] is not None: request.meta['proxy'] = "http://%s" % proxy['ip_port'] encoded_user_pass = base64.encodestring(proxy['user_pass']) request.headers['Proxy-Authorization'] = 'Basic ' + encoded_user_pass else: request.meta['proxy'] = "http://%s" % proxy['ip_port'] 8、登陆Post和Cookie8.1、POST表单登陆豆瓣登陆页面有验证码比无验证时post表单提交的数据多了captcha-id和captcha-solution。有验证码时下载图片，手动输入。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom scrapy.http import Request, FormRequest, HtmlResponsefrom douban.items import DoubanItemimport faker, requests, osfrom PIL import Imageclass DoubanSpiderSpider(CrawlSpider): def _requests_to_follow(self, response): """重写加入cookiejar的更新""" if not isinstance(response, HtmlResponse): return seen = set() for n, rule in enumerate(self._rules): links = [l for l in rule.link_extractor.extract_links(response) if l not in seen] if links and rule.process_links: links = rule.process_links(links) for link in links: seen.add(link) r = Request(url=link.url, callback=self._response_downloaded) # 下面这句是我重写的 r.meta.update(rule=n, link_text=link.text, cookiejar=response.meta['cookiejar']) yield rule.process_request(r) name = 'douban_spider' allowed_domains = ['douban.com'] start_urls = ['https://www.douban.com/'] rules = ( Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True), ) f = faker.Factory().create() user_agent = f.user_agent() # 随机生成的浏览器UA，都次调用返回结果都不同 headers = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Cache-Control': 'max-age=0', 'Referer': 'https://www.douban.com', 'User-Agent': user_agent, # 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36', 'Accept-Encoding': 'gzip, deflate, sdch, br', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Host': 'accounts.douban.com', 'Connection': 'keep-alive' &#125; your_email = '1XXX' your_password = 'XXXX' postdata = &#123;'source': 'None', 'redir': 'https://www.douban.com/', 'form_email': your_email, 'form_password': your_password, # 'captcha-solution': vcode, # 'captcha-id': captcha, 'login': '登录' &#125; def start_requests(self): # 访问登录页面 # Scrapy通过使用meta['cookiejar']来支持单spider追踪多cookie session。默认情况下其使用一个cookie jar(session)，不过可以传递一个标示符来使用多个。如meta=&#123;'cookiejar': 1&#125;这句，后面那个1就是标示符。 return [Request(url='https://www.douban.com/accounts/login', headers=self.headers, meta=&#123;'cookiejar': 1&#125;, callback=self.post_login)] def post_login(self, response): print 'Preparing login====', response.url # 如果登陆要验证码，则post数据中加入验证码的id和值 if response.body.find('captcha_image') &gt; 0: captcha_url = response.xpath('//img[@id="captcha_image"]/@src').extract()[0] print u'验证码的URL：%s' % captcha_url with open('v.jpg', 'wb') as f: f.write(requests.get(captcha_url, verify=False).content) with open('v.jpg', 'rb') as f: image = Image.open(f) image.show() self.postdata['captcha-solution'] = raw_input('请输入图片验证码:\n') os.remove('v.jpg') self.postdata['captcha-id'] = response.xpath('//input[@name="captcha-id"]/@value').extract()[0] print self.postdata return [FormRequest.from_response(response, headers=self.headers, meta=&#123;'cookiejar': response.meta['cookiejar']&#125;, formdata=self.postdata, callback=self.after_login, )] def after_login(self, response): # 登陆之后,访问的网页内容应该会包含用户信息 self.headers['Host'] = 'www.douban.com' print response.body with open('a.txt', 'w') as f: f.write(response.body) item = DoubanItem() item['main_page'] = response.body return Request('https://www.douban.com/doumail/', headers=self.headers, meta=&#123;'cookiejar': response.meta['cookiejar'], 'item': item&#125;, callback=self.openfile) def openfile(self, response): item['doumail_page'] = response.body return item 8.2、使用Cookie登陆豆瓣使用浏览器或requests模拟登陆，然后将cookie传进scrapy的spider。 由于每次从浏览器复制出来的cookies或headers都是字符串形式，手工改成字典太累了，写个小程序： 1234567891011121314151617181920212223242526272829# 从chrome浏览器的Request Header中，点击view source，然后复制header字符串headers_str = '''Host: www.douban.comConnection: keep-aliveCache-Control: max-age=0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Upgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36Referer: https://www.douban.com/people/108243932/Accept-Encoding: gzip, deflate, sdch, brAccept-Language: zh-CN,zh;q=0.8Cookie: viewed="1770782"; bid=SrGB-eJRKEE; gr_user_id=a37ff9ef-33cc-4de7-971b-c161f747a77b; ll="118254"; ps=y; dbcl2="108243932:kVKKGRcAZVg"; _ga=GA1.2.1495924407.1465701053; ck=LRet; _pk_ref.100001.8cb4=%5B%22%22%2C%22%22%2C1483063011%2C%22https%3A%2F%2Faccounts.douban.com%2Fregister%22%5D; __utmt=1; ap=1; push_noty_num=0; push_doumail_num=0; _pk_id.100001.8cb4=438cca0c1cd12666.1482926660.4.1483063910.1483060769.; _pk_ses.100001.8cb4=*; __utma=30149280.1495924407.1465701053.1483060678.1483063011.6; __utmb=30149280.24.9.1483063909873; __utmc=30149280; __utmz=30149280.1483021458.4.3.utmcsr=accounts.douban.com|utmccn=(referral)|utmcmd=referral|utmcct=/register; __utmv=30149280.10824; _vwo_uuid_v2=02A1FB5802A3379A6C48F734CB328D35|33e02434e1d90a57e4c63094703cde0f'''headers = &#123;&#125;for i in headers_str.split('\n'): j = i.replace(': ', ':', 1).split(':', 1) headers[j[0]] = j[1]if 'Cookie' in headers: cookies_str = headers['Cookie']else: # 把cookie字符串从浏览器copy进来 cookies_str = ''' '''cookies = &#123;&#125;for i in cookies_str.split(';'): q = i.split('=') key = q[0].replace(' ', '', 1) value = q[1].replace(' ', '', 1) cookies[key] = value# print cookiesprint headers.pop('Cookie')print headers 使用cookie登陆豆瓣： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Ruleclass DoubanCookieSpider(CrawlSpider): name = 'douban_cookie' allowed_domains = ['douban.com'] start_urls = ['https://www.douban.com/mine/', 'https://www.douban.com/doumail/', 'https://www.douban.com/people/108243932/', 'https://www.douban.com/mine/orders/' ] cookies = &#123;'ck': 'LRet', 'ps': 'y', '__utmz': '30149280.1483021458.4.3.utmcsr', '__utmv': '30149280.10824', 'push_doumail_num': '0', '__utmt': '1', 'bid': 'SrGB-eJRKEE', 'push_noty_num': '0', '_ga': 'GA1.2.1495924407.1465701053', '_pk_ref.100001.8cb4': '%5B%22%22%2C%22%22%2C1483063011%2C%22https%3A%2F%2Faccounts.douban.com%2Fregister%22%5D', '_vwo_uuid_v2': '02A1FB5802A3379A6C48F734CB328D35|33e02434e1d90a57e4c63094703cde0f', 'ap': '1', 'dbcl2': '"108243932:kVKKGRcAZVg"', '_pk_id.100001.8cb4': '438cca0c1cd12666.1482926660.4.1483063910.1483060769.', '_pk_ses.100001.8cb4': '*', 'gr_user_id': 'a37ff9ef-33cc-4de7-971b-c161f747a77b', '__utma': '30149280.1495924407.1465701053.1483060678.1483063011.6', '__utmb': '30149280.24.9.1483063909873', '__utmc': '30149280', 'll': '"118254"', 'viewed': '"1770782"'&#125; headers = &#123;'Accept-Language': 'zh-CN,zh;q=0.8', 'Accept-Encoding': 'gzip, deflate, sdch, br', 'Connection': 'keep-alive', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36', 'Host': 'www.douban.com', 'Referer': 'https://www.douban.com/', 'Cache-Control': 'max-age=0', 'Upgrade-Insecure-Requests': '1'&#125; def start_requests(self): return [scrapy.Request('https://www.douban.com/', headers=self.headers, cookies=self.cookies, meta=&#123;'cookiejar': 1&#125;, callback=self.after_set_cookie)] def after_set_cookie(self, response): req = [] self.headers['Referer'] = response.url for url in self.start_urls: req.append(scrapy.Request(url=url, headers=self.headers, meta=&#123;'cookiejar': response.meta['cookiejar']&#125;, callback=self.parse_item)) return req def parse_item(self, response): print response.url filename = response.url.split('/')[-2] + '.html' with open(filename, 'w') as f: f.write(response.body) # 保存的网页中应该有登陆账号后的个人信息 9、JS 、XHR分析 主要是分析js请求的url。挖个坑，暂时不打算填。 不好解决的话，还是用selenium + phantomjs吧 10、Scrapy Redis分布式爬虫10.1、安装redis windows 下载地址：https://github.com/rgl/redis/downloads 安装完成后， 运行redis服务器的命令：安装目录下的redis-server.exe 运行redis客户端的命令：安装目录下的redis-cli.exe Linux 1234567$sudo apt-get install redis-server# 启动 Redis$ redis-server &amp;# 或者$ service redis-server start# 查看 redis 是否启动？$ redis-cli 12345# vi /etc/redis/redis.conf#注释bind#bind 127.0.0.1# 如果要设置密码，取消注释requirepassrequirepass mypasswd 修改配置文件后，要重启服务生效。有密码的连接方式，redis-cli -a mypasswd -h 172.16.28.24 -p 6379 还是单开一篇笔记来写吧！ 11、记录一些error11.1、url相关 ValueError: Missing scheme in request url: h 这种问题一般是url出错了，要以http开头，如果是从网页解析的url，可以用 response.urljoin(relative_url) 生成绝对路径。 如果在下载图片中出错，是因为image_field必须是图片url的列表，只存了一个url字符串的话，scrapy遍历url时取出的第一个对象是http url的第一个字符h，故报错h是非法url。 太坑了，花了半个小时才发现原因。 注意twisted库的版本，在conda中先装twisted再装scrapy]]></content>
      <categories>
        <category>技术</category>
        <category>scrapy</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 配置SNMP服务]]></title>
    <url>%2F2016%2F12%2F14%2F%E6%8A%80%E6%9C%AF%2Flinux-snmp-server%2F</url>
    <content type="text"><![CDATA[Linux服务器配置snmp服务安装snmp server 安装snmp 1apt-get install snmp snmpd 安装mib库 1apt-get install snmp-mibs-downloader 参考debian-wiki-snmp 查看默认配置文件 12cat /etc/snmp/snmpd.conf cat /etc/snmp/snmp.conf 启动SNMP 12# service snmpd start# snmpd -C -c /etc/snmp/myconfig.conf ### -C不适用默认配置文件 -c指定自定义配置文件 如有需要，安装net-snmp 源码下载地址 编译 12345# tar -zxvf net-snmp-5.7.3.tar.gz ### 解压源码# ./configure --help ### 查看编译配置# ./configure ### 开始编译# make# make install 如果编译报错lperl，使用 apt-get install libperl-dev 使用net-snmp工具 查询到主机CPU空闲率为99% # snmpwalk -v 2c -c public localhost 1.3.6.1.4.1.2021.11.11.0 UCD-SNMP-MIB::ssCpuIdle.0 = INTEGER: 99 修改snmp配置文件/etc/snmp/snmpd.conf 配置允许网络访问,找到【AGENT BEHAVIOUR】，注释掉`agentAddress udp:127.0.0.1：161`,添加一行`agentAddress udp:161` 选择SNMPv2C协议版本,找到【ACTIVE MONITORING】，注释掉`trapsink localhost public`，添加`trap2sink localhost public` 设置访问权限,找到【ACCESS CONTROL】,注释掉`rocommunity public default -V systemonly`，添加`rocommunity public default `,允许所有访问请求。 /etc/snmp/snmp.conf 注释掉开头第一行 开启防火墙161端口 查看防火墙规则 iptables –L –n 添加规则 iptables -I INPUT -p udp --dport 161 -j ACCEPT 保存生效 iptables-save Debian iptables -save ubuntu 常用Linux监控OID 系统信息 网络接口 CPU及负载 内存及磁盘 System Group sysDescr 1.3.6.1.2.1.1.1 sysObjectID 1.3.6.1.2.1.1.2 sysUpTime 1.3.6.1.2.1.1.3 sysContact 1.3.6.1.2.1.1.4 sysName 1.3.6.1.2.1.1.5 sysLocation 1.3.6.1.2.1.1.6 sysServices 1.3.6.1.2.1.1.7 Interfaces Group ifNumber 1.3.6.1.2.1.2.1 ifTable 1.3.6.1.2.1.2.2 ifEntry 1.3.6.1.2.1.2.2.1 ifIndex 1.3.6.1.2.1.2.2.1.1 ifDescr 1.3.6.1.2.1.2.2.1.2 ifType 1.3.6.1.2.1.2.2.1.3 ifMtu 1.3.6.1.2.1.2.2.1.4 ifSpeed 1.3.6.1.2.1.2.2.1.5 ifPhysAddress 1.3.6.1.2.1.2.2.1.6 ifAdminStatus 1.3.6.1.2.1.2.2.1.7 ifOperStatus 1.3.6.1.2.1.2.2.1.8 ifLastChange 1.3.6.1.2.1.2.2.1.9 ifInOctets 1.3.6.1.2.1.2.2.1.10 ifInUcastPkts 1.3.6.1.2.1.2.2.1.11 ifInNUcastPkts 1.3.6.1.2.1.2.2.1.12 ifInDiscards 1.3.6.1.2.1.2.2.1.13 ifInErrors 1.3.6.1.2.1.2.2.1.14 ifInUnknownProtos 1.3.6.1.2.1.2.2.1.15 ifOutOctets 1.3.6.1.2.1.2.2.1.16 ifOutUcastPkts 1.3.6.1.2.1.2.2.1.17 ifOutNUcastPkts 1.3.6.1.2.1.2.2.1.18 ifOutDiscards 1.3.6.1.2.1.2.2.1.19 ifOutErrors 1.3.6.1.2.1.2.2.1.20 ifOutQLen 1.3.6.1.2.1.2.2.1.21 ifSpecific 1.3.6.1.2.1.2.2.1.22 IP Group ipForwarding 1.3.6.1.2.1.4.1 ipDefaultTTL 1.3.6.1.2.1.4.2 ipInReceives 1.3.6.1.2.1.4.3 ipInHdrErrors 1.3.6.1.2.1.4.4 ipInAddrErrors 1.3.6.1.2.1.4.5 ipForwDatagrams 1.3.6.1.2.1.4.6 ipInUnknownProtos 1.3.6.1.2.1.4.7 ipInDiscards 1.3.6.1.2.1.4.8 ipInDelivers 1.3.6.1.2.1.4.9 ipOutRequests 1.3.6.1.2.1.4.10 ipOutDiscards 1.3.6.1.2.1.4.11 ipOutNoRoutes 1.3.6.1.2.1.4.12 ipReasmTimeout 1.3.6.1.2.1.4.13 ipReasmReqds 1.3.6.1.2.1.4.14 ipReasmOKs 1.3.6.1.2.1.4.15 ipReasmFails 1.3.6.1.2.1.4.16 ipFragsOKs 1.3.6.1.2.1.4.17 ipFragsFails 1.3.6.1.2.1.4.18 ipFragCreates 1.3.6.1.2.1.4.19 ipAddrTable 1.3.6.1.2.1.4.20 ipAddrEntry 1.3.6.1.2.1.4.20.1 ipAdEntAddr 1.3.6.1.2.1.4.20.1.1 ipAdEntIfIndex 1.3.6.1.2.1.4.20.1.2 ipAdEntNetMask 1.3.6.1.2.1.4.20.1.3 ipAdEntBcastAddr 1.3.6.1.2.1.4.20.1.4 ipAdEntReasmMaxSize 1.3.6.1.2.1.4.20.1.5 ICMP Group icmpInMsgs 1.3.6.1.2.1.5.1 icmpInErrors 1.3.6.1.2.1.5.2 icmpInDestUnreachs 1.3.6.1.2.1.5.3 icmpInTimeExcds 1.3.6.1.2.1.5.4 icmpInParmProbs 1.3.6.1.2.1.5.5 icmpInSrcQuenchs 1.3.6.1.2.1.5.6 icmpInRedirects 1.3.6.1.2.1.5.7 icmpInEchos 1.3.6.1.2.1.5.8 icmpInEchoReps 1.3.6.1.2.1.5.9 icmpInTimestamps 1.3.6.1.2.1.5.10 icmpInTimestampReps 1.3.6.1.2.1.5.11 icmpInAddrMasks 1.3.6.1.2.1.5.12 icmpInAddrMaskReps 1.3.6.1.2.1.5.13 icmpOutMsgs 1.3.6.1.2.1.5.14 icmpOutErrors 1.3.6.1.2.1.5.15 icmpOutDestUnreachs 1.3.6.1.2.1.5.16 icmpOutTimeExcds 1.3.6.1.2.1.5.17 icmpOutParmProbs 1.3.6.1.2.1.5.18 icmpOutSrcQuenchs 1.3.6.1.2.1.5.19 icmpOutRedirects 1.3.6.1.2.1.5.20 icmpOutEchos 1.3.6.1.2.1.5.21 icmpOutEchoReps 1.3.6.1.2.1.5.22 icmpOutTimestamps 1.3.6.1.2.1.5.23 icmpOutTimestampReps 1.3.6.1.2.1.5.24 icmpOutAddrMasks 1.3.6.1.2.1.5.25 icmpOutAddrMaskReps 1.3.6.1.2.1.5.26 TCP Group tcpRtoAlgorithm 1.3.6.1.2.1.6.1 tcpRtoMin 1.3.6.1.2.1.6.2 tcpRtoMax 1.3.6.1.2.1.6.3 tcpMaxConn 1.3.6.1.2.1.6.4 tcpActiveOpens 1.3.6.1.2.1.6.5 tcpPassiveOpens 1.3.6.1.2.1.6.6 tcpAttemptFails 1.3.6.1.2.1.6.7 tcpEstabResets 1.3.6.1.2.1.6.8 tcpCurrEstab 1.3.6.1.2.1.6.9 tcpInSegs 1.3.6.1.2.1.6.10 tcpOutSegs 1.3.6.1.2.1.6.11 tcpRetransSegs 1.3.6.1.2.1.6.12 tcpConnTable 1.3.6.1.2.1.6.13 tcpConnEntry 1.3.6.1.2.1.6.13.1 tcpConnState 1.3.6.1.2.1.6.13.1.1 tcpConnLocalAddress 1.3.6.1.2.1.6.13.1.2 tcpConnLocalPort 1.3.6.1.2.1.6.13.1.3 tcpConnRemAddress 1.3.6.1.2.1.6.13.1.4 tcpConnRemPort 1.3.6.1.2.1.6.13.1.5 tcpInErrs 1.3.6.1.2.1.6.14 tcpOutRsts 1.3.6.1.2.1.6.15 UDP Group udpInDatagrams 1.3.6.1.2.1.7.1 udpNoPorts 1.3.6.1.2.1.7.2 udpInErrors 1.3.6.1.2.1.7.3 udpOutDatagrams 1.3.6.1.2.1.7.4 udpTable 1.3.6.1.2.1.7.5 udpEntry 1.3.6.1.2.1.7.5.1 udpLocalAddress 1.3.6.1.2.1.7.5.1.1 udpLocalPort 1.3.6.1.2.1.7.5.1.2 SNMP Group snmpInPkts 1.3.6.1.2.1.11.1 snmpOutPkts 1.3.6.1.2.1.11.2 snmpInBadVersions 1.3.6.1.2.1.11.3 snmpInBadCommunityNames 1.3.6.1.2.1.11.4 snmpInBadCommunityUses 1.3.6.1.2.1.11.5 snmpInASNParseErrs 1.3.6.1.2.1.11.6 NOT USED 1.3.6.1.2.1.11.7 snmpInTooBigs 1.3.6.1.2.1.11.8 snmpInNoSuchNames 1.3.6.1.2.1.11.9 snmpInBadValues 1.3.6.1.2.1.11.10 snmpInReadOnlys 1.3.6.1.2.1.11.11 snmpInGenErrs 1.3.6.1.2.1.11.12 snmpInTotalReqVars 1.3.6.1.2.1.11.13 snmpInTotalSetVars 1.3.6.1.2.1.11.14 snmpInGetRequests 1.3.6.1.2.1.11.15 snmpInGetNexts 1.3.6.1.2.1.11.16 snmpInSetRequests 1.3.6.1.2.1.11.17 snmpInGetResponses 1.3.6.1.2.1.11.18 snmpInTraps 1.3.6.1.2.1.11.19 snmpOutTooBigs 1.3.6.1.2.1.11.20 snmpOutNoSuchNames 1.3.6.1.2.1.11.21 snmpOutBadValues 1.3.6.1.2.1.11.22 NOT USED 1.3.6.1.2.1.11.23 snmpOutGenErrs 1.3.6.1.2.1.11.24 snmpOutGetRequests 1.3.6.1.2.1.11.25 snmpOutGetNexts 1.3.6.1.2.1.11.26 snmpOutSetRequests 1.3.6.1.2.1.11.27 snmpOutGetResponses 1.3.6.1.2.1.11.28 snmpOutTraps 1.3.6.1.2.1.11.29 snmpEnableAuthenTraps 1.3.6.1.2.1.11.30]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML语法基础]]></title>
    <url>%2F2016%2F12%2F13%2F%E6%8A%80%E6%9C%AF%2FHTML-basic-grammar%2F</url>
    <content type="text"><![CDATA[HTML语法基础 学习django的时候发现还是要懂点html语法的 语法 html是一种展示网页的标记语言 由标签和被其标记的内容组成，标签的属性控制着显示的方式 &lt;tag key=&quot;value&quot;&gt;被标记的内容&lt;/tag&gt; 语法不区分大小写，默认使用小写 语法不区分回车、空格和缩进，为了代码可读和逻辑清晰，应该保持严格的缩进风格，必要时添加注释 注释内容&lt;!-- XXXXX --!&gt;，不会被显示 字符字体：保留字符无法直接使用，如&lt; &gt; 会和标签符号产生歧义，可以使用&amp;lt 或 &amp;#60表示小于号 HTML基本结构123456789101112131415161718&lt;html lang="en"&gt;&lt;!--html标签是文档开始和结束的标志--&gt;&lt;head&gt; &lt;!--HTML文件头标记，用来包含文件的基本信息，比如网页的标题、关键字，在 可以放&lt;title&gt;&lt;/title&gt;、&lt;meta&gt;&lt;/meta&gt;、&lt;style&gt;&lt;/style&gt;等等标记--&gt; &lt;!--注意:在&lt;head&gt;&lt;/head&gt;标记内的内容不会在浏览器中显示--&gt; &lt;meta charset="UTF-8"&gt; &lt;!--&lt;meta&gt; 元素可提供有关某个 HTML 元素的元信息 (meta-information)，比如描述、针对搜索引擎的关键词以及刷新频率。--&gt; &lt;title&gt;这是网页的标题&lt;/title&gt; &lt;!--网页的“主题”，显示在浏览器的标题栏，网页的标题不能太长，要短小精悍，能具体反应页面的内容，&lt;title&gt;&lt;/title&gt;标记中不能包含其他标记--&gt;&lt;/head&gt;&lt;body&gt;&lt;!--HTML文档的主体标记--&gt;&lt;!--功能：&lt;body&gt;...&lt;/body&gt;是网页的主体部分，在此标记之间可以包含如&lt;p&gt;&lt;/p&gt;、&lt;h1&gt;&lt;/h1&gt;、&lt;br&gt;、&lt;hr&gt;等等标记，这些内容组成了我们所看见的网页--&gt;&lt;!--body的属性有：背景颜色 bgcolor="red",文本颜色text="green",链接颜色 link="blue"，--&gt;&lt;!-- 已访问过链接的颜色vlink="yellow",正在被点击连接的颜色 alink="red"--&gt;&lt;/body&gt;&lt;/html&gt; 格式标记12345678910&lt;br&gt; 强制换行，后面的内容会显示在下一行&lt;p&gt;...&lt;/p&gt; 段落标记&lt;center&gt;...&lt;/center&gt; 居中对齐标记：让段落或者是文字相对于父标记居中显示&lt;pre&gt;...&lt;/pre&gt; 预格式化标记：保留预先编排好的格式&lt;li&gt;第一个&lt;/li&gt; 列表项目,默认无序&lt;ul&gt;...&lt;/ul&gt; 声明无序列表，内嵌套&lt;li&gt;&lt;ol&gt;...&lt;/ol&gt; 声明列表有序内嵌套&lt;li&gt;,属性type=&quot;[1|A|a|I|i]&quot; value=&quot;序列起始值&quot;&lt;dl&gt; &lt;dt&gt; &lt;dd&gt; 定义性列表，对列表条目进行简短的说明&lt;hr&gt; 水平分割线&lt;div&gt;...&lt;/div&gt; 分区显示/层标记，可以多层嵌套，用来编排一大段HTML代码 文本标记12345678910111213&lt;h1&gt;...&lt;/h1&gt; h1~6 1级标题文本最大，6级标题文本最小&lt;font size=&apos;3&apos; color=&apos;green&apos; face=&quot;微软雅黑&quot;&gt;有字体格式的文字&lt;/font&gt;&lt;b&gt;粗体字&lt;/b&gt;&lt;i&gt;斜体字&lt;/i&gt;&lt;sub&gt;下标字体&lt;/sub&gt;&lt;sup&gt;上标字体&lt;/sup&gt;&lt;tt&gt;打印机字体&lt;/tt&gt;&lt;cite&gt;引用方式的字体，通常是斜体&lt;/cite&gt;&lt;em&gt;强调字体，通常是斜体&lt;/em&gt;&lt;strong&gt;强调字体，通常是粗体&lt;/strong&gt;&lt;small&gt;小型字体&lt;/small&gt;&lt;big&gt;大型字体&lt;/big&gt;&lt;u&gt;带下划线的字&lt;/u&gt; 图像标签123456789&lt;img src=&quot;路径/文件名.图片格式&quot; width=&quot;属性值&quot; height=&quot;属性值&quot; border=&quot;属性值&quot; alt=&quot;属性值&quot;&gt;属性： 作用src 指定加载文件的路径width 指定图片的宽度，单位px、em、cm、mmheight 指定图片的高度，单位px、em、cm、mmborder 指定图标的边框宽度，单位px、em、cm、mmalt 当网页上的图片被加载完成后，鼠标移动到上面去，会显示这个图片指定的属性文字 如果图像没有下载或者加载失败，会用文字来代替图像显示 搜索引擎可以通过这个属性的文字来抓取图片 超链接1234567891011&lt;a href=&quot;&quot; target=&quot;打开方式&quot; name=&quot;页面锚点名称&quot; &gt;链接文字或者图片&lt;/a&gt;href 链接的地址，可以是网页或视频、图片、音乐等等target 定义超链接的打开方式target取值 _blank:在一个新的窗口中打开链接 _seif(默认值):在当前窗口中打开链接 _parent:在父窗口中打开页面（框架中使用较多） _top:在顶层窗口中打开文件（框架中使用较多）name 指定页面的锚点名称使用a标签实现页内跳转&lt;a href=&quot;#id1&quot;&gt;点击后跳到本页id值为id1的tag位置&lt;/a&gt; HTML表格12345678910111213141516171819202122232425262728293031&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt;&lt;table width="80%" border="1" height="100" align="center"&gt; &lt;!-- width 和 height的值可以是px或父级的%值， border是表格外框的宽度，默认值是0； align是表格的显示位置，left默认，right，center--&gt; &lt;!-- cellspacing，单元格之间的间距，默认是2px；cellpadding 单元格内容与单元格边框的显示距离，单位像素 --&gt; &lt;!-- frame 控制表格边框最外层的四条线框；rules 控制是否显示以及如何显示单元格之间的分割线；--&gt; &lt;caption align="bottom"&gt;表格的标题&lt;/caption&gt; &lt;!--caption位于table之后，tr表格行之前,其align属性取值 top、bottom、left、right，指定标题的位置--&gt; &lt;tr&gt; &lt;!-- 对于每一个表格行，都是由一对&lt;tr&gt;...&lt;/tr&gt;标记表示，每一行&lt;tr&gt;标记内可以嵌套多个&lt;td&gt;或者&lt;th&gt;标记 --&gt; &lt;th&gt;班级&lt;/th&gt; &lt;!--td 和th标签，必须在tr内，th是表头的单元格标记，即表格的首行 --&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;th&gt;成绩&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;四年级一班&lt;/td&gt; &lt;!--td 表格的数据单元格标记--&gt; &lt;td&gt;张三&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; HTML表单 form表单 1234567891011121314151617181920212223242526&lt;div&gt; &lt;form action="." method="post" name="login"&gt; &lt;!--&lt;form action="服务器端地址（接受表单内容的地址）" name="表单名称" method="post|get"&gt;...&lt;/form&gt;--&gt; &lt;!--get方式提交时，会将表单的内容附加在URL地址的后面，所以限制了提交的内容的长度，不超过8192个字符，且不具备保密性--&gt; &lt;!--post方式提交时，将表单中的数据一并包含在表单主体中，一起传送到服务器中处理，没有数据大小限制--&gt; &lt;!-- action=表单数据的处理程序的URL地址,为空则使用当前文档的URL地址，如果不需要使用,action="no"--&gt; &lt;!-- enctype,设置表单的资料的编码方式; target,和超链接的属于类似，用来指定目标窗口--&gt; &lt;p&gt;用户名 &lt;input type="text" name="login_name" value="QQ or Email" maxlength="20" size="10"&gt;&lt;/p&gt; &lt;p&gt;密码 &lt;input type="password" name="login_passwd" value="" size="10"&gt;&lt;/p&gt; &lt;p&gt;性别：&lt;input type="radio" name="sex" checked="checked"&gt;男&lt;input type="radio" name="sex"&gt;女&lt;/p&gt; &lt;p&gt;&lt;input type="hidden" value="隐藏的内容" name="mihiddenma" size="10"&gt;&lt;/p&gt; &lt;p&gt;爱好：&lt;input type="checkbox" name="tiyu" checked="checked"&gt;体育&lt;input type="checkbox" name="changge"&gt;唱歌&lt;br&gt;&lt;/p&gt; &lt;p&gt;&lt;br&gt; 地址： &lt;select name="dizhi"&gt; &lt;option value="sichuan"&gt;四川&lt;/option&gt; &lt;option value="beijing"&gt;北京&lt;/option&gt; &lt;option value="shanghai"&gt;上海&lt;/option&gt; &lt;/select&gt; &lt;/p&gt; &lt;p&gt;自我介绍 &lt;br&gt; &lt;textarea cols="35" rows="10" name="自我介绍"&gt;介绍一下你自己呗！&lt;/textarea&gt; &lt;/p&gt; &lt;p&gt;&lt;input type="submit" value="提交注册"&gt;&lt;/p&gt; &lt;p&gt;&lt;input type="reset" value="重置"&gt;&lt;/p&gt; &lt;p&gt;&lt;input type="button" value="一个按钮"&gt;&lt;/p&gt; &lt;/form&gt;&lt;/div&gt; input标签 12345678910111、文本域/密码框： &lt;input type=&quot;text/password&quot; name=&quot;&quot; value=&quot;框内初始化值&quot; size=&quot;框长&quot; maxlength=&quot;最大字符数&quot;&gt;2、提交，重置和普通按钮 &lt;input type=&quot;submit/reser/button&quot;&gt;3、单选框和复选框 &lt;input type=&quot;radio/checkbox&quot; checked=&quot;checked&quot;&gt; checked=&quot;checked&quot; 会默认该项被选中4、隐藏域 &lt;input type=&quot;hidden&quot; value=&quot;&quot; name=&quot;&quot;&gt; 一般用来提交验证信息5 多行文本 &lt;textarea name=&quot;name&quot; rows=&quot;value&quot; cols=&quot;value&quot; value=&quot;value&quot;&gt; ... ... &lt;/textarea&gt; rows和 cols指定行数和列数6、下拉菜单&lt;select name=&quot;&quot; size=&quot;列表高度&quot; multiple&gt; ### 如果使用mutiple关键字，则是一个下拉列表，不使用时是下拉框 &lt;option value=&quot;value&quot; selected&gt;选项1&lt;/option&gt; ### 指定了selected关键字是默认选中项，value的值是要发送给服务器上的数据 &lt;option value=&quot;value&quot;&gt;选项2&lt;/option&gt; &lt;option value=&quot;value&quot;&gt;选项3&lt;/option&gt;&lt;/select&gt;]]></content>
      <categories>
        <category>技术</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>Html</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mongodb 学习笔记]]></title>
    <url>%2F2016%2F12%2F01%2F%E6%8A%80%E6%9C%AF%2Fmongodb%20%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[ubuntu安装，mongodb apt-get install mongodb 安装好之后，服务自动运行，service mongodb stop停止服务，找到数据库存储位置/var/lib/mongodb/和日志文件位置/var/log/mongodb/mongodb.log,然后手动运行服务mongod --dbpath /var/lib/mongodb/ --logpath /var/log/mongodb/mongodb.log --logappend &amp; 如果想作为服务一直运行 修改/etc/mongodb.conf参数，允许远程登录和身份认证， 123456789101112131415161718192021# mongodb.conf# Where to store the data.dbpath=/var/lib/mongodb#where to loglogpath=/var/log/mongodb/mongodb.loglogappend=true# 允许远程注释掉此处bind_ip = 127.0.0.1# 修改端口在此处port = 27017# Enable journaling, http://www.mongodb.org/display/DOCS/Journalingjournal=true# 默认是关闭认证的，开启账号密码认证改此处# Turn on/off security. Off is currently the default#noauth = trueauth = true# Verbose logging output.verbose = true 然后运行服务service mongodb restart，本地连接mongodb后，给某个数据库添加用户，然后重启service。给admin添加的用户有数据库管理员权限。 1234567891011121314MongoDB shell version: 2.4.9connecting to: test&gt; show dbsadmin (empty)local 0.078125GB&gt; use adminswitched to db admin&gt; db.addUser('mongo','mongo123')&#123; "user" : "mongo", "readOnly" : false, "pwd" : "6a524e081c761e95c4d0b0096840d87c", "_id" : ObjectId("591078d7a129373b2f059bb3")&#125; 连接mongodb 指定数据库及身份信息 mongo ip/dbname -u username -p password 先连接，后认证 123mongo ipuse dbnamedb.auth('username', 'password') URI MONGO_URI = &#39;mongodb://localhost:27017&#39; MONGO_URI = &#39;mongodb://user:pass@localhost:27017&#39; mongodb 使用技巧 pymongo api find 查询非空 db.mycollection.find({&quot;pic_url&quot;:{$ne:null}}); 查询某个key的值 db.mycollection.find({&quot;IMAGE URL&quot;:{$ne:&#39;&#39;}}); ​ ​]]></content>
      <categories>
        <category>技术</category>
        <category>mongodb</category>
      </categories>
      <tags>
        <tag>Mongodb</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python cookbook 笔记1]]></title>
    <url>%2F2016%2F11%2F16%2F%E6%8A%80%E6%9C%AF%2Fpython-cookbook-note1%2F</url>
    <content type="text"><![CDATA[数据结构和算法 多变量同时赋值 12345678910111213141516171819202122# 左右两边必须数目一致，右边为列表或任意可迭代对象l = [&apos;a&apos;, 1, 345]a, b, c = lprint(a, b, c)a, b, c = &apos;qwe&apos;print(a, b, c)# 变量数小于赋值数，使用*args表达式a, *others1 = l*others2, b = lprint(a, b, others1, others2) #others是列表类型# 处理字符串分割时，或变长元组的序列时，使用*args变量来占位，很方便line = &apos;nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false&apos;uname, *fields, homedir, sh = line.split(&apos;:&apos;)records = [(&apos;foo&apos;, 1, 2),(&apos;bar&apos;, &apos;hello&apos;),(&apos;foo&apos;, 3, 4),(&apos;a&apos;, 4, 5, 6),]for tag, *args in records:if tag == &apos;foo&apos;: 保存一个迭代对象的最后N个元素使用大小为N的队列实现，队列 123456from collections import dequeq = deque(maxlen=3)q.append(1)q.appendleft(0)q.pop()q.popleft() 获得最大或最小的N个元素 123456789101112131415161718heapq模块有两个函数：nlargest()和nsmallest()可以完美解决这个问题。import heapqnums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]print(heapq.nlargest(3, nums)) # Prints [42, 37, 23]print(heapq.nsmallest(3, nums)) # Prints [-4, 1, 2]两个函数都能接受一个关键字参数，用于更复杂的数据结构中：portfolio = [&#123;&apos;name&apos;: &apos;IBM&apos;, &apos;shares&apos;: 100, &apos;price&apos;: 91.1&#125;,&#123;&apos;name&apos;: &apos;AAPL&apos;, &apos;shares&apos;: 50, &apos;price&apos;: 543.22&#125;,&#123;&apos;name&apos;: &apos;FB&apos;, &apos;shares&apos;: 200, &apos;price&apos;: 21.09&#125;,&#123;&apos;name&apos;: &apos;HPQ&apos;, &apos;shares&apos;: 35, &apos;price&apos;: 31.75&#125;,&#123;&apos;name&apos;: &apos;YHOO&apos;, &apos;shares&apos;: 45, &apos;price&apos;: 16.35&#125;,&#123;&apos;name&apos;: &apos;ACME&apos;, &apos;shares&apos;: 75, &apos;price&apos;: 115.65&#125;]cheap = heapq.nsmallest(3, portfolio, key=lambda s: s[&apos;price&apos;])expensive = heapq.nlargest(3, portfolio, key=lambda s: s[&apos;price&apos;])译者注：上面代码在对每个元素进行对比的时候，会以price的值进行比较。heapq.heapify(nums) //生成堆 字典一个键映射多个值将字典对应的多个值放到列表或集合里,集合元素是无序且不重复的 123456789from collections import defaultdictd = defaultdict(list)d[&apos;a&apos;].append(1)d[&apos;a&apos;].append(2)d[&apos;b&apos;].append(4)d = defaultdict(set)d[&apos;a&apos;].add(1)d[&apos;a&apos;].add(2)d[&apos;b&apos;].add(4) 有序字典在迭代或序列化时，希望字典是有序的，有序dict是普通dict内存的2倍 123456789101112131415from collections import OrderedDictdef ordered_dict():d = OrderedDict()d[&apos;foo&apos;] = 1d[&apos;bar&apos;] = 2d[&apos;spam&apos;] = 3d[&apos;grok&apos;] = 4# Outputs &quot;foo 1&quot;, &quot;bar 2&quot;, &quot;spam 3&quot;, &quot;grok 4&quot;for key in d:print(key, d[key])# 输出有序的json编码&gt;&gt;&gt; import json&gt;&gt;&gt; json.dumps(d)&apos;&#123;&quot;foo&quot;: 1, &quot;bar&quot;: 2, &quot;spam&quot;: 3, &quot;grok&quot;: 4&#125;&apos;&gt;&gt;&gt; 字典运算查找最大值和最小值，zip函数，将dict的值和value转置，直接使用max，min处理dict，计算的值是dict的key，得到的结果也是key。也可以配合sort 1234567891011121314151617181920prices = &#123;&apos;ACME&apos;: 45.23,&apos;AAPL&apos;: 612.78,&apos;IBM&apos;: 205.55,&apos;HPQ&apos;: 37.20,&apos;FB&apos;: 10.75, &apos;qw&apos;: 10.75&#125;b = zip(prices.values(), prices.keys())for i in b: print(i)//b是一个只能迭代一次的对象(205.55, &apos;IBM&apos;)(10.75, &apos;qw&apos;)(10.75, &apos;FB&apos;)(45.23, &apos;ACME&apos;)(612.78, &apos;AAPL&apos;)(37.2, &apos;HPQ&apos;)b = zip(prices.values(), prices.keys())print(min(b)) //如果有多个相同值的key存在，会再根据key的大小返回结果(10.75, &apos;FB&apos;) 寻找2个字典的相同之处 集合操作，&amp; - + 12345678910111213141516171819202122a = &#123;&apos;x&apos; : 1,&apos;y&apos; : 2,&apos;z&apos; : 3&#125;b = &#123;&apos;w&apos; : 10,&apos;x&apos; : 11,&apos;y&apos; : 2&#125;print(a.keys() &amp; b.keys())&#123;&apos;y&apos;, &apos;x&apos;&#125;print(a.items() &amp; b.items())&#123;(&apos;y&apos;, 2)&#125;print(a.items() - b.items())&#123;(&apos;z&apos;, 3), (&apos;x&apos;, 1)&#125;print(a.items() | b.items())&#123;(&apos;y&apos;, 2), (&apos;w&apos;, 10), (&apos;z&apos;, 3), (&apos;x&apos;, 11), (&apos;x&apos;, 1)&#125;# dict.values()不支持集合操作，因为不能保证值无重复，可以先转为set。# 排除字典里某些元素，构建新的字典c = &#123;key:a[key] for key in a.keys() - &#123;&apos;z&apos;, &apos;w&apos;&#125;&#125; 保持序列原有的序列顺序，同时删除重复的值 12345678910111213# 利用集合或者生成器，直接使用set会打乱原有顺序def dedupe(items): seen = set() for item in items: if item not in seen: yield item seen.add(item)a = [1, 5, 2, 1, 9, 1, 5, 10]print(list(deque(a)))# 用来去除文件里重复的行也可以with open(&apos;a.txt&apos;) as f: for line in deque(f): print(line) 命名切片 使用可读的变量名定义slice对象，增加代码可维护可读性 12345record = &apos;....................100 .......513.25 ..........&apos;# s.start, s.stop, s.step 分片对象属性，s.indices(len(record)),根据record长度重新映射分片的边界PRICE = slice(31,37)NUMBERS = slice(20,23)cost = int(record[NUMBERS]) * float(record[PRICE]) 统计序列中元素出现的次数 常规方法我们会用字典计数实现，key保存元素，value用来计数，count[&#39;s&#39;] += 1 。collections.Counter类更方便，还有most_common方法,更重要的是两个counter对象实例可以进行数学运算+，- 1234567891011121314words = [&apos;look&apos;, &apos;into&apos;, &apos;my&apos;, &apos;eyes&apos;, &apos;look&apos;, &apos;into&apos;, &apos;my&apos;, &apos;eyes&apos;,&apos;the&apos;, &apos;eyes&apos;, &apos;the&apos;, &apos;eyes&apos;, &apos;the&apos;, &apos;eyes&apos;, &apos;not&apos;, &apos;around&apos;, &apos;the&apos;,&apos;eyes&apos;, &quot;don&apos;t&quot;, &apos;look&apos;, &apos;around&apos;, &apos;the&apos;, &apos;eyes&apos;, &apos;look&apos;, &apos;into&apos;,&apos;my&apos;, &apos;eyes&apos;, &quot;you&apos;re&quot;, &apos;under&apos;]from collections import Counterword_counts = Counter(words)# 出现频率最高的3 个单词top_three = word_counts.most_common(3)print(top_three)# Outputs [(&apos;eyes&apos;, 8), (&apos;the&apos;, 5), (&apos;look&apos;, 4)]print(word_counts)Counter(&#123;&apos;eyes&apos;: 8, &apos;the&apos;: 5, &apos;look&apos;: 4, &apos;my&apos;: 3, &apos;into&apos;: 3, &apos;around&apos;: 2, &quot;don&apos;t&quot;: 1, &apos;not&apos;: 1, &apos;under&apos;: 1, &quot;you&apos;re&quot;: 1&#125;) 通过某几个字典关键字段排序字典列表 operator的itemgetter，支持多个key 123456789from operator import itemgetterrows_by_uid = sorted(rows, key=itemgetter(&apos;uid&apos;))# output[&#123;&apos;uid&apos;: 1001, &apos;lname&apos;: &apos;Cleese&apos;, &apos;fname&apos;: &apos;John&apos;&#125;, &#123;&apos;uid&apos;: 1002, &apos;lname&apos;: &apos;Beazley&apos;, &apos;fname&apos;: &apos;David&apos;&#125;, &#123;&apos;uid&apos;: 1003, &apos;lname&apos;: &apos;Jones&apos;, &apos;fname&apos;: &apos;Brian&apos;&#125;, &#123;&apos;uid&apos;: 1004, &apos;lname&apos;: &apos;Jones&apos;, &apos;fname&apos;: &apos;Big&apos;&#125;]rows_by_lfname = sorted(rows, key=itemgetter(&apos;lname&apos;,&apos;fname&apos;))# itemgetter()有时候也可以用lambda表达式代替，lambda比较慢，比如：rows_by_fname = sorted(rows, key=lambda r: r[&apos;fname&apos;])rows_by_lfname = sorted(rows, key=lambda r: (r[&apos;lname&apos;],r[&apos;fname&apos;]))# itemgetter同样适用于min、max的key关键字参数 排序不支持比较的对象 sorted方法的key参数接收一个callable对象，该对象作用于每个排序的个体，根据其返回的值作为排序比较的依据，同理max，min函数。加入比较的是一类对象，key的接收函数可以定义为lambda函数，也可以使用operator.attrgetter(‘user_id’） 12345sorted(users, key=lambda u: u.user_id) ### 二者均可，但是attrgetter运行速度更快，而且支持多个变量&gt;&gt;&gt; from operator import attrgetter&gt;&gt;&gt; sorted(users, key=attrgetter(&apos;user_id&apos;))[User(3), User(23), User(99)] 根据某个字段将数据分组 先将数据排序，然后使用itertools.groupby(&#39;key_field&#39;)，或者使用 collections.defaultdict（list）, dict_a[row[&#39;key_field&#39;]].append(row) 1234567891011121314151617181920212223242526272829303132from operator import itemgetterfrom itertools import groupbyrows = [&#123;&apos;address&apos;: &apos;5412 N CLARK&apos;, &apos;date&apos;: &apos;07/01/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;5148 N CLARK&apos;, &apos;date&apos;: &apos;07/04/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;5800 E 58TH&apos;, &apos;date&apos;: &apos;07/02/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;2122 N CLARK&apos;, &apos;date&apos;: &apos;07/03/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;5645 N RAVENSWOOD&apos;, &apos;date&apos;: &apos;07/02/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;1060 W ADDISON&apos;, &apos;date&apos;: &apos;07/02/2012&apos;&#125;,&#123;&apos;address&apos;: &apos;4801 N BROADWAY&apos;, &apos;date&apos;: &apos;07/01/2012&apos;&#125;,]l = groupby(rows, key=itemgetter(&apos;date&apos;)) //l值能被迭代一次for i, j in l: print(i) for z in j: print(z)# 输出结果07/01/2012&#123;&apos;date&apos;: &apos;07/01/2012&apos;, &apos;address&apos;: &apos;5412 N CLARK&apos;&#125;&#123;&apos;date&apos;: &apos;07/01/2012&apos;, &apos;address&apos;: &apos;4801 N BROADWAY&apos;&#125;07/02/2012&#123;&apos;date&apos;: &apos;07/02/2012&apos;, &apos;address&apos;: &apos;5800 E 58TH&apos;&#125;&#123;&apos;date&apos;: &apos;07/02/2012&apos;, &apos;address&apos;: &apos;5645 N RAVENSWOOD&apos;&#125;&#123;&apos;date&apos;: &apos;07/02/2012&apos;, &apos;address&apos;: &apos;1060 W ADDISON&apos;&#125;07/03/2012&#123;&apos;date&apos;: &apos;07/03/2012&apos;, &apos;address&apos;: &apos;2122 N CLARK&apos;&#125;07/04/2012&#123;&apos;date&apos;: &apos;07/04/2012&apos;, &apos;address&apos;: &apos;5148 N CLARK&apos;&#125;### 使用defaultdictfrom collections import defaultdictrows_by_date = defaultdict(list)for row in rows: rows_by_date[row[&apos;date&apos;]].append(row) 过滤序列元素 最简单的方法，当过滤规则简单时，使用列表推导式或生成器表达式；当过滤规则负责时，编写一个过滤规则函数(对每个元素返回True或False)，然后使用内建的filter()函数，filter(func, list_a)得到的是生成器，想要得到列表，可以使用list()函数。 1234567891011121314151617mylist = [1, 4, -5, 10, -7, 2, 3, -1]l = [n for n in mylist if n &gt; 0]# 上述的列表推导式会占用大量的内存，使用生成器表达式迭代产生要过滤的元素pos = (n for n in mylist if n &gt; 0)for i in pos: print(i)def is_int(i): try: x = int(i) return True except ValueError: return Falsel = list(filter(is_int, &apos;123wertsdfg&apos;))print(l)# 输出 [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;] 使用列表推导式，可以同时转换(处理)符合条件的数据；也可以替换(处理)不符合条件的结果，而不是直接丢弃。 123import mathl1 = [math.sqrt(n) for n in mylist if n &gt; 0]l2 = [n if n &gt; 0 else 0 for n in mylist] 另外一个值得关注的过滤工具就是itertools.compress()，它以一个iterable对象和一个相对应的Boolean选择器序列作为输入参数。然后输出iterable对象中对应选择器为True的元素,是个生成器。当你需要用另外一个相关联的序列来过滤某个序列的时候，这个函数是非常有用的. 12345678910# 关键点在于先创建一个Boolean序列，指示哪些元素复合条件。然后compress()函数根据这个序列去选择输出对应位置为True的元素。from itertools import compresss = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;]count = [99, 20, 16, 71, 56]big50 = [n &gt; 50 for n in count]l = list(compress(s, big50))print(l)# 输出 [&apos;a&apos;, &apos;d&apos;, &apos;e&apos;] 从字典中提取子集 首先考虑字典推导,dict()函数传入元组的推导式也可以实现，但比字典推到速度慢1倍： 1234567891011121314prices = &#123;&apos;ACME&apos;: 45.23,&apos;AAPL&apos;: 612.78,&apos;IBM&apos;: 205.55,&apos;HPQ&apos;: 37.20,&apos;FB&apos;: 10.75&#125;# Make a dictionary of all prices over 200p1 = &#123;key: value for key, value in prices.items() if value &gt; 200&#125;# Make a dictionary of tech stockstech_names = &#123;&apos;AAPL&apos;, &apos;IBM&apos;, &apos;HPQ&apos;, &apos;MSFT&apos;&#125;p2 = &#123;key: value for key, value in prices.items() if key in tech_names&#125;# dict()方式慢p1 = dict((key, value) for key, value in prices.items() if value &gt; 200) 映射名称到序列， 命名元组替代不可读的下标，或作为有大量字典元素的替代，节省空间，但命名元组的元素是不可修改的，（可以_replace()方法更新，如果需要大量更新，则不适合使用nametuple） 12345678910111213141516171819202122232425from collections import namedtuplenamedtuple(typename,field_names,verbose=False,rename=False)# 返回一个类，但该类支持元组操作，是元组类的一个子类Subscriber = namedtuple(&apos;Subscriber&apos;, [&apos;addr&apos;, &apos;joined&apos;])sub = Subscriber(&apos;jonesy@example.com&apos;, &apos;2012-10-19&apos;)print(sub.addr, sub.joined)a, b = sublen(sub)# 使用命名元组增加可读性的一个例子from collections import namedtupleStock = namedtuple(&apos;Stock&apos;, [&apos;name&apos;, &apos;shares&apos;, &apos;price&apos;])def compute_cost(records): //records [[price, nums],] total = 0.0 for rec in records: s = Stock(*rec) total += s.shares * s.price return total## 如果非要改变命名元组的属性，可以使用_replace()方法。该方法常用于填充数据，返回一个新的元组实例# 比如我们命名了一个具有缺省值的默认元组，用该方法创建实例stock_prototype = Stock(&apos;&apos;, 0, 0.0, None, None)def dict_to_stock(s): return stock_prototype._replace(**s) 转换并同时处理数据 处理函数与生成器参数结合 123456s = sum(x*x for x in nums)import os if any(name.endswith(&apos;.py&apos;) for name in os.listdir(&apos;.&apos;)): print(1)s = (&apos;ACME&apos;, 50, 123.45)print(&apos;,&apos;.join(str(x) for x in s)) 合并多个字典或映射使用collections.ChainMap()或dict的update方法，update会改变原有的字典结构，或创建新的字典对象，（当原字典更新时，update无法反应这种改变） 12345678910a = &#123;&apos;x&apos;: 1, &apos;z&apos;: 3 &#125;b = &#123;&apos;y&apos;: 2, &apos;z&apos;: 4 &#125;# 现在假设你必须在两个字典中执行查找操作(比如先从a中找，如果找不到再在b中找)from collections import ChainMapc = ChainMap(a,b) //字典的列表容器，仍支持大部分字典的方法print(c[&apos;x&apos;]) # (from a)print(c[&apos;y&apos;]) # (from b)print(c[&apos;z&apos;]) # (from a)，永远只输出第一个匹配字典的结果# 对于字典的更新或删除操作总是影响的是列表中第一个字典。del c[&apos;z&apos;] 字符串和文本日期和时间迭代器和生成器文件和IO数据编码和处理函数类和对象元编程模块和包网络和web编程并发编程脚本和系统管理异常、调试、测试]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix部署]]></title>
    <url>%2F2016%2F10%2F30%2F%E6%8A%80%E6%9C%AF%2Fzabbix%2F</url>
    <content type="text"><![CDATA[zabbix appliance 部署 登陆 appliance：zabbix 修改中文支持修改你的 locales . inc . php 这个文件，/usr/share/zabbix/include/ 下‘zhCN’ =&gt; [‘name’ =&gt; (‘Chinese (zh_CN)’), ‘display’ =&gt; true], #也就是把false改为true，在web页面点击用户信息头像，选择语言。 解决图形字体乱码修改你的defines.inc.php 这个文件 1234#修改第93行define(&apos;ZBX_FONT_NAME&apos;, &apos;msyh&apos;); #修改第45行改为 define(&apos;ZBX_GRAPH_FONT_NAME&apos;, &apos;msyh&apos;) 然后下载微软雅黑字体，传入zabbix/fonts下，架设服务器，python -m SimpleHTTPServer 8080 配置snmp server]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[魅蓝3s移动版刷机+修改串号]]></title>
    <url>%2F2016%2F10%2F24%2F%E6%8A%98%E8%85%BE%2F2016-10-24-meizu-mobile-meilan3s-imei-change%2F</url>
    <content type="text"><![CDATA[最近魅蓝3s移动版，但是移动定制版，开机logo，系统软件，功能限制，网络限制，很恶心，果断刷机。 刷机教程刷机教程魅族论坛有。 移动定制版的系统recovery会验证固件，因此直接刷全网通版的固件会提示固件损坏，无法写入固件。 使用flashfire刷入全网通的update.zip， 首先获取root权限，安装superSU，并重启。 然后flashfire写入，等待系统自动重启。这时的系统并不是clean的，会有各种问题，相当于只是从移动版升级到全网通，绕过系统的rom检测。 开机后，使用系统自带的recovery再刷一次全网通的固件，刷时选择清除数据。这是最后一步，然后就是全网通版的魅蓝3s啦。如果还是有相机图库无法使用的情况，恢复出厂设置，或再刷一次全网通固件，记得选清除数据。 修改串号 在拔号按##3646633##–&gt; 进入connectivity 选 cds information –&gt; radioinformation 选 phone1(SIM1) 在command (有A+的位置)列按入”AT +EGMR = 1,7,”你的IMEI””(在输入sim2时改为”AT +EGMR = 1,10, “你的IMEI”) 按 “SEND ATCOMMAND” 完成后重启手机 注意：步骤4中AT和+加个空格（AT +EGMR=1,7,””像这样，+号前加个空格） 这个不用root 我试过可以改魅族note2 以下是网上找到的参考 系统必须降价到Flyme 4.5，因为Flyme5.1工程模式(拨号界面输入*#*#3646633#*#*)没有cds information 选项，无法进行下一步操作； 选 cds information --&gt; radioinformation之后，选择phone1(SIM1) or phone2(SIM2)时候，最好把2个都搞一下，因为我才开始选择phone1(SIM1)，但是输入*#06#后，还是原来的串号，但是工程模式下却是我要修改的串号，但是再改phone2(SIM2)时候，两个就一样了； 在输入AT + EGMR = 1,7,”你的IMEI”这一串代码时候，请注意，把原来的AT+删掉，直接输入这一串代码-----AT + EGMR = 1,7,&quot;你的IMEI&quot;（AT和+之间有空格，一定要有）； 改完之后英文显示ok，要重启，然后再看是不是已经改成功了 用工具侠或者移动叔叔 应该也是要在Flyme4.5下进行，昨天我是在Flyme5.1下弄的，始终不行 使用工具侠查看iemi，更改成功。]]></content>
      <categories>
        <category>折腾</category>
        <category>手机</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Git服务器]]></title>
    <url>%2F2016%2F09%2F22%2F%E6%8A%80%E6%9C%AF%2Fgit-server-on-centos%2F</url>
    <content type="text"><![CDATA[cent os 搭建git server + gitolite 爬了很久的坑，终于爬出来了，真累。说说走的弯路，最初打算配置gitlab（有和github一样漂亮的web界面），然后下源码编译，失败。后来发现gitlab有中文站，提供二进制包，可能是centos32位的原因，为了少浪费生命，果断放弃，转而采用git-core + gitolite。后来，git服务器搭好了，但是没人用，实验室还是在用SVN。估计是迁移和学习成本吧，不愿意改变。 1、安装git1.1、创建git用户12345678# yum update# yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel perl-devel# mkdir -p /home/git //创建git用户的目录# groupadd git //创建git用户组# useradd -g git -d /home/git -s /bin/bash git // 添加linux用户git# passwd git //设置git用户密码# passwd -d git //在所有工作完成后禁用git用户的密码，这样git无法ssh远程登录主机 1.2、更改git目录权限12# chown -R git:git /home/git# chmod -R 2755 /home/git 1.3、源码安装git先从官网 http://git-scm.com/download 上下载 git源码，然后解压，tar -zxvf git-xxx.xxx.tar.gz 1234567891011121314系统默认版本为1.8.3，版本太老，删除并通过源代码安装删除安装的gityum -y remove git安装git依赖包$ sudo yum install curl-devel expat-devel gettext-devel openssl-devel perl-devel zlib-devel下载源码并编译安装 mkdir /tmp/git &amp;&amp; cd /tmp/git curl --progress https://www.kernel.org/pub/software/scm/git/git-2.8.2.tar.gz | tar xz cd git-2.8.2/ ./configure make make prefix=/usr/local install为确保$PATH环境变量生效，需要重新连接后执行git --versionctrl + d 关闭ssh会话，然后重新ssh登录。 2、安装gitolite 参照官方的quick install，只需3行命令 12345678910111213# su – git //切换到git用户下安装$ pwd /home/git$ mkdir bin// 获取源码$ git clone https://github.com/sitaramc/gitolite.git或者git clone git://github.com/ossxp-com/gitolite.git $ ls bin gitolite// 安装$ ./gitolite/install --to /home/git/bin/$ ls bin/commands gitolite gitolite-shell lib syntactic-sugar triggers VERSION VREF 3、配置gitolite管理员首先生成rsa key,然后用管理员的public key初始化gitolite 1234567891011121314// Windows安装git for windows2.6.2，安装完后打开Git Bash$ ssh-keygen –f admin –C 20151030@qq.com$ scp admin.pub git@serverIP:/tmp/admin.pub// 切换到git用户，为gitolite配置管理员$ su - git$ bin/gitolite setup -pk /tmp/admin.pub Initialized empty Git repository in /home/git/repositories/gitolite-admin.git/Initialized empty Git repository in /home/git/repositories/testing.git/WARNING: /home/git/.ssh missing; creating a new one (this is normal on a brand new install)WARNING: /home/git/.ssh/authorized_keys missing; creating a new one (this is normal on a brand new install)$ ls bin gitolite projects.list repositories 4、ssh配置服务端 123456789su -// vim /etc/ssh/sshd_config，将下面几句前面的#号去掉RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keysAuthorizedKeysCommand noneAuthorizedKeysCommandRunAs nobody// 重启sshservice sshd restart 客户端的git bash， 1234在 ~/.ssh/config 添加以下内容，以便连接到服务器Host 192.168.1.254Compression yesIdentityFile ~/.ssh/id_rsa 5、gitolite添加用户、库，设置用户管理权限12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455561. windows安装TortoiseGit2. 新建git文件夹，进入，右键Git Clone…URL: git@serverIP:gitolite-admin.git 勾选Load Putty Key，选择你的私钥（此处需使用ppk格式私钥，可以使用puttygen将私钥admin转换成admin.ppk） 点击OK即可clone服务器上的gitolite-admin文件夹到本地3. 添加用户管理员将其他用户的公钥(如dong.pub)复制到gitolite-admin/keydir/下4. 添加库进入gitolite-admin/conf/，右键Git Bash Here，$ vim gitolite.conf (此文件用于添加库和配置用户对库的权限)repo gitolite-admin RW+ = adminrepo testing RW+ = @all repo git-dong #新建库git-dong RW+ = admin dong #设置用户admin dong有读写权限 将修改push到服务器，即可添加库和用户。:wq$ cd ..$ git status #查看git库状态$ git add keydir/dong.pub conf/gitolite.conf 或者git add .$ git commit –m “add repo git-admin;add user dong”进入gitolite-admin文件夹，右键TortoiseGitPush点击OK即可。5. 用户权限管理在客户端clone gitolite-admin.git,编辑gitolite-admin/conf/gitolite.conf配置各用户权限。1 @team1 = zc2 @team2 = aws zc3 repo gitolite-admin4 RW+ = admin56 repo ossxp/.+7 C = admin8 RW = @all910 repo testing11 RW+ = admin12 RW master = junio13 RW+ pu = junio14 RW cogito$ = pasky15 RW bw/ = linus16 - = somebody17 RW tmp/ = @all18 RW refs/tags/v[0-9] = junio在上面的示例中，我们演示了很多授权指令。• 第1行，定义了用户组 @admin，包含两个用户 jiangxin 和 wangsheng。• 第3-4行，定义了版本库 gitolite-admin。并指定只有用户 jiangxin 才能够访问，并拥有读(R)写(W)和强制更新(+)的权限。• 第6行，通过正则表达式定义了一组版本库，即在 ossxp/ 目录下的所有版本库。• 第7行，用户组 @admin 中的用户，可以在 ossxp/ 目录下创建版本库。创建版本库的用户，具有对版本库操作的所有权限。• 第8行，所有用户都可以读写 ossxp 目录下的版本库，但不能强制更新。• 第9行开始，定义的 testing 版本库授权使用了引用授权语法。• 第11行，用户组 @admin 对所有的分支和里程碑拥有读写、重置、添加和删除的授权。• 第12行，用户 junio 可以读写 master 分支。（还包括名字以 master 开头的其他分支，如果有的话）。• 第13行，用户 junio 可以读写、强制更新、创建以及删除 pu 开头的分支。第14行，用户 pasky 可以读写 cogito 分支。 (仅此分支，精确匹配）。 以下是通过修改gitolite-admin库，管理git用户和权限。添加新用户wuxiaohui的公钥，赋予其读写testing.git库的权限。 123456789## 对权限文件配置gitolite.conf的修改repo gitolite-admin RW+ = adminrepo testing RW+ = @allrepo new-source RW+ = wuxiaohui Chenchuneng maning 修改完成后push到git server。 12345678910111213141516171819202122232425262728293031shuaiyy@shuaiyy-PC MINGW64 /g/git/gitolite-admin (master)$ git statusOn branch masterYour branch is up-to-date with &apos;origin/master&apos;.Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: conf/gitolite.confUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) keydir/wuxiaohui.pubno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)shuaiyy@shuaiyy-PC MINGW64 /g/git/gitolite-admin (master)$ git add keydir/wuxiaohui.pub conf/gitolite.confwarning: LF will be replaced by CRLF in keydir/wuxiaohui.pub.The file will have its original line endings in your working directory.shuaiyy@shuaiyy-PC MINGW64 /g/git/gitolite-admin (master)shuaiyy@shuaiyy-PC MINGW64 /g/git/gitolite-admin (master)$ git commit -m &quot;add repo new-source;add user wuxiaohui&quot;[master bf3f6fe] add repo new-source;add user wuxiaohuiwarning: LF will be replaced by CRLF in keydir/wuxiaohui.pub.The file will have its original line endings in your working directory. 2 files changed, 4 insertions(+) create mode 100644 keydir/wuxiaohui.pub 6、git基础教程6.1、基本操作 git init要想使用git进行版本管理，必须先初试化仓库。新建一个文件夹，在该文件夹下执行 git init 命令，就可以将该文件夹初始化为git仓库，其下会生成.git的文件夹，记录git的版本追踪管理，千万不要删除或更改。 git status显示当前仓库的状态，，如 处于master分支，没有修改内容。 git add我们创建的文件不会被自动加入git仓库的管理文件中，必须使用git add filename或git add .使其加入本地的Index暂存区。然后git status 查看改变。 git commit -m “First Commit !”将git add的内容提交，保存到版本管理的历史记录中。git status 查看状态。 git log查看日志，其中 commit ID是版本号，如果想要改变到某个版本，需要指定版本号。git log file/dir 查看指定文件或文件夹的日志 git diff 查看当前状态和最新提交，历史版本的不同之处。其中+代表新增，-代表删除； 6.2、分支操作 git branch 显示分支一览表 git checkout -b 创建、切换分支 git merge 合并分支 git log –graph 以图表形式查看分支 6.3、更改提交 git reset 回溯历史版本 git commit –amend 修改提交信息 git rebase -i 压缩历史 6.4、推送至远程仓库 git remote add 添加远程仓库 git push 推送至远程仓库 6.5、从远程仓库获取 git clone 获取远程仓库到本地 git pull 获取最新的远程仓库分支 6.6、在线git命令练习 LearnGitBranching 提供在线的git命令学习与实践 Try Git 6.7、git命令备忘录master: 默认开发分支 origin: 默认远程版本库 Head: 默认开发分支 Head^: Head的父提交 创建版本库12$ git clone &lt;url&gt; #克隆远程版本库$ git init #初始化本地版本库 修改和提交123456789$ git status #查看状态$ git diff #查看变更内容$ git add . #跟踪所有改动过的文件$ git add &lt;file&gt; #跟踪指定的文件$ git mv &lt;old&gt;&lt;new&gt; #文件改名$ git rm&lt;file&gt; #删除文件$ git rm --cached&lt;file&gt; #停止跟踪文件但不删除$ git commit -m &quot;commit messages&quot; #提交所有更新过的文件$ git commit --amend #修改最后一次改动 查看提交历史123$ git log #查看提交历史$ git log -p &lt;file&gt; #查看指定文件的提交历史$ git blame &lt;file&gt; #以列表方式查看指定文件的提交历史 撤销1234$ git reset --hard HEAD #撤销工作目录中所有未提交文件的修改内容$ git checkout HEAD &lt;file&gt; #撤销指定的未提交文件的修改内容$ git revert &lt;commit&gt; #撤销指定的提交$ git log --before=&quot;1 days&quot; #退回到之前1天的版本 分支与标签1234567$ git branch #显示所有本地分支$ git checkout &lt;branch/tag&gt; #切换到指定分支和标签$ git branch &lt;new-branch&gt; #创建新分支$ git branch -d &lt;branch&gt; #删除本地分支$ git tag #列出所有本地标签$ git tag &lt;tagname&gt; #基于最新提交创建标签$ git tag -d &lt;tagname&gt; #删除标签 合并与衍合12$ git merge &lt;branch&gt; #合并指定分支到当前分支$ git rebase &lt;branch&gt; #衍合指定分支到当前分支 远程操作12345678$ git remote -v #查看远程版本库信息$ git remote show &lt;remote&gt; #查看指定远程版本库信息$ git remote add &lt;remote&gt; &lt;url&gt; #添加远程版本库$ git fetch &lt;remote&gt; #从远程库获取代码$ git pull &lt;remote&gt; &lt;branch&gt; #下载代码及快速合并$ git push &lt;remote&gt; &lt;branch&gt; #上传代码及快速合并$ git push &lt;remote&gt; :&lt;branch/tag-name&gt; #删除远程分支或标签$ git push --tags #上传所有标签 7、githubGitHub 除了 Git 代码仓库托管及基本的 Web 管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能。 Github的三个精彩功能 fork，star，watch 1.想拷贝别人项目到自己帐号下就fork一下。2.持续关注别人项目更新就star一下3.watch是设置接收邮件提醒的。 7.1、开始 创建账户 完善个人信息 设置Key 添加公钥 7.2、使用 创建仓库 连接仓库 提交开源代码 Github Desktop 7.3、GitHUb Flow ————以部署为中心的开发模式GitHub Flow 保持master分支始终是可部署使用的 新的开发要从master分支创建新的分支，分支名要有描述性意义 在本地仓库的新建分支提交 PUSH本地分支作业到github服务器端的同名分支 审查测试push的新分支，确认无误后与master合并 合并后的master分支可以立即投入使用，如果有bug，和回退到提交状态之前 8、Git server 使用指导8.1、将自己的代码上传到远程仓库我在 project 目录下创建了4个用户对应的仓库 Chenchuneng.git Maning.git Wuxiaohui.git Wangba.git因此对应的git仓库地址为： git@121.41.15.6:project/Wangba.git使用git clone 克隆到本地： 假如我的workspace目录下有2个项目源码，project1，project2，想上传project1到远程仓库 Wangba.git。 在workspace下打开git bash here，执行 git init 初试化本地仓库 git add project1,添加需要git log的文件，git add .可以添加所有变更的文件，省事。 git commit -m &quot; 备注&quot;，提交变化到本地仓库 git remote add mywork git@121.41.15.6:project/Yangshuai.git,为本地仓库添加远程仓库，命令中的mywork 是我们为远程仓库起的别名（可以自己任取），然后push的时候使用别名 mywork 就可以少打很多字了。 将本地仓库push到服务器上的远程仓库，右键workspace，选择Tortogit小乌龟里的push命令： 以后修改添加的代码想要上传至git服务器，重复 2 3 5步骤，即 add commit push。其他目录下的代码想要上传的话，重复步骤 1-6 。 注意：git clone 和git push 我们都是用的程序而不是命令行，是因为在git bash 下如何配置git服务器认证所需的ssh-key暂时没找到解决方法，而GUI工具可以”load ssh key”。]]></content>
      <categories>
        <category>技术</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 模板语法]]></title>
    <url>%2F2016%2F08%2F19%2F%E6%8A%80%E6%9C%AF%2FDjango%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Django模板语法模板变量 {{var_name}} ,view视图传入参数可以为 Context({'name':'vale',}) {{dict.key}},view视图传入参数为dict字典 {{object_name.attribute 或 method}} ,view视图传入一个对象object。 {{list_name.N}},view视图出入一个list，可以取出list的第N个元素。 条件分支 {% if user.age < 18 %} 未成年{% else %} XXX {% endif %} 条件中的逻辑关系词 and or not，其中and与or不能同时使用。 循环 for 123456789101112&#123;% raw %&#125;&#123;% for book in booklist reversed %&#125; &#123;&#123;book&#125;&#125;&#123;&#123;% empty %&#125;&#125; &lt;li&gt;&#123;&#123;forloop.counter&#125;&#125;&lt;/li&gt; &lt;li&gt;&#123;&#123;forloop.revcounter&#125;&#125;&lt;/li&gt; &lt;li&gt;&#123;&#123;foorloop.counter0&#125;&#125;&lt;/li&gt; &lt;li&gt;&#123;&#123;forloop.revcounter0&#125;&#125;&lt;/li&gt;&#123;% endfor %&#125;&#123;% for key in dict %&#125; &#123;&#123;key&#125;&#125; &#123;% endfor %&#125;&#123;% endraw %&#125; reversed 是逆序输出,{{% empty %}}判断循环体是否为空，for 没有continue和break； forloop变量forloop.counter 从1计数的当前循环次数，forloop.revcounter，剩余的循环次数forloop.counter0，从0开始计数的循序。forloop.first和forloop.last,是否为第一次和最后一次循环，True 或False。 过滤器 filter |{{book | upper | lower | capfisrt}},将变量book先变大写在变小写在变首字母大写|过滤器标识，类似于linux的管道，{{today | date:"Y-m-d"}},today变量为datetime.datetime.now()。 自定义过滤器模板自定义的过滤器位于 templatetags文件夹下，这是一个python包，里面要有一个空的__init__文件，一般一个过滤器函数为一个单独的py文件 12345from django import templateregister = template.Library()def percent(value): return value + '%'register.filter('percent',percent) 过滤器必须用register对象注册，有2个参数，name和func，也可以在func处使用装饰器，如下： 1234567from django import templateregister = template.Library()@register.filter(name='percent')def percent(value): return value + '%' #register.filter('percent',percent) 使用自定义过滤器时必须在模板中先load py文件，{% load percent %} 需要重启服务器使过滤器生效]]></content>
      <categories>
        <category>技术</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 学习笔记(二)]]></title>
    <url>%2F2016%2F08%2F16%2F%E6%8A%80%E6%9C%AF%2FDjango%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%2F</url>
    <content type="text"><![CDATA[Django进阶1、用户信息扩展1.1、使用profile扩展在models定义一个profile表，使用外键关联 1.2、继承AbstractUser在models创建继承AbstractUser的User类，为其增加新的字段，比如QQ号，手机号。在setting里设置字段 AUTH_USER_MODEL = &#39;blog.User&#39;在admin注册自己的User。 2、自定义认证2.1、自定义认证方式2.2、session保持3、权限设计和使用3.1、用户注册4、自定义模板库base文件和base.html 123456\&#123;\% block head_css \%\&#125; &lt;继承扩展此处部分&gt;\&#123;\% endblock \%\&#125;\&#123;\% block head_css \%\&#125; &lt;继承扩展此处部分&gt;\&#123;\% endblock \%\&#125; 模板文件中引用base.html,重载里面需要定制的block，其他的继承base里默认的。 1234\&#123;\% extend "base/base.html" \%\&#125;\&#123;\% block container \%\&#125;实现部分\&#123;\% endblock \%\&#125; 5、自定义标签在app目录下建立templatetags文件夹， 12345678910111213141516from django import templateregister = template.Library()class UpperNode(template.Node): def __init__(self,nodelist): self.nodelist = nodelist def render(self,context): content = self.nodelist.render(context) return content.lower()@register.tagdef upper(parse,token): nodelist = parse.parse("endupper") parse.delete_first_token() return UpperNode(nodelist) 在模板中加载使用自定义标签： 1234\&#123;\% load upper \%\&#125; \&#123;\% upper \%\&#125; upper标签将作用于此块的内容 \&#123;\% endupper \%\&#125; 6、自定义过滤器在templatetags下创建 assets.py,实现区分开发环境的assets资源定位 123456789from django import templateregister = template.Library()from blog.settings import is_Debug@register.filterdef assets(value): if is_debug: return '/static/'+value return '/static/assets/' +value 在模板中使用过滤器： 1234&#123;% raw %&#125;&#123;% load assets %&#125; &lt;link src="&#123;&#123;'1.css' | assets&#125;&#125;"/&gt;&#123;% endraw %&#125; 7、model的定义和同步，增删改在models.py中导入django.db 的models,数据类定义：在models.py中定义一个评论类123456789101112class review(models.Model): user = models.ForeignKey(User) new = models. content = models.TextField creat_time = models.DateTimeField() is_dele = models.BoolenField(default = 0) def __str__(self): return self.content class Meta: permission = () 同步数据库: 增加数据：在视图views中导入数据类，比如user类，然后创建user对象的实例，然后调用save方法。修改：先get实例，然后赋值修改，最后save 删除： 1、修改is_dele标志字段 2、删除数据实例，先获取实例。然后调用delete()方法 8、django单表查询和多表查询ORM查询 多表查询用到外键 9、聚合查询Q协助查询，实现与或非。12from django.db.models import Qres_list = news.objects.filter(Q(title = 'today')|Q(body = 'new')) 10、ORM无法满足查询需求时，直接使用SQL语句查询10.1、raw() XXX.objects.raw(‘SQL 语句’)，但raw的SQL语句里，必须包含主键；对复杂语句支持不好 12res_list = user.objects.raw('select * from blog_user where id = 1 or sex = "女" ') 10.2、cursor()12345from django.db import connection,transactioncursor = connection.cursor()sql = 'select * from user'cursor.execute(sql)res_list = cursor.fetchall() #每条记录是一个元组 11、querysets和惰性机制querysets查询结果集，可以遍历，也可以继续执行查询筛选，res_list = news.objects.filter(Q(title = &#39;today&#39;)|Q(body = &#39;new&#39;)) 惰性机制：只有querysets被使用到，其数据查询才被执行，并返回结果。 1234res_list = news.objects.filter(Q(title = 'today')|Q(body = 'new')) res_list.order_by('age')res_dict = res_list.value('id','name') 12、自定义manager管理器当ORM的方法无法满足我们的需求时， 新增manager方法类似article.objects.all()就是一个manager，用来获取数据的方法，我们可以定义自己的manager，在models.py中新建类，继承models.Manager， 12345678910111213141516class PollManager(models.Manager): def with_counts(self): from django.db import connection with connection.cursor() as cursor: cursor.execute(""" SELECT p.id, p.question,p.poll_date, COUNT(*) FROM polls_opinionpoll p, polls_response r WHERE p.id = r.poll_id GROUP BY p.id, p.question, p.poll_date ORDER BY p.poll_date DESC""") result_list = [] for row in cursor.fetchall(): p = self.model(id=row[0], question=row[1],poll_date=row[2]) p.num_responses = row[3] result_list.append(p) return result_listclass OpinionPoll(models.Model): question = models.CharField(max_length=200) poll_date = models.DateField() objects = PollManager() 然后就可以使用OpinionPoll.objects.with_count()方法 改变原有的querySet方法覆盖重写原有方法。 13、Form表单在views.py中定义form类：1234567891011from django import formsclass LoginForm(forms.Form) email = forms.CharField(label = 'email',max_length =100) pwd = forms.CharField(label ='password',widget = forms.PasswordInput)#添加验证 def clean_email(self): pass def clean(self): if len(self.cleaned_data["email"].split('@'))&lt;2: raise forms.ValidationError('email is wrong!') return email django开发实践技巧：1、从setting.py中读取配置信息为全局使用settings中： 123456789101112131415161718192021222324252627#网站的基本信息配置SITE_URL = 'http://localhost:8000/'SITE_NAME = '个人博客'SITE_DESC = '专注Python开发，欢迎和大家交流'WEIBO = 'http://weibo.com/holdhiitfitness/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo&amp;is_all=1'WEIBO_TENCENT = 'http://weibo.com/holdhiitfitness/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo&amp;is_all=1'PRO_RSS = 'http://www.abroadrecommend.com'PRO_EMAIL = 'johnson_hugh@163.com'# 在templates中配置处理器TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [ os.path.join(BASE_DIR,'templates'), ], 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', 'blog.views.global_setting' # 注意此处 ], &#125;, &#125;,] 在 views中定义templates处理器的方法12345from django.conf import settingsdef globe_setting(request): return &#123;'SITE_URL':settings.SITE_URL, 'SITE_NAME':settings.SITE_NAME &#125; 在模板文件中，可直接引用变量{{SITE_URL}} 2、数据库模型设计技巧使用工具 PowerDesign、ERWin、Visio、Navicat Data MOdeler 分析可能存在的数据库表 分析可能会有的数据列，以及对应的数据类型和约束。 设计数据模型图 在modles.py中定义模型： 1234567891011class Tag(models.Model): name = models.CharField(max_length =30,verbose_name ='标签名称') class Meta: # 在admin管理界面可以看到 verbose_name = '标签' verbose_name_plural = verbose_name def __unicode__(self): #print 对象时的输出 return self.name# 自定义User modelclass User(AbstactUser): avatar = models.ImageField(upload_to ='url',default ='avatar/default.png',max_length =200,) QQ = models.CharField() 自定义user model需要在setting中添加参数：AUTH_USER_MODEL= &#39;blog.User&#39;，可以继承abstactUser，或关联外键扩展。 mysql配置 settings: 123456789101112DATABASES = &#123; 'default': &#123; #配置别忘了用逗号，否则不被识别为元组 'ENGINE': 'django.db.backends.mysql', 'NAME': 'blogdb', #开发环境可用，生产环境不要用 'USER': 'root', 'PASSWORD': '123456', 'HOST': '', 'PORT': '',#默认3306 &#125;&#125; 在mysql中创建blogdb数据库，utf-8编码。 富文本编辑框：使用kindeditor，下载kindeditor，解压文件放到/static/js/kindeditor 下面。定义媒体文件 1234class Media： js = ('/static/js/kindeditor-4.10/kindeditor-min.js', '/static/js/kindeditor-4.10/lang/zh-CN.js', '/static/js/kindeditor-4.10/config.js',) 注意新建配置文件config.js，参考官方文档：K.create(‘#id’),#id 是对应html页面中需要富文本编辑器的网页元素 12345678KindEditor.ready(function(K) &#123; K.create('textarea[name=content]',&#123; width:'800px', height:'200px', //配置上传地址，这个地址在url.py中已经配置好了，要和它对应 uploadJson: '/admin/upload/kindeditor', &#125;); &#125;); 需要自己定义文件上传接口。配置: 根目录下创建uploads，在settings中配置MEDIA_URL和MEDIA_ROOT 配置文件上传 123#上传图片设置MEDIA_URL = '/uploads/'MEDIA_ROOT = os.path.join(BASE_DIR,'uploads') 路由设置： 123456#配置用于处理图片上传的url映射url(r"^uploads/(?P&lt;path&gt;.*)$", \ #django.views.static.serve专门用于处理静态文件 "django.views.static.serve", \ #这里用到了settings中配置好的路径MEDIA_ROOT &#123;"document_root": settings.MEDIA_ROOT,&#125;), 配置富文本编辑器的文件上传 12345from blog.upload import upload_image#用于映射富文本编辑器的图片上传url(r'^admin/upload/(?P&lt;dir_name&gt;[^/]+)$', upload_image,\ name='upload_image'), 自己实现上传处理函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# -*- coding: utf-8 -*-from django.http import HttpResponsefrom django.conf import settingsfrom django.views.decorators.csrf import csrf_exemptimport osimport uuidimport jsonimport datetime as dt#这个装饰器用于不再进行表单验证提交@csrf_exemptdef upload_image(request, dir_name): ################## # 这是kindeditor想要的格式 # kindeditor图片上传返回数据格式说明： # &#123;"error": 1, "message": "出错信息"&#125; # &#123;"error": 0, "url": "图片地址"&#125; ################## result = &#123;"error": 1, "message": "上传出错"&#125; #imgFile来自于富文本编辑器查看源码之后找到的它定义的文件名字 files = request.FILES.get("imgFile", None) if files: result =image_upload(files, dir_name) return HttpResponse(json.dumps(result), content_type="application/json")#目录创建def upload_generation_dir(dir_name): today = dt.datetime.today() dir_name = dir_name + '/%d/%d/' %(today.year,today.month) if not os.path.exists(settings.MEDIA_ROOT + dir_name): os.makedirs(settings.MEDIA_ROOT + dir_name) return dir_name# 图片上传def image_upload(files, dir_name): #允许上传文件类型 allow_suffix =['jpg', 'png', 'jpeg', 'gif', 'bmp'] file_suffix = files.name.split(".")[-1] if file_suffix not in allow_suffix: return &#123;"error": 1, "message": "图片格式不正确"&#125; relative_path_file = upload_generation_dir(dir_name) path=os.path.join(settings.MEDIA_ROOT, relative_path_file) if not os.path.exists(path): #如果目录不存在创建目录 os.makedirs(path) file_name=str(uuid.uuid1())+"."+file_suffix path_file=os.path.join(path, file_name) file_url = settings.MEDIA_URL + relative_path_file + file_name #写入操作，二进制形式，最终完成上传，真正保存图片 open(path_file, 'wb').write(files.file.read()) return &#123;"error": 0, "url": file_url&#125; 配置好路由，在kindeditor的config js中配置上传路径： 12345678KindEditor.ready(function(K) &#123; K.create('textarea[name=content]',&#123; width:'800px', height:'200px', //配置上传地址，这个地址在url.py中已经配置好了，要和它对应 uploadJson: '/admin/upload/kindeditor', &#125;); &#125;); ​ 3、模板设计base.html保留不变的内容，变化的部分以\{\% block XXX \%\} \{\% endblock \%\}代替，子模板继承 base.html,\{\% extends &#39;base.html&#39;},然后重写\{\% block XXX \%\} \{\% endblock \%\}，在{\% block XXX \%} 中间添加变化的内容 {\% endblock \%}。也可将变化的部分单独放入一个文件中，使用 \{\% include &#39;XXX.html&#39; \%\} 4 、导航栏数据获取假如以category模型设计导航栏，先获取对象列表，传递变量给模板，然后在模板中展示123456# views 中from models import *class index(request)： ca_list = category.objects.all() return render(request,'base.html',&#123;'ca_list':ca_list&#125;) 在模板中 12345&#123;% raw %&#125;&#123;% for ca in ca_list %&#125;&lt;a herf =''&gt;&#123;&#123;ca.name&#125;&#125; &lt;/a&gt;&#123;% endfor %&#125;&#123;% endraw %&#125; 5、其他 locals() 将所有变量封装传递进模板，render(&#39;index.html&#39;,locals()) 代码重构：使用模板处理器，上面的global_setting的例子，将每个视图都用到的模板变量数据定义到模板处理器中。 聚合查询 annotate 异常捕获 1234try ： get_article()except Artical.NotExist: xxx 标签过滤器 {{update_date|date:'Y-m-d'}}safe 不转义变量文本里的html便签，确定变量安全的可以用该过滤器。 csrf表单验证 模板页面表单处需要 \{\% csrf_token \%\},防止跨站提交的安全手段。 不使用csrf(不推荐)，post方法使用csrf_exmp装饰器。]]></content>
      <categories>
        <category>技术</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 学习笔记(一)]]></title>
    <url>%2F2016%2F08%2F13%2F%E6%8A%80%E6%9C%AF%2Fdjango%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%2F</url>
    <content type="text"><![CDATA[Django 学习笔记1 基于python2.7和django1.10 django工作流程： 1、快速开始1.1、创建项目 django-admin.py startproject website manage.py startapp blog 修改webisite下的setting.py urls.py 1234567891011# setting 修改# installed_apps 添加你的app#To include the app in our project, we need to add a reference to its configuration class in the INSTALLED_APPS setting. The PollsConfigclass is in the polls/apps.pyfile, so its dotted pathis ’blog.apps.PollsConfig’. Edit the mysite/settings.pyfile and add that dotted path to the INSTALLED_APPSsetting.INSTALLED_APPS = [...XXX..., ’blog.apps.PollsConfig’, ]language zh-HANStime-zone zh/Shanghai# 数据库连接配置 1234567# website/urls.py修改from django.conf.urls import include, url # 注意导入includefrom django.contrib import adminurlpatterns = [ url(r'^blog/', include('blog.urls')), # include app的urls规则 url(r'^admin/', admin.site.urls), # 只有admin例外，可以不用include] ​ 1.2、创建应用 manage.py startapp blog 创建视图 1234# 编辑文件 blog/views.pyfrom django.http import HttpResponsedef index(request):return HttpResponse("Hello, world. You're at the polls index.") views可以获取url中的参数作为变量使用：url(r&#39;^(?P&lt;question_id&gt;[0-9]+)/vote/$&#39;, views.vote, name=&#39;vote&#39;),通过urls中的正则表达式匹配的命名变量: %question_id view视图模板 模板文件存放在 app/templates/app/下。在view文件中加载渲染的模板： 12345678910from django.http import HttpResponsefrom django.template import loaderfrom .models import Questiondef index(request):latest_question_list = Question.objects.order_by('-pub_date')[:5]template = loader.get_template('polls/index.html')context = &#123;'latest_question_list': latest_question_list,&#125;return HttpResponse(template.render(context, request)) 或直接使用render()函数渲染,这样就不用Httpresponse和loader了。 123456from django.shortcuts import renderfrom .models import Questiondef index(request):latest_question_list = Question.objects.order_by('-pub_date')[:5]context = &#123;'latest_question_list': latest_question_list&#125;return render(request, 'blog/index.html', context) 模板中的url软编码在urls.py中每个url匹配规则可以指定一个name参数。在模板中使用{% url 'name' %}引用URL， e.g. {{ question.question_text }} 这样，当url映射改变时，无需修改模板文件。当app很多时，在urls.py中指定url命名空间，即app_name=’XXX’,然后模板引用url时，使用{% url 'blog:detail' question_id %} 模板中的表单和view中对表单数据的处理 404视图使用try catch或get_XXX_or_404方法，推荐后者 123456789try: question = Question.objects.get(pk = question_id) except Question.DoesNotExist: raise Http404("Questin does not exist!") return render(request, 'blog/detail.html', context=&#123;'question':question&#125;) ### 或者 no use try ,we can do this : question = get_object_or_404(Question, pk=question_id) return render(request, 'polls/detail.html', &#123;'question': question&#125;) 使用generic views 12345678from django.views import genericclass IndexView(generic.ListView): template_name = 'blog/index.html' context_object_name = 'latest_question_list' def get_queryset(self): """Return the last five published questions.""" return Question.objects.order_by('-pub_date')[:5] url映射规则 12345678#编辑 blog/urls.pyfrom django.conf.urls import urlfrom . import viewsurlpatterns = [url(r'^$', views.index, name='index'),]# name参数可以在模板文件中以&#123;% raw %&#125;&#123;% url name %&#125;&#123;% endraw %&#125; 引用。# 然后将其include到website下的urls.py内 数据库设置 123#修改settings.py，配置数据库连接# 然后执行命令创建数据库manage.py migrate 创建模式 编辑 blog/models.py 123456789from django.db import modelsclass Question(models.Model): question_text = models.CharField(max_length=200) pub_date = models.DateTimeField('date published')class Choice(models.Model): question = models.ForeignKey(Question,on_delete=models.CASCADE) choice_text = models.CharField(max_length=200) votes = models.IntegerField(default=0) 激活models，执行manage.py makemigrations blog，最后应用到数据库，manage migrate。 总结：• Change your models (in models.py).• Run python manage.py makemigrations to create migrations for those changes• Run python manage.py migrate to apply those changes to the database. views中引用数据库中的数据 获得全部对象： 12345678910from .models import Questionquestion_list = Question.object.all()l1=Question.objects.filter(question_text = u'what 你好') # 对象数据过滤器，等于Question.objects.filter( age__gt = 16) # 属性age &gt; 16Question.objects.filter( age__gte = 16) # 属性age &gt;= 16Question.objects.filter( name__contains = '张') # 属性name中含有张。# 批量更新,无需调用saveQuestion.objects.filter( age__gt = 16).update(name = 'zhang')# 删除调用.delete()方法 filter过滤表达式 123456ques = Question.object.get(id = 2)ques.name = 'a'ques.date = todayques.save() #同步修改# 删除调用.delete()方法ques.delete() 获得单个对象 添加一个新对象数据 12newQ = Question(name = 'new',age =16)newQ.save() 实现实体对应关系一对一和多对多 待补充 1.3、 Django Admin create admin user 1234python manage.py createsuperusername:adminmail:password:**** ​启动server,访问 http://127.0.0.1:8000/admin/ 在admin界面注册app 的object 打开的admin管理界面默认是没有blog app数据的编辑 blog/admin.py 1234from django.contrib import admin# Register your models here.from .models import Questionadmin.site.register(Question) 个性化 app static文件夹，blog\static\blog\ 下存放css文件，模板文件中引用css和图片使用相对路径。 blog\static\blog\images\下存放图片文件，css中和模板中引用图片使用相对路径。 django favicon.ico 处理：https://pypi.python.org/pypi/django-favicon 从url传值 get 12id = request.GET['id']passwd = request.POST['password'] /argv1/argv2/… 123456views.pydef wanda(request,wanda): return HttpResponse(r' `&lt;h4&gt; %s &lt;/h4&gt;`' % wanda)urls.pyurl(r'wanda/(?P&lt;wanda&gt;\d&#123;4&#125;)/$',views.wanda), 其中变量wanda是在url正则表达式中定义的分组变量名称。如果不定义?P&lt;name&gt;,则在view视图函数中可以传入任意变量名，按对应顺序从URL匹配中取值。 2、记录踩过的坑 object has no attribute ‘_state’： django不使用init方法，参考文档creating-objects部分。 IOError: No translation files found for default language zh-CN. 经确认是新版本的django包版本中只有zh_Hans目录，没有zh_CN,把zh_Hans目录复制一个zh_CN就Ok了或者在settings里面直接改成zh-Hans，这样就不用升级完Django，还去改目录了。 debug=False后，网站无法访问静态资源图片等 需要在启动参数中添加insecure manage runserver 0.0.0.0:8080 --insecure]]></content>
      <categories>
        <category>技术</category>
        <category>django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PhantomJS + Selenium实现登陆网站签到]]></title>
    <url>%2F2016%2F08%2F10%2F%E6%8A%80%E6%9C%AF%2FPhantomJS%20%2B%20Selenium%E5%AE%9E%E7%8E%B0%E7%99%BB%E9%99%86%E7%BD%91%E7%AB%99%E7%AD%BE%E5%88%B0%2F</url>
    <content type="text"><![CDATA[PhantomJS + Selenium实现登陆网站签到 js脚本是由浏览器前端加载执行的，因此爬虫要想渲染js，就要实现js引擎，或者我们分析出js发送的请求后，自己构造(很难，复杂)。 另外一种，就是使用selenium调用浏览器自动化处理。phantomjs是一个无头浏览器，没有界面，因此运行速度要比chrome快一点，没有弹窗干扰。 requests，urllib之类的http库，只能将http资源请求下来，其中的img link js标签都不会自动加载。而浏览器会请求所有的资源。 目标网站分析 网站yrw.com,登陆后签到页面是一个js脚本控制的插件,ipinyou.com估计是显示签到效果的 1234567891011121314151617181920 &lt;script type="text/javascript"&gt; var _py = _py || []; var _userId = "0"; var _source = "0"; var _pv = "0"; _py.push(['a', 'qJ..OIEiS_boFsG_SD2lEUB5nX']); _py.push(['domain', 'stats.ipinyou.com']); _py.push(['e', "&#123;\"userId\":\"" + _userId + "\",\"source\":\"" + _source + "\"&#125;"]); _py.push(['pv', _pv]); -function (d) &#123; var s = d.createElement('script'), e = document.body.getElementsByTagName('script')[0]; e.parentNode.insertBefore(s, e), f = 'https:' == location.protocol; s.src = (f ? 'https' : 'http') + '://' + (f ? 'fm.ipinyou.com' : 'fm.p0y.cn') + '/j/adv.js'; &#125;(document);&lt;/script&gt;&lt;noscript&gt; &lt;img src="//stats.ipinyou.com/adv.gif?a=qJ..OIEiS_boFsG_SD2lEUB5nX&amp;e=" style="display:none;"/&gt;&lt;/noscript&gt; 最初用fiddler抓包，发现请求的链接很复杂，不好构造，于是决定用selenium操作浏览器。后来用Chrome的XHR发现了请求链接，囧。 1234567891011121314151617Request-Headers:GET /member/check/?_=1482648421735 HTTP/1.1Host: www.yrw.comConnection: keep-aliveAccept: application/json, text/javascript, */*; q=0.01X-Requested-With: XMLHttpRequestUser-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36Referer: https://www.yrw.com/member/homeAccept-Encoding: gzip, deflate, sdch, brAccept-Language: zh-CN,zh;q=0.8Cookie: JSESSIONID=99ED09E3560F9F944EEAE855532F8684; CNZZDATA1253600367=1638558785-1463053615-%7C1482644670Request-url:https://www.yrw.com/member/check/?_=1482648421735Response-json:&#123;&quot;error&quot;:false,&quot;page&quot;:null,&quot;result&quot;:&#123;&quot;checkDate&quot;:1482648441429,&quot;checkSource&quot;:0,&quot;createTime&quot;:null,&quot;gainPopularity&quot;:2,&quot;id&quot;:null,&quot;memberId&quot;:110850411330,&quot;popularityDouble&quot;:1&#125;,&quot;resultCode&quot;:null,&quot;resultCodeEum&quot;:null,&quot;resultCodeList&quot;:[],&quot;resultList&quot;:null,&quot;success&quot;:true&#125; 签到脚本12345678910111213141516171819202122232425# coding:utf-8from selenium import webdriverimport time # 要注意等待浏览器加载页面完成browser = webdriver.PhantomJS(executable_path=r"E:\C\python\py\pantomjs_selenuim\bin\phantomjs.exe")url = r'http://www.yrw.com/'a = browser.get(url)login_url = r'http://www.yrw.com/security/login/'browser.get(login_url)username= browser.find_elements_by_id('j-cpn2') # find_elements方法返回的列表， find_element返回找到的第一个passwd=browser.find_elements_by_name('password')submit = browser.find_elements_by_id('j-login-submit')username[0].send_keys('username') # 填用户名passwd[0].send_keys('password') # 填密码submit[0].click() # 点击登陆按钮time.sleep(5)page1 = browser.page_sourcebrowser.save_screenshot('login.jpg')browser.find_element_by_id("j-checkin-btn").click()time.sleep(5)page2 = browser.page_source # 打印网页源码browser.save_screenshot('sign.jpg')browser.quit() # 关闭phantom浏览器print 1 使用requests模拟签到 找到了js请求的url：https://www.yrw.com/member/check/?_=1482648421735 12345678910111213141516171819202122232425# 主要是想验证签到的url是否正确，登陆部分直接用的cookie，模拟登陆也很简单，post登陆表单即可，无验证码import requestssign_url = r'https://www.yrw.com/member/check/?_=1482648421735' # 数字含义应该是时间戳，不影响url访问cookie_string = r'JSESSIONID=6F98E86EBE571D3233EE14B986320E86; CNZZDATA1253600367=1638558785-1463053615-%7C1482717076'headers = &#123; 'Host': 'www.yrw.com', 'Connection': 'keep-alive', 'Accept': 'application/json, text/javascript, */*; q=0.01', 'X-Requested-With': 'XMLHttpRequest', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.36', 'Referer': 'https://www.yrw.com/member/home', 'Accept-Encoding': 'gzip, deflate, sdch, br', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Cookie': cookie_string&#125;s = requests.Session()s.headers = headersresponse = s.get(sign_url)q = response.json()if q['success']: print u'签到成功,获得积分%d点' % q['result']['gainPopularity']else: print u'签到失败']]></content>
      <categories>
        <category>技术</category>
        <category>python 爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Selenium</tag>
        <tag>Phantomjs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫笔记(基于python2)]]></title>
    <url>%2F2016%2F08%2F09%2F%E6%8A%80%E6%9C%AF%2Fpython2%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[基于python2.7 urllib2库的用法基础用法 urlopen方法打开网页 传入3个参数，data和timeout不是必须的；返回网页的源码可用read方法读出。 123response = urlopen(url,data,timeout)content = response.read() urlopen()本质上接收一个request对象，返回response对象。构建request对象：request = urllib2.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False) 12345import urllib2request = urllib2.Request(&quot;http://www.baidu.com&quot;)response = urllib2.urlopen(request)print response.read() post方式请求数据 post提交的数据不会在网址中显示 123456789import urllibimport urllib2values = &#123;&quot;username&quot;:&quot;xxxx@qq.com&quot;,&quot;password&quot;:&quot;XXXX&quot;&#125;data = urllib.urlencode(values) url = &quot;https://passport.csdn.net/account/login?from=http://my.csdn.net/my/mycsdn&quot;request = urllib2.Request(url,data)response = urllib2.urlopen(request)print response.read() get方式请求数据 get方式提高的数据直接包含在网址当中 12345678910111213import urllibimport urllib2values=&#123;&#125;values[&apos;username&apos;] = &quot;1016903103@qq.com&quot;values[&apos;password&apos;]=&quot;XXXX&quot;data = urllib.urlencode(values) url = &quot;http://passport.csdn.net/account/login&quot; ### 数据拼接到url中geturl = url + &quot;?&quot;+data request = urllib2.Request(geturl)response = urllib2.urlopen(request)print response.read() 高级用法 下面来说一说urllib2中的两个重要概念：Openers和Handlers。1.Openers：当你获取一个URL你使用一个opener(一个urllib2.OpenerDirector的实例)。正常情况下，我们使用默认opener：通过urlopen。但你能够创建个性的openers。2.Handles：Openers使用处理器handlers，所有的“繁重”工作由handlers处理。每个handlers知道如何通过特定协议打开URLs，或者如何处理URL打开时的各个方面。例如HTTP重定向或者HTTP cookies headers属性模拟浏览器身份 User-Agent : 通常会通过该值来判断是否是浏览器发出的请求Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用application/json ： 在 JSON RPC 调用时使用application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务referer：有些网站会检测该值是否为自身，防盗链。 在构建request对象时设置headers 12345headers = &#123; &apos;User-Agent&apos; : &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos; , &apos;Referer&apos;:&apos;http://www.zhihu.com/articles&apos; &#125;request = urllib2.Request(url, data, headers) response = urllib2.urlopen(request) page = response.read() 用自建的opener()中addheaders属性加入headers参数： 1234567891011121314user_agents = [ &apos;Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11&apos;, &apos;Opera/9.25 (Windows NT 5.1; U; en)&apos;, &apos;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)&apos;, &apos;Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)&apos;, &apos;Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12&apos;, &apos;Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9&apos;, &quot;Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7&quot;, &quot;Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0 &quot;, ] agent = random.choice(user_agents) opener.addheaders = [(&quot;User-agent&quot;,agent),(&quot;Accept&quot;,&quot;*/*&quot;),(&apos;Referer&apos;,&apos;http://www.google.com&apos;)] Proxy代理设置 在创建opner时传入Proxy handler ，urllib2.build_opener(proxy_handler)或在一个opener实例中调用opener.add_handler(proxy_handler)方法传入 创建proxy handler对象 1proxy_handler = urllib2.ProxyHandler(&#123;&quot;http&quot; : &apos;http://some-proxy.com:8080&apos;&#125;) 超时设置timeout 可以设置等待多少秒无响应即为超时，在urlopen(url,data,timeout=10)中设置 http的put和delete方法 http协议有6种请求数据的方法，除了最常用get和post，还有head，put，delete，options；PUT：这个方法比较少见。HTML表单也不支持这个。本质上来讲， PUT和POST极为相似，都是向服务器发送数据，但它们之间有一个重要区别，PUT通常指定了资源的存放位置，而POST则没有，POST的数据存放位置由服务器自己决定。DELETE：删除某一个资源。基本上这个也很少见，不过还是有一些地方比如amazon的S3云服务里面就用的这个方法来删除资源。 123# 在创建request对象时指定put或delete方法request = urllib2.Request(url,data,headers=&#123;&#125;)request.get_method = lambda: &apos;PUT&apos; # or &apos;DELETE&apos; 使用DebugLog 该功能可以将收发包的内容打印出来，不常用。 123httpHandler = urllib2.HTTPHandler(debuglevel=1)httpsHandler = urllib2.HTTPSHandler(debuglevel=1)opener = urllib2.build_opener(httpHandler, httpsHandler) cookie保持登陆在opener中绑定处理cookie对象的handler，即可捕获cookie并在后续的请求中使用。 cookielib模块 cookielib模块提供可存储的cookie对象，配合urllib2使用。cookielib提供的主要对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。 保存cookie到变量中，使用cookiejar()对象 12345678910import urllib2import cookielibcookie = cookielib.CookieJar()cookie_handler = urllib2.HTTPCookieProcessor(cookiejar=cookie)opener = urllib2.build_opener(cookie_handler)opener.open(url) # ...for item in cookie : print item.name + item.value 保存cookie到文件中， 使用FileCookieJar()对象及其子类 MozillaCookieJar和LWPCookiejar。 123456789import urllib2import cookielibcookie_file = &apos;cookie.txt&apos;cookie_jar=cookielib.MozillaCookieJar(filename=cookie_file)cookie_handler = urllib2.HTTPCookieProcessor(cookiejar=cookie_jar)opener = urllib2.build_opener(cookie_handler)opener.open(&apos;http:\\www.baidu.com&apos;)cookie_jar.save(ignore_discard=True, ignore_expires=True) # 即使将废弃的cookie也保存，覆盖cookie文件内容 从文件中加载cookie cookiejar对象的load方法 1234567891011import urllib2import cookielibcookie_file = &apos;cookie.txt&apos;cookie_jar=cookielib.MozillaCookieJar()### load()方法cookie_jar.load(filename=cookie_file, ignore_discard=True, ignore_expires=True)cookie_handler = urllib2.HTTPCookieProcessor(cookiejar=cookie_jar)opener = urllib2.build_opener(cookie_handler)resp=opener.open(&apos;http:\\www.baidu.com&apos;)print resp.read() 实例 登陆小说网站 166zw.com 12345678910111213141516171819202122232425262728293031323334353637383940import urllib2import urllibimport cookielib########################################################################class Browser(object): &quot;&quot;&quot; 创建一个有cookie和headers的opener对象,带有异常处理 &quot;&quot;&quot; #---------------------------------------------------------------------- def __init__(self): &quot;&quot;&quot;Constructor&quot;&quot;&quot; cookie_handler = urllib2.HTTPCookieProcessor(cookiejar=cookielib.CookieJar( )) self.opener = urllib2.build_opener(cookie_handler) self.opener.addheaders = [(&quot;User-agent&quot;, &apos;Opera/9.25 (Windows NT 5.1; U; en)&apos;,),(&quot;Accept&quot;,&quot;*/*&quot;),(&apos;Referer&apos;,&apos;http://www.google.com&apos;)] def openurl(self,url,data=None,timeout=10): try: response = self.opener.open(url,data,timeout) except urllib2.URLError, e: print e.code,&apos;\n&apos;,e.reason return response postData=urllib.urlencode(&#123;&apos;username&apos;:&apos;wocaonima&apos;,\ &apos;password&apos;:&apos;******&apos;,\ &apos;usecookie&apos;:&apos;1&apos;,\ &apos;submit.x&apos;:&apos;25&apos;,\ &apos;submit.y&apos;:&apos;5&apos;,\ &apos;action&apos;:&apos;login&apos;&#125;)loginUrl=r&apos;http://www.166zw.com/loginframe.php&apos;html=Browser().openurl(loginUrl,postData)print html.code,html.msg,html.infocontent= html.read()print content ### 打印的页面含有用户名信息，表明登陆成功 URLError异常处理 http协议状态码 服务器返回的响应请求，包含一个状态码。urllib2.HTTPError可以捕获 100：继续 客户端应当继续发送请求。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。101： 转换协议 在发送完这个响应最后的空行后，服务器将会切换到在Upgrade 消息头中定义的那些协议。只有在切换新的协议更有好处的时候才应该采取类似措施。102：继续处理 由WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。200：请求成功 处理方式：获得响应的内容，进行处理201：请求完成，结果是创建了新资源。新创建资源的URI可在响应的实体中得到 处理方式：爬虫中不会遇到202：请求被接受，但处理尚未完成 处理方式：阻塞等待204：服务器端已经实现了请求，但是没有返回新的信 息。如果客户是用户代理，则无须为此更新自身的文档视图。 处理方式：丢弃300：该状态码不被HTTP/1.0的应用程序直接使用， 只是作为3XX类型回应的默认解释。存在多个可用的被请求资源。 处理方式：若程序中能够处理，则进行进一步处理，如果程序中不能处理，则丢弃301：请求到的资源都会分配一个永久的URL，这样就可以在将来通过该URL来访问此资源 处理方式：重定向到分配的URL302：请求到的资源在一个不同的URL处临时保存 处理方式：重定向到临时的URL304：请求的资源未更新 处理方式：丢弃400：非法请求 处理方式：丢弃401：未授权 处理方式：丢弃403：禁止 处理方式：丢弃404：没有找到 处理方式：丢弃500：服务器内部错误 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器端的源代码出现错误时出现。501：服务器无法识别 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。502：错误网关 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。503：服务出错 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。 URLError HTTPError是URLError的子类,所以try… except…时应先捕获子类，子类捕获不到再捕获父类错误。 1234567891011import urllib2req = urllib2.Request(&apos;http://blog.csdn.net/cqcre&apos;)try: urllib2.urlopen(req)except urllib2.HTTPError, e: print e.codeexcept urllib2.URLError, e: print e.reasonelse: print &quot;OK&quot; 直接捕获一个URLError，如果含有code和reason属性，则说明是一个HTTPError。 123456789101112import urllib2req = urllib2.Request(&apos;http://blog.csdn.net/cqcre&apos;)try: urllib2.urlopen(req)except urllib2.URLError, e: if hasattr(e,&quot;code&quot;): print e.code,e.reason else: print eelse: print ok 正则表达式 和 bs4正则表达式 正则表达式用来匹配符合特定规则的字符串，类似于数学表达式是一种逻辑公式，实现对字符串的过滤匹配。 正则表达式的语法规则 贪婪模式和转义字符 python默认使用贪婪模式匹配查找字符串，即总是尝试匹配尽可能多的字符，非贪婪模式则相反。举例：模式ab 在abbbc字符串中将匹配到3个b，即abbb;非贪婪模式ab? 在abbbc字符串中将匹配到0个b，即a; 转义字符为\，很多编程语言也用\做转义字符，那么编程语言里的正则表达式想要匹配“\”就得使用4个‘\’,即“\\”。先在编程语言环境中转义得到“\”,然后提供给正则表达式。好在python有原生字符串的表示方法，即不转义任何’\’,而将其作为字符’\’,比如 print r’\s\%\‘ 执行后得到的结果就是 \s\%\ 。 re模块 re模块提供正则表达式引擎 pattern 是re 匹配模式的对象，由正则表达式字符串预编译得到。 12import repattern = re.compile(r&apos;\d&apos;) re主要的方法如下： 12345678910#返回pattern对象,该对象包含match，findall等方法p = re.compile(string[,flag]) #以下为匹配所用函数，也可传入pattern字符串，re会先执行compile编译正则表达式字符串生成pattern对象re.match(pattern, string[, flags])re.search(pattern, string[, flags])re.split(pattern, string[, maxsplit])re.findall(pattern, string[, flags])re.finditer(pattern, string[, flags])re.sub(pattern, repl, string[, count])re.subn(pattern, repl, string[, count]) flag参数可选值如下： 123456• re.I(全拼：IGNORECASE): 忽略大小写（括号内是完整写法，下同）• re.M(全拼：MULTILINE): 多行模式，改变&apos;^&apos;和&apos;$&apos;的行为（参见上图）• re.S(全拼：DOTALL): 点任意匹配模式，改变&apos;.&apos;的行为• re.L(全拼：LOCALE): 使预定字符类 \w \W \b \B \s \S 取决于当前区域设定• re.U(全拼：UNICODE): 使预定字符类 \w \W \b \B \s \S \d \D 取决于unicode定义的字符属性• re.X(全拼：VERBOSE): 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。 re方法 re.match(pattern,string,flag)match返回第一次匹配成功的结果（match对象）或None。注意match是从字符串开头的第一个字符开始匹配。hello模式match 字符串say hello就会失败返回None。 1234567891011121314151617 import re# 匹配如下内容：单词+空格+单词+任意字符m = re.match(r&apos;(\w+) (\w+)(?P&lt;sign&gt;.*)&apos;, &apos;hello world!&apos;)print &quot;m.string:&quot;, m.stringprint &quot;m.re:&quot;, m.reprint &quot;m.pos:&quot;, m.posprint &quot;m.endpos:&quot;, m.endposprint &quot;m.lastindex:&quot;, m.lastindexprint &quot;m.lastgroup:&quot;, m.lastgroupprint &quot;m.group():&quot;, m.group() # 默认返回group(0),即整个匹配结果，group(n)可以输出第n个元组匹配的结果。print &quot;m.group(1,2):&quot;, m.group(1, 2)print &quot;m.groups():&quot;, m.groups()print &quot;m.groupdict():&quot;, m.groupdict()print &quot;m.start(2):&quot;, m.start(2)print &quot;m.end(2):&quot;, m.end(2)print &quot;m.span(2):&quot;, m.span(2)print r&quot;m.expand(r&apos;\g \g\g&apos;):&quot;, m.expand(r&apos;\2 \1\3&apos;) re.search(pattern, string[, flags])search()同match()方法相似，区别是match()仅从字符串的开头匹配，如果0位置失败，则匹配以失败结束。search同match有相同的属性和方法。 123456789101112 #导入re模块 import re# 将正则表达式编译成Pattern对象pattern = re.compile(r&apos;world&apos;)# 使用search()查找匹配的子串，不存在能匹配的子串时将返回None# 这个例子中使用match()无法成功匹配res = re.search(pattern,&apos;hello world!&apos;)if res: # 使用Match获得分组信息 print res.group()### 输出 #### world re.split(pattern, string[, maxsplit]) 按照能够匹配的子串将string分割后返回列表。maxsplit用于指定最大分割次数，不指定将全部分割。 123456 import re pattern = re.compile(r&apos;\d+&apos;) print re.split(pattern,&apos;one1two2three3four4&apos;) ### 输出 #### [&apos;one&apos;, &apos;two&apos;, &apos;three&apos;, &apos;four&apos;, &apos;&apos;] re.findall(pattern, string[, flags]) 搜索string，以列表形式返回全部能匹配的子串 re.finditer(pattern, string[, flags]) 搜索string，返回一个顺序访问每一个匹配结果（Match对象）的迭代器.迭代器使用for进行遍历。 re.sub(pattern, repl, string[, count]) 使用repl替换string中每一个匹配的子串后返回替换后的字符串。当repl是一个字符串时，可以使用\id或\g、\g引用分组，但不能使用编号0。当repl是一个方法时，这个方法应当只接受一个参数（Match对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。count用于指定最多替换次数，不指定时全部替换。 123456789101112131415import repattern = re.compile(r&apos;(\w+) (\w+)&apos;)s = &apos;i say, hello world!&apos;print re.sub(pattern,r&apos;\2 \1&apos;, s)def func(m): return m.group(1).title() + &apos; &apos; + m.group(2).title()print re.sub(pattern,func, s)### output #### say i, world hello!# I Say, Hello World! re.subn(pattern, repl, string[, count]) 返回 (sub(repl, string[, count]), 替换次数)。 使用pattern对象调用上述方法则不必传入pattern对象 常用匹配 ip 文件后缀 r&#39;http://img\.tupianzj\.com/uploads/allimg/\d+/.+\.(?:gif|jpg|png|bmp)&#39; BeautifulSoup BeautifulSoup是python的一个第三方库，用于解析网页并从中提取数据。 安装 pip install beautifulsoup4,或去官网下载安装包。 BeautifulSoup使用Python标准库中默认的HTML解析器，想要使用其他可选的第三方html解析器需要提前安装好。 lxml或html5lib。lxml解析器更强大速度更快。html5lib是纯python实现的，解析方式与浏览器相同。 pip install lxml 和 pip install html5lib windows下直接pip安装lxml失败，可以先pip install wheel ,然后去Python Extension Packages for Windows - Christoph Gohlke上下载对应系统的安装包，然后pip install c:\lxml-3.6.4-cp27-cp27m-win_amd64.whl其他pip无法直接安装的模块也可用这种方法。 官方文档 Beautiful Soup 4.4.0 文档 — beautifulsoup 4.4.0 文档 使用 1234567from bs4 import BeautifulSoupimport urllib2URL = r&quot;http://www.baidu.com&quot;content = urllib2.urlopen(URL).read()bsobj = BeautifulSoup(content) #传入html格式的字符串print bsobj.prettify() #打印出html文档树，格式化输出 bs将html文档转换为树形结构，每个节点都是一个对象，对象分为4种：Tag，NavigableString，BeautifulSoup，Comment。 Tag 即HTML中的标签，由&lt;&gt; &lt;/&gt;闭合。比如 News Today 获取标签方法如下，找不到返回None，这种方法只能查找符合条件的第一个标签。tag常用的属性有name，attrs，string。 12345678910111213print bsobj.title# &lt;title&gt;百度一下，你就知道&lt;/title&gt;print bsobj.title.string# 百度一下，你就知道print bsobj.div.attrs# &#123;&apos;id&apos;: &apos;wrapper&apos;&#125; 输出的是标签的属性print bsobj.div.name# div 标签的名字为对象本身的名字，bsobj对象的名字为`[document]`#获取某个属性的值print bsobj.div[&apos;id&apos;]wrapperprint bsobj.div.get(&apos;id&apos;)wrapper NavgableString 可以遍历的字符串，即标签闭合的内容：print bsobj.title.string #输出 百度一下，你就知道 BeautifulSoup BeautifulSoup对象表示一个文档的全部内容，也可以当做tag对象，只不过是包含很多子tag的特殊的tag Comment Comment 对象是一个特殊类型的 NavigableString 对象，但是使用ob.string输出的内容仍然不包括注释符号。 有必要时可进行类型判断 12 if type(soup.a.string)==bs4.element.Comment:print soup.a.string 遍历文档树 tag.contens 属性 bsobj.tag.contents 可以将tag的子节点以列表的形式输出。 tag.children 属性 bsobj.tag.children 是一个子节点的list生成器，可以用for循环遍历 tag.descendants 属性 bsobj.tag.descendants 是tag的子孙节点的list生成器，用for遍历。而contents和children只包含孩子节点（一级子节点），不包含孩子的孩子及后续孙子节点。 tag.string 属性，节点内容 如果tag只含有唯一的一个子tag，那么tag.string输出的是子tag的内容，不含子tag，则直接输出tag的内容；如果有多个子tag，则输出none。 tag.strings 和 tag.stripped_strings 获取多个内容 都需要用for循环遍历，.stripped_strings去除了多余空白内容。 tag.parent 属性 父节点 tag.parents 属性 全部父辈节点 通过递归得到tag全部的父辈节点，用for循环遍历 tag.next_sibling 和 tag.previous_sibling 兄弟节点 获取与tag同一级的节点，.next_sibling 属性获取了该节点的下一个兄弟节点，.previous_sibling 则与之相反，如果节点不存在，则返回 None注意：实际文档中的tag的 .next_sibling 和 .previous_sibling 属性通常是字符串或空白，因为空白或者换行也可以被视作一个节点，所以得到的结果可能是空白或者换行 .next_siblings .previous_siblings 属性 全部兄弟节点 .next_element .previous_element 属性 tag节点的前后节点，与兄弟节点不同，前后节点没有层级关系，可以是父子也可以是兄弟节点。 .next_elements .previous_elements 属性 搜索文档树 bsobj.find_all(name=None, attrs={}, recursive=True, text=None, limit=None)搜索当前tag的所有子tag，返回符合条件的结果 name 参数 查找所有名字满足name条件的tag。 1&gt;如果传入字符串，则搜索完全匹配该字符串的tag : 标签; 2&gt; 传正则表达式，搜索名字匹配正则的tag标签 3&gt; 传入name列表，返回所有与列表元素匹配的tag 4&gt; 传True，返回全部的任意tag，除字符串节点。 5&gt; 传方法，该方法只接收一个参数，根据判断条件返回True 或False。find_all()返回的是满足True的全部节点 attrs 和 keyword 参数 attrs={‘id’:’123’,’color’:’red’},attrs接收一个字典，用来搜索含有指定属性的tag。也可以指定关键字参数，如 id=’today’,class_=’nostyle’,注意class是python保留字，所以bs使用class来做区别。注意 html5中的类似 data-*的参数不能用作关键字搜索，会报错。 text参数 可以匹配文档中的字符串内容，与name参数的可选值一样，接收 字符串，正则表达式，列表，True limit参数 limit = n ，限制返回的数量为n，即找到n个节点就停止。 recursive参数 递归参数默认为true，即搜索子孙节点。如果只搜索子节点怎改为false。 find( name , attrs , recursive , text , **kwargs )它与 find_all() 方法唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果 find_parents() 和 find_parent()find_all() 和 find() 只搜索当前节点的所有子节点,孙子节点等. find_parents() 和 find_parent() 用来搜索当前节点的父辈节点,搜索方法与普通tag的搜索方法相同,搜索文档搜索文档包含的内容 find_next_siblings() find_next_sibling()迭代对象为tag.next_siblings ，对tag 的所有后面解析的兄弟 tag 节点进行迭代, find_next_siblings() 方法返回所有符合条件的后面的兄弟节点,find_next_sibling() 只返回符合条件的后面的第一个tag节点 find_previous_siblings() 和 find_previous_sibling()迭代对象为.previous_siblings ，对当前 tag 的前面解析的兄弟 tag 节点进行迭代, find_previous_siblings() 方法返回所有符合条件的前面的兄弟节点, find_previous_sibling() 方法返回第一个符合条件的前面的兄弟节点 find_all_next() find_next()迭代对象为tag.next_elements，find_all_next() 方法返回所有符合条件的节点, find_next() 方法返回第一个符合条件的节点 find_all_previous() 和 find_previous()迭代对象为.previous_elements,find_all_previous() 方法返回所有符合条件的节点, find_previous()方法返回第一个符合条件的节点 Requests库的用法 Requests是第三方库，比urllib库更高级、抽象。也就是说使用更方便。 文档 Requests: HTTP for Humans — Requests 2.11.1 documentation 安装 pip install requests 初次相见html= requests.get(url),返回的是对象，具有以下属性或方法：’apparent_encoding’, ‘close’, ‘connection’, ‘content’, ‘cookies’, ‘elapsed’, ‘encoding’, ‘headers’, ‘history’, ‘is_permanent_redirect’, ‘is_redirect’, ‘iter_content’, ‘iter_lines’,‘json()’：解析json格式内容,‘links’, ‘ok’, ‘raise_for_status’,‘raw’：获得原始套接字,要在初始的请求中设置stream=True‘reason’, ‘request’, ‘status_code’ ：http响应状态码,‘text’:直接输出内容,‘url’ 基本http请求 requests实现了http的6种请求，get、post、delete、put、options、head。 GET请求方式requests.get(url, params=None),参数传入字典，比如{‘k1’:’v1’ , ‘k2’:’v2’}，则请求url自动编码为url?k1=v1&amp;k2=v2在请求中添加headers， 123456import requestspayload = &#123;&apos;key1&apos;: &apos;value1&apos;, &apos;key2&apos;: &apos;value2&apos;&#125;headers = &#123;&apos;content-type&apos;: &apos;application/json&apos;&#125;r = requests.get(&quot;http://httpbin.org/get&quot;, params=payload, headers=headers)print r.url post请求 12345678910import jsonpost_data =&#123;'key1': 'value1', 'key2': 'value2'&#125; res = requests.post(url, data=post_data, json=None)# 如果提交的信息不是表单形式，而是json格式的数据，可用json.dumps()把表单数据序列化,或使用json参数res = requests.post(url, data=json.dumps(post_data), json=None)res = requests.post(url, data=None, json=post_data)# 以上2者等效# 支持流式上传数据，只需传入一个file-like对象,不用file.read()加载内容至内存with open('test.file','r') as f : res = requests.post(url, data=f) Cookies 如果服务器返回的响应包含cookie，则我们得到的response对象就会保存该cookie，可以利用.cookies属性查看。使用cookie发送请求时只需在get/post方法中指定cookies参数要想使用cookie保持登陆，则需要一个session会话对象 123456789# 查看cookiesimport requestsurl1 = 'http://xxx.com'r = requests.get(url1)print r.cookies# 构造cookies并用来发送请求url2 = 'http://ooo.com'cookies = &#123;'id':'1234'&#125;r = requests.get(url2，cookies=cookies) 超时设置 timeout在get/post方法中传入timeout参数，仅对连接建立有效，与返回response全部数据的时间无关。 会话对象 session 每次直接调用requests.get/post方法都相当与建立了一个新的请求会话，相当于用不同的浏览器发起请求。因而无法使用cookie保持登陆状态。 123456s = requests.session()s.headers.update(&#123;(&quot;User-agent&quot;,&quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11&quot;),(&quot;Accept&quot;,&quot;*/*&quot;),(&apos;Referer&apos;,&apos;http://www.google.com &apos;)&#125;) # 此处设置的headers是全局变量，post/get方法传入的headers参数的同名变量会覆盖此处headers的变量，不使用某个参数可将其值设为None。# 不同名的参数会一块加入到请求的headers中生效.res = s.get(url)# 通过会话发起请求 SSL证书验证Requests可以为HTTPS请求验证SSL证书，就像web浏览器一样。要想检查某个主机的SSL证书，你可以在get/post方法中使用 verify 参数，默认为True，是检查的。如直接requests.get(‘https://www.12306.cn&#39;),就会出现证书无效的错误：requests.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)。使用requests.get(url=&apos;https://www.12306.cn&apos;,verify=False)取消证书检查。 代理在post/get方法中传入参数proxies： 1234proxies = &#123; &quot;https&quot;: &quot;http://41.118.132.69:4433&quot;&#125;r = requests.post(&quot;http://httpbin.org/post&quot;, proxies=proxies) 也可以设置环境变量HTTP_PROXY和HTTPS_PROXY来配置代理。 12export HTTP_PROXY=&quot;http://10.10.1.10:3128&quot;export HTTPS_PROXY=&quot;http://10.10.1.10:1080&quot; 参考：官方API文档 xpath语法与lxml库 lxml解析库使用xpath语法，beautifulSoup的内部实现是RExpath语法参考lxml官方文档 - Processing XML and HTML with Python xpath语法 XPath 是一门在 XML 文档中查找信息的语言。XPath 可用来在 XML 文档中对元素和属性进行遍历。XPath 是 W3C XSLT 标准的主要元素，并且 XQuery 和 XPointer 都构建于 XPath 表达之上。 xpath使用路径表达在xml中选取节点。节点的逻辑关系有：父节点Parent、先辈节点Ancestor、子节点Children、后代节点Descendant、兄弟/同胞节点sibling。 路径表达式：nodename: 选择nodename节点的所有子节点,/:从根路径选取,绝对路径，//:从当前路径下选取全部,不考虑位置，相对路径，.:选取当前节点,..:选取当前节点的父节点,@:选取属性. 谓语：用来查找某个特定的节点或者包含某个特定的值的节点。谓语嵌在方括号[]中。 | 路径表达式 | 结果 || /bookstore/book[1] | 选取属于 bookstore 子元素的第一个 book 元素。 || /bookstore/book[last()] | 选取属于 bookstore 子元素的最后一个 book 元素。 || /bookstore/book[last()-1] | 选取属于 bookstore 子元素的倒数第二个 book 元素。 || /bookstore/book[position()35.00] | 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。 || /bookstore/book[price&gt;35.00]/title | 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 | 选取未知节点： *:匹配任何节点; @*:匹配任何属性的节点; node():匹配任何类型的节点; 选取多个路径：|运算符，//book/title | //book/price xpath表达式运算符|:找到满足2个路径表达式之一的节点集合；+:加法-:减法*:乘法div:除法，取整 ; mod:取余数=:等于，比较，相等返回true，否则返回false！=:不等于，比较&lt;:小于 ； &lt;=:小于或等于&gt;:大于 ； &gt;=:大于或等于or: 或，连接2个布尔表达式，有一个为真返回trueand: 与，都为真才返回true lxml用法PhantomJS的用法Selenium的用法PyQuery的用法 PySpider框架]]></content>
      <categories>
        <category>技术</category>
        <category>python 爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python编程技巧]]></title>
    <url>%2F2016%2F08%2F07%2F%E6%8A%80%E6%9C%AF%2Fpython%E7%BC%96%E7%A8%8B%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[python编程技巧 x的n次方求解 ： x ** n 22]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python字典键值转置]]></title>
    <url>%2F2016%2F07%2F29%2F%E6%8A%80%E6%9C%AF%2Fpython-dict-reverse%2F</url>
    <content type="text"><![CDATA[dict的key与value转置 如果value1值唯一，则直接交换key1与value1的位置 1234from collections import OrderedDict as od_dict #生成有序的dictd= od_dict((v,k) for k,v in OID_FS_lst.items()) print d 如果value1不唯一，直接交换，则作为key2插入的value1值，会被相同的key2覆盖。也就说会丢失部分值。]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 字符串技巧]]></title>
    <url>%2F2016%2F07%2F29%2F%E6%8A%80%E6%9C%AF%2Fpython-str%2F</url>
    <content type="text"><![CDATA[将 \u3232\u6674转换成unicode字符 运用str对象的decode方法 1234567#任何字符，都可以在：#http://unicodelookup.com/#中，查找到对应的unicode的值s=&apos;\u6210\u529f&apos;print s.decode(&apos;unicode-escape&apos;)...成功 写入unicode报错‘ascii’ codec can’t encode character in position 0-3。py2如果开头指定了utf-8，Python的解释器根据 “#encoding:utf-8” 来按utf-8的编码规则来理解这些字符码•当你把字符串写到文件中去时，因为你的字符串是 unicode，所以Python会（自动）先调用encode方法来编码unicode字符串，然后再写入文件•当调用encode方法时，因为没有指定编码格式，所以采用默认值 ascii，ascii并不能解释定义的utf-8中翻译的字符码，所以报错 今天遇到一个错误： UnicodeEncodeError: ‘ascii’ codec can’t encode characters in position 0-3 搜索网上找到一个解决办法（转载自 http://blog.sina.com.cn/s/blog_727b603701019pyl.html） 异常: ‘ascii’ codec can’t encode characters 字符集的问题，在文件前加两句话： reload(sys) sys.setdefaultencoding( “utf-8” ) 完美解决，]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python创建项目]]></title>
    <url>%2F2016%2F07%2F28%2F%E6%8A%80%E6%9C%AF%2Fsomething-with-python-project%2F</url>
    <content type="text"><![CDATA[使用Cookiecutter模板创建Python项目样式 首先安装cookiecutter pip install -U cookiecutter 在github上找一个模板 在某个目录下执行 cookiecutter https://github.com/audreyr/cookiecutter-pypackage.git，使用找到的模板创建项目，输入相关参数 是否开源发布看个人 import自己的模块 首先.py文件和主文件在一个目录下 import name ,不要加.py,否则报错 Fatal error in launcher: Unable to create process using ‘“‘Windows新装的python3自带了easy_install和pip，但执行时会出错Fatal error in launcher: Unable to create process using &#39;&quot;&#39; ,1、确保环境变量中没空格。2、如果不是环境变量问题，则重装\升级easy_install和pip，python3 -m pip install -U pip,python3 -m pip install -U setuptools。问题解决。 pip安装模块失败的解决办法windows下直接pip安装lxml失败，可以先pip install wheel ,然后去Python Extension Packages for Windows - Christoph Gohlke上下载对应系统的安装包，然后pip install c:\lxml-3.6.4-cp27-cp27m-win_amd64.whl其他pip无法直接安装的模块也可用这种方法。 python2 Unicode和普通字符串之间转换问题描述：You need to deal with data that doesn’t fit inthe ASCII character set.解决： 将Unicode转换成普通的Python字符串:”编码(encode)” 12u_string = u&quot;你好&quot;utf8_s = u_string.encode(&apos;utf-8&apos;) ascii_s ,iso_s,utf16_s 对应的编码集分别为”ascii” “ISO-8859-1” “utf-16”将普通的Python字符串转换成Unicode: “解码(decode)” 1234plainstring1 = unicode(utf8string, &quot;utf-8&quot;) plainstring2 = unicode(asciistring, &quot;ascii&quot;) plainstring3 = unicode(isostring, &quot;ISO-8859-1&quot;)plainstring4 = unicode(utf16string, &quot;utf-16&quot;)]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统服务配置]]></title>
    <url>%2F2016%2F07%2F26%2F%E6%8A%80%E6%9C%AF%2Fconfig-of-Linux%2F</url>
    <content type="text"><![CDATA[查看内核版本1uname -r 稳定版本的偶数版，如2.6.x，适合于商业与家用环境使用；开发中版本，如2.5.x，适合开发特殊功能的环境。 安装内核文件：apt-get install linux-headers-$(uname -r) 修改源vim /etc/apt/sources.list deb http://http.kali.org/kali kali-rolling main contrib non-free ssh远程登陆设置允许root登陆ssh新装的系统可能需要要修改root密码gedit /etc/ssh/sshd_config修改 PermitRootLogin yes PermitRootLogin yesPermitEmptyPasswords no注意重启ssh service 设置系统开机启动sshd服务chkconfig sshd on 即可but debian8不行， 如果是debian 8，直接使用systemd，比如：sudo systemctl enable xxx.servicesudo systemctl disable xxx.service Mac：（若以root身份登陆，将username改为root） 登陆 1ssh root@IPaddress Linux：(若以root身份登陆，将username删掉）Linux默认root的提示符为#,而一般身份用户的提示符为$。 1su - username 使用ssh上传下载文件 很多系统初始不带这工具，但感觉比起ftp，sftp传输文件，还是方便很多的。 首先安装sz，rz命令行工具，apt-get install lrzsz。然后使用xshell或secureRT登陆服务器，putty好像不行。 上传 rz -E输入rz -E会自动弹出打开文件窗口，选择要上传的文件。 下载 sz file-path输入sz file，会弹出窗口选择file文件下载存放的位置。 FTP 服务配置参考链接:proftp 安装配置FTP服务器 切换root 安装proftpd，在确认安装中选Y，并选择《Standalone》安装apt-get install proftpd 安装完以后将实现先停掉，以方便改配置/etc/init.d/proftpd stop 或service proftpd stop 编辑proftpd的配置文件nano /etc/proftpd/proftpd.conf更改FTP根目录, 默认为: DefaultRoot ~，比如说改为:DefaultRoot /var/www允许匿名用户访问,找到配置文件中的 ““ 和 ““ 之间的部分，将注释移除即可。 保存配置文件 “/etc/proftpd.conf”,重启proftpd服务/etc/init.d/proftpd restart 为FTP服务器添加用户名、密码和读写权限 FTP服务的用户名密码其实就是拥有特定目录权限的linux用户及其密码，所以添加一个FTP用户并设置密码，用户信息即可adduser tester -home /var/www 还要为此用户添加FTP共享目录的读写权限，[直接改变所有权]chown tester /var/www [可选] 当用户非常多时，可以添加一个用户组，统一配置权限addgroup ftpuser并将tester添加进ftpuser组adduser tester ftpuser注* 删除用户和用户组deluser testerdelgroup tester安装VMware tools 修改防火墙端口 开启防火墙 UDP161端口 for snmp 查看防火墙配置：iptables –L –n 添加一条规则记录：iptables -I INPUT -p udp --dport 161 -j ACCEPT 保存规则使防火墙生效iptables-save(debian linux),ubuntu下的保存iptables命令iptables -save。 常用端口 21是指FTP默认端口、80是指web服务器端口、3306是指MySQL数据库链接端口，22是指SSH远程管理端口，9000到9045是FTP被动模式端口范围 修改DNS永久修改网卡DNSsudo –icd /etc/resolvconf/resolv.conf.dvim base添加如下内容nameserver 8.8.8.8nameserver 8.8.4.4 Linux下python2 与3共存 安装Python3后，建立ln，使用Python（Python2），Python3 来区分两个版本，使用sudo apt-get install python3-setuptools 安装Easy_install，再使用sudo easy_install3 pip 安装Pip，Pip 对应Python2，Pip3 对应Python3。Easy_Install 对应Python2，Easy_Install3 对应Python3. python使用virtualenv创建隔离环境 注销Linux注销Linux并不意味着关机，只是用户离开系统。 1exit 基础命令的操作1command [-options] parameter1 parameter2 ··· 123456echo $LANG #显示目前支持的语言LANG=en_US #将语言改为英文系date #显示日期与实践cal 10 2014 #显示日历bc #计算器quit #退出 重要的热键[tab]：连按两次，具有“命令补全”和“文件补齐”的作用。[control]+c：中断目前程序。[control]+d：键盘输入结束；直接离开文字界面（相当于`exit）。 在线求助 man page1man command #command是要查询的命令名称 进入man命令后，可按空格往下翻页，按q键离开。在man page中，可以在任何时候输入/keyword来查询关键字，比如/date. 正确的关机方法123who #查看目前有谁在线netstat -a #查看网络的联机状态ps -aux #查看后台执行的程序 数据同步写入磁盘：为了防止不正常关机导致的内存数据没有来得及写入磁盘，在文字界面输入 1sync 惯用的关机命令： 123456shutdown -h now #立刻关机shutdown -h 20:25 #晚上8点25分关机shutdown -h +10 #过十分钟后关机shutdown -r now #立刻重启shutdown -r +30 ‘The system will be reboot’ #再过30分钟关机，并显示后面的消息给所有在线用户shutdown -k now ‘The system will be reboot’ #仅发出警告，系统并不会真正关机 重启、关机：reboot，halt，poweroff。务必用man去查询一下。 压缩解压unrar x a.rar ./a/ 使用screen12345678910111213141516171819screen -S myjobSessionscreen -r sessionname/ID 恢复sessionscreen -lsexit 退出screen session，终止任务，ctrl+a+d 暂时离开/后台运行session，先按住ctrl，然后按一下a，按一下d远程演示首先演示者先在服务器上执行 screen -S test 创建一个screen会话，观众可以链接到远程服务器上执行screen -x test 观众屏幕上就会出现和演示者同步其它命令Ctrl + a，d #暂离当前会话Ctrl + a，c #在当前screen会话中创建一个子会话 Ctrl + a，w #子会话列表 Ctrl + a，p #上一个子会话 Ctrl + a，n #下一个子会话 Ctrl + a，0-9 #在第0窗口至第9子会话间切换有时在恢复screen时会出现There is no screen to be resumed matching ****，遇到这种情况咋办呢？输入命令screen -d ****]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 目录 文件 权限]]></title>
    <url>%2F2016%2F07%2F25%2F%E6%8A%80%E6%9C%AF%2Ffile-and-dir-of-Linux%2F</url>
    <content type="text"><![CDATA[Linux目录操作12345. # 代表此层目录.. # 代表上一层目录- # 代表前一个工作目录~ # 代表『目前使用者身份』所在的家目录~account # 代表 account 这个使用者的家目录(account是个帐号名称) 在所有目录底下都会存在的两个目录，分别是.与.. 根目录的上一层(..)与根目录自己(.)是同一个目录 几个常见的处理目录的命令 cd：变换目录，cd是Change Directory的缩写 pwd：显示目前的目录，pwd是Print Working Directory的缩写 mkdir：创建一个新的目录 rmdir：删除一个空的目录 mv：移动文件 1234pwd -P # -P：代表显示正确的完整路径，而不是连接路径mkdir -m xxx # -m：直接配置文件的权限mkdir -p test1/test2 # -p：直接将所需要的目录(包含上一级目录)递回创建起来！PATH=&quot;$PATH&quot;:/root # 将/root路径加入PATH环境变量中 文件与目录管理 文件与目录的检视： ls 复制、删除与移动： cp, rm, mv 1234567cp -a # 将文件的所有特性都一起复制过来cp -p # 连同文件的属性一起复制过去，而非使用默认属性(备份常用)cp -r # 可以复制目录，但是，文件与目录的权限可能会被改变rm -i # 互动模式，在删除前会询问使用者是否动作rm -r # 连目录下的东西一起删掉，并且不会询问，慎用mv -f # force强制移动，如果目标文件已经存在，不会询问而直接覆盖mv -i # 若目标文件 (destination) 已经存在时，就会询问是否覆盖 文件内容查询 直接检视文件内容： cat, tac, nl （常用） 可翻页检视： more, less （常用） 数据撷取： head, tail 非纯文字档： od 修改文件时间与建置新档： touch 12345678cat [-AbEnTv] filename # 由第一行开始显示文件内容。-b列出非空白行行号；-n列出所有行号。tac # 从最后一行开始显示文件内容，tac就是cat倒着写！nl # 显示文件内容，顺便输出行号more # 一页一页地显示文件内容less # 与more类似，但可以往前翻页head [-n number] # 只看文件头几行，默认是10行，number是自定义行数tail # 只看文件尾几行，文件很大的时候常用od # 以二进制方式读取文件内容 文件与目录的默认权限与隐藏权限 文件默认权限：umask 文件隐藏属性： chattr, lsattr 文件特殊权限：SUID, SGID, SBIT, 权限配置 观察文件类型：file 123umask # 后三位数是被拿走的权限分数，比如0022，u没有被拿走权限，g和o被拿走了w权限umask -S # 以符号类型来显示权限umask number # 配置自己需要的权限 在默认的情况中，root的umask会拿掉比较多的属性，root的umask默认是022， 这是基於安全的考量啦～至於一般身份使用者，通常他们的 umask 为002 ，亦即保留同群组的写入权力。 特殊权限s和t Set UID，简称SUID，当s标志在文件拥有者的x项目为SUID，对目录无效 Set GID，简称SGID，当s标志在群组的x项目为SGID，对目录有效 Sticky Bit, 简称SBIT，目前只针对目录有效，对於文件已经没有效果了 配置SUID,SGID,SBIT权限在原有的权限数字前面加上需要配置的权限数字。比如755-&gt;4755 ，就意味着-rwxr-xr-x变为了-rwsr-xr-x。 4 为 SUID 2 为 SGID 1 为 SBIT 123chmod 4755 filenamechmod u=rwxs,go=x test; ls -l test # 配置权限为-rws--x--x的模样chmod g+s,o+t test; ls -l test # 配置权限为-rws--s--t，即加入SGID,SBIT权限 ###命令与文件的搜寻 命令档名的搜寻：which 文件档名的搜寻：whereis, locate, find 权限与命令的关系一、让使用者能进入某目录成为『可工作目录』的基本权限为何： 可使用的命令：例如 cd 等变换工作目录的命令； 目录所需权限：使用者对这个目录至少需要具有 x 的权限 额外需求：如果使用者想要在这个目录内利用 ls 查阅档名，则使用者对此目录还需要 r 的权限。 二、使用者在某个目录内读取一个文件的基本权限为何？ 可使用的命令：例如本章谈到的 cat, more, less等等 目录所需权限：使用者对这个目录至少需要具有 x 权限； 文件所需权限：使用者对文件至少需要具有 r 的权限才行！ 三、让使用者可以修改一个文件的基本权限为何？ 可使用的命令：例如 nano 或未来要介绍的 vi 编辑器等； 目录所需权限：使用者在该文件所在的目录至少要有 x 权限； 文件所需权限：使用者对该文件至少要有 r, w 权限 四、让一个使用者可以创建一个文件的基本权限为何？ 目录所需权限：使用者在该目录要具有 w,x 的权限，重点在 w 啦！ 五、让使用者进入某目录并运行该目录下的某个命令之基本权限为何？ 目录所需权限：使用者在该目录至少要有 x 的权限； 文件所需权限：使用者在该文件至少需要有 x 的权限]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-磁盘与文件系统管理]]></title>
    <url>%2F2016%2F07%2F24%2F%E6%8A%80%E6%9C%AF%2Fdisk-and-fileSystem-management-of-Linux%2F</url>
    <content type="text"><![CDATA[认识EXT2文件系统每种操作系统能够使用的文件系统并不相同。 举例来说，windows 98以前的微软操作系统主要利用的文件系统是FAT(或FAT16)，windows 2000以后的版本有所谓的NTFS文件系统，至于Linux的正统文件系统则为Ext2(Linux second extended file system, ext2fs)这一个。此外，在默认的情况下，windows操作系统是不会认识Linux的Ext2的。 那么文件系统是如何运行的呢？这与操作系统的文件数据有关。较新的操作系统的文件数据除了文件实际内容外，通常含有非常多的属性，例如Linux操作系统的文件权限(rwx)与文件属性(拥有者、群组、时间参数等)。 文件系统通常会将这两部份的数据分别存放在不同的区块，权限与属性放置到inode中，至于实际数据则放置到data block区块中。 另外，还有一个超级区块(superblock)会记录整个文件系统的整体信息，包括inode与block的总量、使用量、剩余量等。 文件系统的简单操作磁盘的分割、格式化、检验与挂载 磁盘分区： fdisk, partprobe 磁盘格式化： mkfs, mke2fs 磁盘检验： fsck, badblocks 磁盘挂载与卸除： mount, umount 磁盘参数修订： mknod, e2label, tune2fs, hdparm 配置启动挂载 启动挂载 /etc/fstab 及 /etc/mtab 特殊装置 loop 挂载(映象档不刻录就挂载使用) 内存置换空间(swap)之建置 使用实体分割槽建置swap 使用文件建置swap swap使用上的限制 文件系统的特殊观察与操作 boot sector 与 superblock 的关系 磁盘空间之浪费问题 利用 GNU 的 parted 进行分割行为 重点回顾 基本上 Linux 的正统文件系统为 Ext2 ，该文件系统内的信息主要有： superblock：记录此 filesystem 的整体信息，包括inode/block的总量、使用量、剩余量， 以及文件系统的格式与相关信息等； inode：记录文件的属性，一个文件占用一个inode，同时记录此文件的数据所在的 block 号码； block：实际记录文件的内容，若文件太大时，会占用多个 block 。 Ext2 文件系统的数据存取为索引式文件系统(indexed allocation) 需要碎片整理的原因就是文件写入的 block 太过于离散了，此时文件读取的效能将会变的很差所致。 这个时候可以透过碎片整理将同一个文件所属的 blocks 汇整在一起。 Ext2文件系统主要有：boot sector, superblock, inode bitmap, block bitmap, inode table, data block 等六大部分。 data block 是用来放置文件内容数据地方，在 Ext2 文件系统中所支持的 block 大小有 1K, 2K 及 4K 三种而已 inode 记录文件的属性/权限等数据，其他重要项目为： 每个 inode 大小均固定为 128 bytes； 每个文件都仅会占用一个 inode 而已； 因此文件系统能够创建的文件数量与 inode 的数量有关； 文件的 block 在记录文件的实际数据，目录的 block 则在记录该目录底下文件名与其 inode 号码的对照表； 日志式文件系统 (journal) 会多出一块记录区，随时记载文件系统的主要活动，可加快系统复原时间； Linux 文件系统为添加效能，会让主存储器作为大量的磁盘高速缓存； 实体链接只是多了一个文件名对该 inode 号码的链接而已； 符号链接就类似Windows的快捷方式功能。 磁盘的使用必需要经过：分割、格式化与挂载，分别惯用的命令为：fdisk, mkfs, mount三个命令 启动自动挂载可参考/etc/fstab之配置，配置完毕务必使用 mount -a 测试语法正确否； 参考资料 鸟哥的Linux私房菜 第八章]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的装机必备软件]]></title>
    <url>%2F2016%2F07%2F20%2F%E6%8A%98%E8%85%BE%2F2016-07-20-software-tools-on-mylaptop%2F</url>
    <content type="text"><![CDATA[windows美化 RocketDock 实现mac上的Dock栏 TrueLancherBar 目录式的任务栏快捷操作入口 Q-Dir 1分4的资源管理器 Fliqlo 极客感十足的时钟屏保 fences2.01 桌面整理，提高效率 win10安装破解后，如出现桌面图标无法移动，需要安装破解补丁 破解补丁 实用工具 QQ Internet 国际版QQ，功能少，无广告，消息多时不卡顿 魔影工厂 音视频格式转换工具 镜像工具 DAEMON 虚拟光驱 V-Disk 虚拟光驱 UltraIso 光驱加载刻录 win32diskImager 镜像刻录 编辑器 MarkdownPad2 notepad++ 替换系统的记事本，打开大文件不卡死，语法高亮，可辅助编程 sublime text 3 同上，作为Python编辑器很方便 ultra edit 对二进制支持友好 虚拟机与镜像 VMware VirtualBox 客户机系统镜像下载，解压直接使用 360隔离沙箱 ：用来运行从网下二进制小程序， docker 翻墙工具 lantern ：自己编译生成的exe是免安装，无流量限制的 xx-net ：基于GoAgent实现的，第一次部署比较麻烦，流量每天1G*n（GAE apps） ss：配合ss服务器食用 安卓软件 微博国际版 日事清 Newton 邮件客户端 JuiceSSh 双开助手 BusyBox + 终端模拟器]]></content>
      <categories>
        <category>资源</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux 下配置sublime text]]></title>
    <url>%2F2016%2F07%2F11%2F%E6%8A%80%E6%9C%AF%2FSublime-Text3-On-Linux%2F</url>
    <content type="text"><![CDATA[Linux下配置sublime text Sublime Text是个跨平台的编辑器，支持Windows、Linux、Mac系统平台，支持各种语言的代码编辑，配合上对应的插件，话上点时间学习，你将会对它爱不释手，大大的提高你的编码效率。本文将讲解在Ubuntu 14.04系统中安装SublimeText 3，并配置SublimeClang插件来配置C/C++开发环境。 1. Sublime Text 3的下载安装 到官方网站上http://www.sublimetext.com/3下载64位（系统位64位）的.deb安装包（http://c758482.r82.cf2.rackcdn.com/sublime-text_build-3059_amd64.deb），下载后双击安装即可。安装好之后，通过命令subl即可打开程序，此时已经可以编写代码了。在开始之前建议先记下一些常用的快捷键，可参考：http://blog.csdn.net/cywosp/article/details/31791881 2. 安装Package ControlPackage Control是一个用于管理插件的好工具，可以用于安装、删除、禁用相应的插件，常用的插件都能在上面找到。其源码地址在https://github.com/wbond/package_control_channel上，安装非常方便，使用git将该代码先克隆下来即可，然后拷贝到~/.config/sublime-text-3/Packages/目录下并命名为Package Control即可。（也可以直接在github上打包下载，然后解压复制到~/.config/sublime-text-3/Packages/目录下并命名为Package Control）。 cd ~/.config/sublime-text-3/Packages/git clone https://github.com/wbond/package_control_channel.git Package\ Control 或者打开sublime_text然后按快捷键ctrl+`(Esc下面那个键)，在弹出的命令输入窗口输入下面信息回车即可： 12[python] view plain copyimport urllib.request,os,hashlib; h = &apos;2915d1851351e5ee549c20394736b442&apos; + &apos;8bc59f460fa1548d1514676163dafc88&apos;; pf = &apos;Package Control.sublime-package&apos;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &apos;http://packagecontrol.io/&apos; + pf.replace(&apos; &apos;, &apos;%20&apos;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&apos;Error validating download (got %s instead of %s), please try manual install&apos; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &apos;wb&apos; ).write(by) 重新启动SublimeText 3，然后使用快捷键Ctrl + Shift + p，在弹出的输入框中输入Package Control则可以看到Install Package的选项，选择它后一会儿（看左下角的状态）会弹出插件查询及安装窗口，输入想用的插件，选中回车即可。如果用于C/C++开发建议安装C++ snipptes，ConvertToUTF8，SublimeAStyleFormatter插件，具体代表什么意思baidu一下就清楚了。 3. 安装强大的SublimeClang插件 SublimeClang是Sublime Text中唯一的C/C++自动补全插件，功能强大，自带语法检查功能，不过最近作者已经停止更新了，目前只能在Sublime Text 2的Package Control中可以找到并自动安装，在SublimeText 3中只能手动通过源码安装，其代码线在https://github.com/quarnster/SublimeClang中。具体安装步骤如下：安装相关软件 12345678910sudo apt-get install cmake build-essential clang gitcd ~/.config/sublime-text-3/Packagesgit clone --recursive https://github.com/quarnster/SublimeClang SublimeClangcd SublimeClangcp /usr/lib/x86_64-linux-gnu/libclang-3.4.so.1 internals/libclang.so #这一步很重要，如果你的clang库不是3.4版本的话，请将对应版本的库拷贝到internals中cd srcmkdir buildcd buildcmake ..make 一切成功的话将会在SublimeClang/internals目录中生成libcache.so库文件。重启Sublime Text，然后按快捷键Ctrl + ~ (Esc下面那个键)打开自带的控制输出，看看有没有错误，如果没有错误就说明一切OK了。接下来就是配置自己的文件了，按下ctrl + shift + p快捷键，在弹出的输入框中输入 sublimeclang settings ，然后选择带User那一行，在打开的文件中输入如下信息： 123456789101112131415161718&#123; &quot;show_output_panel&quot;: false, &quot;dont_prepend_clang_includes&quot;: true, &quot;inhibit_sublime_completions&quot;: false, &quot;options&quot;: [ &quot;-std=gnu++11&quot;, &quot;-isystem&quot;, &quot;/usr/include&quot;, &quot;-isystem&quot;, &quot;/usr/include/c++/*&quot;, &quot;-isystem&quot;, &quot;/usr/include/c++/4.8&quot;, &quot;-isystem&quot;, &quot;/usr/include/c++/4.8/*&quot;, &quot;-isystem&quot;, &quot;/usr/include/boost&quot;, &quot;-isystem&quot;, &quot;/usr/include/boost/**&quot;, &quot;-isystem&quot;, &quot;/usr/lib/gcc/x86_64-linux-gnu/4.8/include&quot;, &quot;-isystem&quot;, &quot;/usr/lib/gcc/x86_64-linux-gnu/4.8/include/*&quot; ]&#125; 注释：我的gcc版本为4.8，如果你的不是请替换对应的版本，在#include相应的头文件后保存当前文件，在接下来的操作中将更快的提示所包含在头文件的函数或者变量。 4. 工程实例通过菜单栏中的Project -&gt; Add Folder To Project…把你已有的原代码目录加入到Sublime Text中，然后通过Project -&gt; Save Project As…来保存你的项目，这样就创建好了项目。例如我的机器在/media/WinE/WorkStation/Swift中有个C++项目，代码分别放在了Swift下的swift/base和swift/disruptor两个目录下，现在想要把这两个目录中的内容在写代码时能够自动提示则需要相应的配置修改。Project -&gt; Edit Project，在所打开的配置文件中我更改如下： 12345678910111213141516171819&#123; &quot;folders&quot;: [ &#123; &quot;follow_symlinks&quot;: true, &quot;path&quot;: &quot;/media/WinE/WorkStation/Swift&quot; &#125; ], &quot;settings&quot;: &#123; &quot;sublimeclang_options&quot;: [ &quot;-I/media/WinE/WorkStation/Swift&quot;, &quot;-I/media/WinE/WorkStation/Swift/swift/base&quot;, &quot;-I/media/WinE/WorkStation/Swift/swift/disruptor&quot;, ] &#125;&#125;]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-3]]></title>
    <url>%2F2016%2F07%2F06%2F%E6%8A%80%E6%9C%AF%2Fstudy-of-python-3%2F</url>
    <content type="text"><![CDATA[正则表达式 通过正则表达式匹配字符串，\w+\@\w+.\w+ 匹配一个邮箱地址，\w可以匹配一个数字或字母，\d \D \s \S,[a-z,A-Z,0-9,_] 取值范围；[P|p]ython 匹配Python或python；^\d 必须以数字开头； \d$ 必须以数字结尾； re模块 判断字符串是否匹配re.match() 12345test = &apos;用户输入的字符串&apos;if re.match(r&apos;正则表达式&apos;, test): print &apos;ok&apos;else: print &apos;failed&apos; 切分字符串，用正则表达式试试： 12&gt;&gt;&gt; re.split(r&apos;\s+\,\;&apos;, &apos;a b c&apos;)[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;] 分组正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group） 1234567891011121314&gt;&gt;&gt; m = re.match(r&apos;^(\d&#123;3&#125;)-(\d&#123;3,8&#125;)$&apos;, &apos;010-12345&apos;)&gt;&gt;&gt; m&lt;_sre.SRE_Match object at 0x1026fb3e8&gt;&gt;&gt;&gt; m.group(0) # `group(0)`永远是原始字符串&apos;010-12345&apos;&gt;&gt;&gt; m.group(1)&apos;010&apos;&gt;&gt;&gt; m.group(2)&apos;12345&apos;&gt;&gt;&gt; t = &apos;19:05:30&apos;&gt;&gt;&gt; m = re.match(r&apos;^(0[0-9]|1[0-9]|2[0-3]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])$&apos;, t)&gt;&gt;&gt; m.groups()(&apos;19&apos;, &apos;05&apos;, &apos;30&apos;) 预编译一个匹配模式被多次用到，可以先预编译，re.compile(‘string’) 贪婪模式 常用内建模块collections namedtuple 12345678&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Point = namedtuple(&apos;Point&apos;, [&apos;x&apos;, &apos;y&apos;]) #定义一个点&gt;&gt;&gt; Circle = namedtuple(&apos;Circle&apos;, [&apos;x&apos;, &apos;y&apos;, &apos;r&apos;])#定义一个圆&gt;&gt;&gt; p = Point(1, 2)&gt;&gt;&gt; p.x #按属性访问&gt;&gt;&gt; 1&gt;&gt;&gt; p.y&gt;&gt;&gt; 2 deque 双向列表，deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈 123456&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])&gt;&gt;&gt; q.append(&apos;x&apos;)&gt;&gt;&gt; q.appendleft(&apos;y&apos;)&gt;&gt;&gt; q&gt;&gt;&gt; deque([&apos;y&apos;, &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;x&apos;]) deque除了实现list的append()和pop()外，还支持appendleft()和popleft()，这样就可以非常高效地往头部添加或删除元素。 defaultdict 使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict： 1234567&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(lambda: &apos;N/A&apos;)&gt;&gt;&gt; dd[&apos;key1&apos;] = &apos;abc&apos;&gt;&gt;&gt; dd[&apos;key1&apos;] # key1存在&gt;&gt;&gt; &apos;abc&apos;&gt;&gt;&gt; dd[&apos;key2&apos;] # key2不存在，返回默认值&gt;&gt;&gt; &apos;N/A&apos; 注意默认值是调用函数返回的，而函数在创建defaultdict对象时传入。 除了在Key不存在时返回默认值，defaultdict的其他行为跟dict是完全一样的。 OrderedDict 使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。如果要保持Key的顺序，可以用OrderedDict： 1234567&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)])&gt;&gt;&gt; d # dict的Key是无序的&gt;&gt;&gt; &#123;&apos;a&apos;: 1, &apos;c&apos;: 3, &apos;b&apos;: 2&#125;&gt;&gt;&gt; od = OrderedDict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)])&gt;&gt;&gt; od # OrderedDict的Key是有序的&gt;&gt;&gt; OrderedDict([(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)]) 注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序： 123456&gt;&gt;&gt; od = OrderedDict()&gt;&gt;&gt; od[&apos;z&apos;] = 1&gt;&gt;&gt; od[&apos;y&apos;] = 2&gt;&gt;&gt; od[&apos;x&apos;] = 3&gt;&gt;&gt; od.keys() # 按照插入的Key的顺序返回&gt;&gt;&gt; [&apos;z&apos;, &apos;y&apos;, &apos;x&apos;] OrderedDict可以实现一个FIFO（先进先出）的dict，当容量超出限制时，先删除最早添加的Key： 12345678910111213141516171819from collections import OrderedDictclass LastUpdatedOrderedDict(OrderedDict): def __init__(self, capacity): super(LastUpdatedOrderedDict, self).__init__() self._capacity = capacity def __setitem__(self, key, value): containsKey = 1 if key in self else 0 if len(self) - containsKey &gt;= self._capacity: last = self.popitem(last=False) print 'remove:', last if containsKey: del self[key] print 'set:', (key, value) else: print 'add:', (key, value) OrderedDict.__setitem__(self, key, value) Counter Counter是一个简单的计数器，例如，统计字符出现的个数： 1234567&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in &apos;programming&apos;:&gt;&gt;&gt; ... c[ch] = c[ch] + 1&gt;&gt;&gt; ...&gt;&gt;&gt; c&gt;&gt;&gt; Counter(&#123;&apos;g&apos;: 2, &apos;m&apos;: 2, &apos;r&apos;: 2, &apos;a&apos;: 1, &apos;i&apos;: 1, &apos;o&apos;: 1, &apos;n&apos;: 1, &apos;p&apos;: 1&#125;) Counter实际上也是dict的一个子类，统计的元素作为key，出现次数是key的值。 base64对二进制数据进行处理，每3个字节一组，一共是3x8=24bit，划为4组，每组正好6个bit，对应编码规则里定义的64个字符。二进制字节不是3的倍数，Base64用\x00字节在末尾补足后，再在编码的末尾加上1个或2个=号。解码的=。urlsafe编码，把字符+和/分别变成-和_。字符+和/，在URL中就不能直接作为参数。 123456&gt;&gt;&gt; base64.b64encode('i\xb7\x1d\xfb\xef\xff')'abcd++//'&gt;&gt;&gt; base64.urlsafe_b64encode('i\xb7\x1d\xfb\xef\xff')'abcd--__'&gt;&gt;&gt; base64.urlsafe_b64decode('abcd--__')'i\xb7\x1d\xfb\xef\xff' struct Python提供了一个struct模块来解决str和其他二进制数据类型的转换。 struct的pack函数把任意数据类型变成字符串： 123&gt;&gt;&gt; struct.unpack(&apos;&gt;IH&apos;, &apos;\xf0\xf0\xf0\xf0\x80\x80&apos;)(4042322160, 32896)### &gt;IH的说明，后面的str依次变为I：4字节无符号整数和H：2字节无符号整数,&gt;表示大端存储方式 struct模块定义的数据类型可以参考： Python官方文档 hashlib用户名如果不可变可以作为salt与passwd一起hash 12345678910import hashlibmd5 = hashlib.md5()md5.update('how to use md5 in python hashlib?')print md5.hexdigest() import hashlibsha1 = hashlib.sha1()sha1.update('how to use sha1 in ')sha1.update('python hashlib?') #多次update和一次做完结果一样print sha1.hexdigest() itertools count() cycle() repeat() chain() groupby() imap() imap()可以作用于无穷序列，并且，如果两个序列的长度不一致，以短的那个为准。 12for x in itertools.imap(lambda x, y: x * y, [10, 20, 30], itertools.count(1)): print x 注意imap()返回一个迭代对象，而map()返回list。当你调用map()时，已经计算完毕.当你调用imap()时，并没有进行任何计算,必须用for循环迭代，每次计算出一个元素。 ifliter() XMLHTMLParser###第三方模块 PILNumpy图形界面网络编程TCP/IPTCP编程UDP编程电子邮件SMTP发送邮件pop3接收邮件访问数据库使用SQLite使用MySQL使用SQLAlchemyWeb开发HTTP协议HTMLWSGI接口使用Web框架使用模板协程12]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础学习笔记]]></title>
    <url>%2F2016%2F07%2F05%2F%E6%8A%80%E6%9C%AF%2Flinux%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Linux简介系统安装与grub引导 系统运行级别6级，0：关机；1：单用户模式，进行系统维护；2：多用户，无网络连接；3：完全多用户模式，默认不用图形化；4：保留；5：窗口模式，多用户；6：重启。 安装linux grub系统引导 登陆系统有图形登陆、终端登陆、远程终端登陆。 linux默认提供6个终端，使用ctrl + alt + F1~F6切换。 使用ctrl + alt + F7切回桌面图形登陆。 在字符终端中使用startx可以启动图形桌面。 终端用户退出登陆exit. 常用命令 ls pwd cd cat ps ifconfig netstat -ano 查看命令帮助文档 man 和 info在查看结果中 使用 /keyword进行关键字搜索，n向下查找，N向上查找。空格向下翻页，pageup/pagedown翻页，q退出查看。 123$ man -f reboot man -f 查看命令位于哪些man文件里$ man 2 reboot 查看2号文档里的reboot命令$ info ls 获取ls命令的说明文档 用户管理用户和用户组 UIDUID为0的是root超级用户，1~500是系统用户，500以后是普通用户。 GID一个用户可以属于多个组 12345$ id 查看自己的uid$ group 查看自己的gid$ whoami 查看用户自己的信息$ who 查看当前已登录的用户信息$ w 查看更详细的用户信息 /etc/passwd /etc/shadow两个文件记录了用户的用户名和密码。 账号管理和用户切换 增加删除用户 1234# useradd -u 555 - g group1 -d /home/john -s /bin/bash john 指定uid，加入的组，家目录，shell路径，组必须是已经存在的# useradd -G group1,group2 john -G加入一系列的组# userdel john 删除了john用户，从shadow中清除记录，用户文件还在# userdel -r john 删除john的home目录和邮件 修改密码和删除密码 1234567# passwd 不加参数则是修改当前用户的密码# passwd user1 修改user1的密码，新建的用户必须修改密码后才能登陆# passwd -d user1 删除密码，即密码为空# passwd -l user1 禁用user1的密码，实际在shadow文件的记录前加了`!`# passwd -u user1 解锁账号# usermod -L user1 # usermod -U user1 原理同上 修改用户# usermod 增加删除组 12# groupadd group1 新增group1# groupdel group1 删除用户组，如果组内有用户，则删除失败 切换用户 123456$ su 直接切换到root用户，当前用户环境不变，工作目录，shell等# exit 退出root，切回普通用户$ su - 切换到root用户，同时使用root用户环境# su - john 切换为john用户，root切换为任何user无需密码， 普通user相互切换要知道user密码$ sudo command 普通用户以root身份执行某条命令，输入自己的密码即可 用户能否使用sudo命令，在/etc/sudoers中配置。使用visudo命令进行配置。123456789# User privilege specificationroot ALL=(ALL:ALL) ALLjohn ALL=(ALL:ALL) ALLluna ALL=(ALL:ALL) NOPASSWD:/sbin/shutdown# 在此处添加，允许john 从任何地方登陆后，执行任何人的，任何命令,对应3个all# 允许luna 不用输密码即可执行 sudo shutdown 命令。# Members of the admin group may gain root privileges%admin ALL=(ALL) ALL# 允许admin组的所有成员，......执行sudo 文件管理文件和目录管理和权限 目录结构 123456789101112131415161718/├── bin 常见用户指令├── boot 内核和启动文件├── cdrom 光驱挂载位置├── dev 设备文件├── etc 系统和服务配置├── home 用户主目录├── lib 系统函数库├── lib64├── lost+found ext3 文件系统用于磁盘检查├── media 挂载u盘等临时文件系统├── mnt 系统加载文件系统的挂载点├── opt 第三方软件安装目录├── root root用户主目录├── sbin 系统管理命令├── tmp 临时文件存放目录├── usr 存放和用户直接相关的文件├── var 路径和特殊目录绝对路径，从/根路径开始，比如/root/abc;当前路径，pwd命令可以查看。shell命令执行默认基于当前路径；特殊目录.和..，分别表示当前目录和当前目录的父目录。相对路径，从当前路径开始计算,通常配合.和..使用。 文件操作 1234567891011121314151617181920212223### 文件创建$ touch a.txt touch一个已存在的文件，会更新其时间戳属性$ vi a.txt$ echo &apos;XXX&apos; &gt; a.txt### 删除文件$ rm -f a.txt 直接删除，不用确认### 复制、移动文件$ cp a.txt /home/test/$ cp a.txt /path/to/b.txt 可以实现重命名$ mv a.txt /path/to/filename$ mv a.txt /path/to/### 文件查看$ cat readme.txt$ cat -n readme.txt 查看文件时显示行号$ head readme.txt 默认显示前10行内容$ head -n 20 readme.txt 查看前20行内容$ tail readme.txt 查看最后10行内容，-n 指定显示的行数目$ tail -f error.log 文件不断写入时，可以动态查看文件末尾的内容### 改变属主# chown john a.txt 改变a的所有者为john用户# chown :john a.txt 改变a的所有组为john组# chown -R john:john dir_a 递归改变dir_a下所有文件的所有组为john，所有者为john# chgrp -R john dir_b 递归改变所有组 目录操作 123$ cd /home/ 进入到指定目录$ mkdir dir_a 创建目录$ rm -rf dir_a 删除目录及其包含的全部内容，无需确认 权限和属性 1234567891011# ls -al总用量 1712drwx------ 27 root root 4096 10月 31 12:54 .-rw------- 1 root root 16213 10月 30 22:25 .bash_history#第一个字符的含义：d目录，-普通文件，l链接文件，b块文件， c字符文件，s socket文件，p管道文件#接下来的3组 X 3个字符，表示文件所有者，文件所有组和其他用户对该文件的权限# rwx为可读、可写、可执行，-表示不拥有该位置的权限 # 第二列的数字为连接数，文件为1，目录为其包含的目录数（包括特殊目录.和..）# 第三四列 为文件的所有人和所有组# 第五列文件大写，第六列文件最近修改时间 文件隐藏属性a和i123456# lsattr run_dedicated_servers.sh -------------e-- run_dedicated_servers.sh## 有13个短横，# chattr +a a.ttx append,a属性的文件即使root也不能删除，可以追加的方式写文件# chattr +i b.txt i属性确保文件无法删除写入和改名，常用于关键配置文件# man chattr 查看更多 文件特殊属性SUID/SGID/Sticky1234567891011# ll /usr/bin/passwd -rwsr-xr-x 1 root root 54256 3月 29 2016 /usr/bin/passwd*### 原本的执行权限x变成了s，表示其他用户可以以文件的所有者身份来执行该文件### SGID与SUID类似，都只能用于可执行文件# chmod u+s ‘可执行文件’ 设置SUID# chmod g+s ‘可执行文件’ 设置SGID，用的较少### Sticky属性用于目录# ll -d /tmpdrwxrwxrwt 26 root root 4096 10月 31 14:24 /tmp/### 最后一个t表示，任何人都能在此创建修改文件，但只有owner和root可以删除文件。# chmod o+t ‘目录’ 添加t属性 默认权限和umask，用户或系统创建的文件有默认的权限设置，root和普通用户创建文件，权限为644，root的目录为755，普通用户的为775. 文件查找 findfind PATH -name FILENAME locatelocate查找依赖一个数据库，因此使用之前一般先执行updatedb。 which/whereis 查找可执行文件which command可以找到命令所在位置，which在path路径中查找。whereis同时给出相关man文件。 文件压缩解压1234567# gzip a.txt 压缩成gz格式压缩包# gunzip a.txt.gz 解压# tar -zcvf a.tar.gz /home/a 创建压缩文件### 参数z，使用gzip压缩；参数c，创建压缩文件；参数f，使用文件名；参数v，详情模式；# tar -zxvf a.tar.gz -C /tmp -C 指定解压到的目录# bzip2 -z a 压缩得到bz2格式的压缩包# bzip2 -d a.bz2 解压文件 文件系统磁盘分区，文件系统的挂载和卸载1234# fdisk -l 查看磁盘设备和分区# mount DEVICE MOUNT_POINT 将设备挂载到指定位置### 设备自动挂载，在/etc/fstab中配置# df -h 查看磁盘使用情况 linux逻辑卷硬链接和软链接硬链接：多个文件名指向同一个文件的inode索引节点，使得一个文件拥有多个合法的路径。删除一个硬链接不会影响其他的硬链接和文件本身。目录无法创建硬链接，不同文件系统/分区之间不能建立硬链接。ln &#39;源文件&#39; &#39;新建硬链接文件&#39;软连接：包含指向另一个文件的路径，类似windows的快捷方式。ln -s &#39;源文件&#39; &#39;新建软连接&#39; 字符处理管道和grep管道连接符command1 | command2，将command1的输出作为command2的输入。grep文本搜索123$ cat a.txt | grep &apos;name&apos; 打印含有name的行$ grep [-ivnc] &apos;search_str&apos; CONTENT/FILE### i忽略大小写，v反向选择，n同时输出行号，c统计匹配的行数 sort排序和uniq去重1234567891011### sort参数，-t指定分割符，k排序的列，n以数字排序，默认是字符排序，r反向排序，即降序# sort -t &apos;:&apos; -k 3 -n /etc/passwd daemon:*:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:*:2:2:bin:/bin:/usr/sbin/nologinsys:*:3:3:sys:/dev:/usr/sbin/nologinsync:*:4:65534:sync:/bin:/bin/syncgames:*:5:60:games:/usr/games:/usr/sbin/nologinman:*:6:12:man:/var/cache/man:/usr/sbin/nologinlp:*:7:7:lp:/var/spool/lpd:/usr/sbin/nologin### uniq只能去除连续重复的行，因此一般配合sort使用# uniq -ic CONTENT 忽略大小写，输出该行重复的次数 cut、tr、paste、split网络管理网络配置和dns配置1234567891011121314151617181920$ ifconfig 查看网卡信息eth0 Link encap:以太网 硬件地址 f4:8e:38:b8:1e:fd inet 地址:172.16.27.118 广播:172.16.27.255 掩码:255.255.252.0 inet6 地址: fe80::ec5d:eb79:5e03:112/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 跃点数:1 接收数据包:1002 错误:0 丢弃:3 过载:0 帧数:0 发送数据包:147 错误:0 丢弃:0 过载:0 载波:0 碰撞:0 发送队列长度:1000 接收字节:107932 (107.9 KB) 发送字节:18302 (18.3 KB)eth1 ***lo ***wlan ***$ ifconfig eth0 查看指定网卡### ifconfig动态修改网卡的配置，重启后失效# ifconfig eth0 192.168.159.130 netmask 255.255.255.0# ifconfig eth0 192.168.159.130/24 指定eth0的ip和子网# ifconfig eth0 down 停用网卡# ifconfig eth0 up 启用网卡# ifdown/ifup eth0 效果同上# route add/del default gw 192.168.8.1 修改网关 修改网卡配置文件1234567891011121314/etc/network/interfaces文件默认的内容如下： auto lo iface lo inet loopback 在后面添加内容 1、获取动态配置： auto eth0 iface eth0 inet dhcp 2、获取静态配置： auto eth0 iface eth0 inet static address 192.168.0.1 netmask 255.255.255.0 gateway 192.168.0.1 重启networking服务 修改DNS配置1234567永久修改网卡DNSsudo –icd /etc/resolvconf/resolv.conf.dvim base添加如下内容nameserver 8.8.8.8nameserver 8.8.4.4 网络测试 ping发icmp echo请求，判断主机是否可达 hosthost命令查询DNS记录 traceroute追溯数据包所经过的路由 网络故障排查1、ping 127.0.0.1 判断网卡工作是否正常，tcp/ip协议栈出问题2、ping 本机ip 判断本地设备驱动/物理端口3、ping 同网段其他主机 看交换机是否正常4、ping 网关ip5、ping 公网ip ，本地路由，nat6、ping 公网域名 dns设置是否正确 进程管理查看进程1234567# ps aux 显示所有包含其他使用者的有效进程USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.1 119732 5812 ? Ss 16:49 0:01 /sbin/init splashroot 2 0.0 0.0 0 0 ? S 16:49 0:00 [kthreadd]# top 查看结果是实时动态变化的# 按下O，大写的，进入排序过滤器，然后... 杀死进程12345678910#### 先查看要杀死进程的PID# ps -ef | grep &apos;ssh&apos;root 975 1 0 16:49 ? 00:00:00 /usr/sbin/sshd -Droot 3291 975 0 16:50 ? 00:00:00 sshd: root@pts/8root 4873 3782 0 20:05 pts/8 00:00:00 grep --color=auto ssh# pidof sshd3291 975# kill 3291 停止pid = 3291的进程# kill -1 pid 1重启，9强制退出，15正常退出（默认）# killall sshd 停止进程，跟进程名 查询进程打开的文件1234567891011# lsof -i:22 查询打开22端口的进程COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 975 root 3u IPv4 19938 0t0 TCP *:ssh (LISTEN)sshd 975 root 4u IPv6 19940 0t0 TCP *:ssh (LISTEN)# lsof -c sshd 显示COMMAND中包含指定字符串的进程所打开的全部文件COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 975 root cwd DIR 8,2 4096 2 /sshd 975 root rtd DIR 8,2 4096 2 /sshd 975 root txt REG 8,2 799216 12328642 /usr/sbin/# lsof FILENAME 显示所有打开FILENAME文件的进程 进程优先级调整：nice、renice使用top时，可以看到进程的NI、PR字段。NI表示进程优先级，取值-20~19，默认为0。普通用户可以设置进程NI值0~19.PR是动态优先级，系统进程调用采取的’动态优先级‘调度算法。最终程序的优先级是NI+PR，定义和修改进程优先级nice和renice12345# topPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 119732 5812 3904 S 0.0 0.1 0:01.58 systemd# nice -n -12 ./job.sh 设定程序运行时的优先级为-12# renice -10 -p 5555 将优先级调整为-10 编译、安装软件安装包管理器编译安装执行定时任务 at crontab screen 正则表达式与sed、awkvi和vim单独讲 shell编程单独讲]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-2]]></title>
    <url>%2F2016%2F07%2F04%2F%E6%8A%80%E6%9C%AF%2Fstudy-of-python-2%2F</url>
    <content type="text"><![CDATA[错误、调试、测试 try机制 12345678910 try: print 'try...' r = 10 / 0 #此处出现异常直接跳转到except print 'result:', r except ZeroDivisionError, e: print 'except:', e finally: # 若没有错误发生，except语句块不会被执行，但是finally如果有，则一定会被执行（可以没有finally语句）。#此外，如果没有错误发生，可以在except语句块后面加一个else，当没有错误发生时，会自动执行else语句： print 'finally...'print 'END' 错误类型都继承自BaseException，所以在使用except时需要注意的是，它不但捕获该类型的错误，还把其子类也“一网打尽”。比如ValueError是StandardError的子类，如果有except先捕获了StandardError，则第二个except就捕获不到ValueError。 记录错误 Python内置的logging模块可以非常容易地记录错误信息。logging.exception(e) 抛出错误如果要抛出错误，首先根据需要，可以定义一个错误的class，选择好继承关系，然后，用raise语句抛出一个错误的实例 12345678class FooError(StandardError): pass# 只有在必要的时候才定义我们自己的错误类型。如果可以选择Python已有的内置的错误类型（比如ValueError，TypeError），尽量使用Python内置的错误类型。def foo(s): n = int(s) if n==0: raise FooError('invalid value: %s' % s) return 10 / n 一种常见的抛出用法 12345678910def foo(s): n = int(s) return 10 / n def bar(s): try: return foo(s) * 2 except StandardError, e: #此处捕获只是为了记录错误，如果无法处理，可以上抛 print 'Error!' raise 调试 简单粗暴的print将可能出错的变量打印出来，print太多的话看着不爽 断言凡是用print来辅助查看的地方，都可以用断言（assert）来替代，assert n!=0,’n is zero’ ,如果断言失败，assert语句本身就会抛出AssertionError：n is zero运行时加参数-O可以关闭断言。 logging 12345678import logginglogging.basicConfig(level=logging.INFO) #它允许你指定记录信息的级别，有debug，info，warning，error等几个级别，#当我们指定level=INFO时，logging.debug就不起作用了。#同理，指定level=WARNING后，debug和info就不起作用了s = '0'n = int(s)logging.info('n = %d' % n)print 10 / n IDE 断点据说最好的IDE是pycharm，我用的是wing IDE，有个debug probe窗口，可以程序断点暂停后进行python命令交互，print之类的。编辑器推荐sublime text。 单元测试“测试驱动开发”（TDD：Test-Driven Development）；编写单元测试时，我们需要编写一个测试类，从unittest.TestCase继承。以test开头的方法就是测试方法，不以test开头的方法不被认为是测试方法，测试的时候不会被执行。 对每一类测试都需要编写一个test_xxx()方法。 由于unittest.TestCase提供了很多内置的条件判断，我们只需要调用这些方法就可以断言输出是否是我们所期望的。最常用的断言就是assertEquals()： 另一种重要的断言assertRaises()就是期待抛出指定类型的Error，比如通过d[&#39;empty&#39;]访问不存在的key时，断言会抛出KeyError：运行测试单元if __name__ == &#39;__main__&#39;: unittest.main()可以在单元测试中编写两个特殊的setUp()和tearDown()方法。这两个方法会分别在每调用一个测试方法的前后分别被执行 文档测试Python内置的“文档测试”（doctest）模块可以直接提取注释中的代码并执行测试。 IO 文件读写 123456789try: f = open('/path/to/file', 'r') print f.read()finally: if f: f.close()# 或者 使用with，Python会自动调用close方法with open('/path/to/file', 'r') as f: print f.read() 调用read()会一次性读取文件的全部内容，如果文件有10G，内存就爆了，所以，要保险起见，可以反复调用read(size)方法，每次最多读取size个字节的内容。另外，调用readline()可以每次读取一行内容，调用readlines()一次读取所有内容并按行返回list。因此，要根据需要决定怎么调用。 如果文件很小，read()一次性读取最方便；如果不能确定文件大小，反复调用read(size)比较保险；如果是配置文件，调用readlines()最方便 1234for line in f.readlines(): print line.strip() #strip()删除字符串末尾的回车等字符。os.mknod("new.txt")#创建空白新文件 fp=open('a.txt','w')#打开一个文件，如果不存在则创建文件 文件操作 fp.read([size]) #size为读取的长度，以byte为单位 fp.readline([size]) #读一行，如果定义了size，有可能返回的只是一行的一部分 fp.readlines([size]) #把文件每一行作为一个list的一个成员，并返回这个list。其实它的内部是通过循环调用readline()来实现的。如果提供size参数，size是表示读取内容的总长，也就是说可能只读到文件的一部分。 fp.write(str) #把str写到文件中，write()并不会在str后加上一个换行符 fp.writelines(seq) #把seq的内容全部写到文件中(多行一次性写入)。这个函数也只是忠实地写入，不会在每行后面加上任何东西。 fp.close() #关闭文件。python会在一个文件不用后自动关闭文件，不过这一功能没有保证，最好还是养成自己关闭的习惯。 如果一个文件在关闭后还对其进行操作会产生ValueError fp.flush() #把缓冲区的内容写入硬盘 fp.fileno() #返回一个长整型的”文件标签“ fp.isatty() #文件是否是一个终端设备文件（unix系统中的） fp.tell() #返回文件操作标记的当前位置，以文件的开头为原点 fp.next() #返回下一行，并将文件操作标记位移到下一行。把一个file用于for … in file这样的语句时，就是调用next()函数来实现遍历的。 fp.seek(offset[,whence]) #将文件打操作标记移到offset的位置。这个offset一般是相对于文件的开头来计算的，一般为正数。但如果提供了whence参数就不一定了，whence可以为0表示从头开始计算，1表示以当前位置为原点计算。2表示以文件末尾为原点进行计算。需要注意，如果文件以a或a+的模式打开，每次进行写操作时，文件操作标记会自动返回到文件末尾。 fp.truncate([size]) #把文件裁成规定的大小，默认的是裁到当前文件操作标记的位置。如果size比文件的大小还要大，依据系统的不同可能是不改变文件，也可能是用0把文件补到相应的大小，也可能是以一些随机的内容加上去。 open模式 w 以写方式打开，a 以追加模式打开 (从 EOF 开始, 必要时创建新文件)r+ 以读写模式打开w+ 以读写模式打开 (参见 w )a+ 以读写模式打开 (参见 a )rb 以二进制读模式打开wb 以二进制写模式打开 (参见 w )ab 以二进制追加模式打开 (参见 a )rb+ 以二进制读写模式打开 (参见 r+ )wb+ 以二进制读写模式打开 (参见 w+ )ab+ 以二进制读写模式打开 (参见 a+ ) 目录操作 os.mkdir(“file”) 创建目录复制文件：shutil.copyfile(“oldfile”,”newfile”) oldfile和newfile都只能是文件shutil.copy(“oldfile”,”newfile”) oldfile只能是文件夹，newfile可以是文件，也可以是目标目录复制文件夹：shutil.copytree(“olddir”,”newdir”) olddir和newdir都只能是目录，且newdir必须不存在重命名文件（目录）os.rename(“oldname”,”newname”) 文件或目录都是使用这条命令移动文件（目录）shutil.move(“oldpos”,”newpos”) 删除文件os.remove(“file”)删除目录os.rmdir(“dir”)只能删除空目录shutil.rmtree(“dir”) 空目录、有内容的目录都可以删转换目录os.chdir(“path”) 换路径 文件或文件夹操作python中对文件、文件夹 （文件操作函数） 的操作需要涉及到os模块和shutil模块。 得到当前工作目录，即当前Python脚本工作的目录路径: os.getcwd() 返回指定目录下的所有文件和目录名:os.listdir() 函数用来删除一个文件:os.remove() 删除多个目录：os.removedirs（r“c：\python”） 检验给出的路径是否是一个文件：os.path.isfile() 检验给出的路径是否是一个目录：os.path.isdir() 判断是否是绝对路径：os.path.isabs() 检验给出的路径是否真地存:os.path.exists() 返回一个路径的目录名和文件名:os.path.split() eg os.path.split(‘/home/swaroop/byte/code/poem.txt’) 结果：(‘/home/swaroop/byte/code’, ‘poem.txt’) 分离扩展名：os.path.splitext() 获取路径名：os.path.dirname() 获取文件名：os.path.basename() 运行shell命令: os.system() 读取和设置环境变量:os.getenv() 与os.putenv() 给出当前平台使用的行终止符:os.linesep Windows使用’\r\n’，Linux使用’\n’而Mac使用’\r’ 指示你正在使用的平台：os.name 对于Windows，它是’nt’，而对于Linux/Unix用户，它是’posix’os.uname() 系统详细版本，Windows上无 重命名：os.rename（old， new） 创建多级目录：os.makedirs（r“c：\python\test”） 创建单个目录：os.mkdir（“test”） 获取文件属性：os.stat（file） 修改文件权限与时间戳：os.chmod（file） 终止当前进程：os.exit（） 获取文件大小：os.path.getsize（filename） 比如我们要列出当前目录下的所有目录，只需要一行代码： 12&gt;&gt;&gt; [x for x in os.listdir(&apos;.&apos;) if os.path.isdir(x)][&apos;.lein&apos;, &apos;.local&apos;, &apos;.m2&apos;, &apos;.npm&apos;, &apos;.ssh&apos;, &apos;.Trash&apos;, &apos;.vim&apos;, &apos;Adlm&apos;, &apos;Applications&apos;, &apos;Desktop&apos;, ...] 要列出所有的.py文件，也只需一行代码： 12&gt;&gt;&gt; [x for x in os.listdir(&apos;.&apos;) if os.path.isfile(x) and os.path.splitext(x)[1]==&apos;.py&apos;][&apos;apis.py&apos;, &apos;config.py&apos;, &apos;models.py&apos;, &apos;pymonitor.py&apos;, &apos;test_db.py&apos;, &apos;urls.py&apos;, &apos;wsgiapp.py&apos;] 序列化 我们把变量从内存中变成可存储或传输的过程称之为序列化，在Python中叫pickling，在其他语言中也被称之为serialization，marshalling，flattening等等。序列化之后，就可以把序列化后的内容写入磁盘，或者通过网络传输到别的机器上。反过来，把变量内容从序列化的对象重新读到内存里称之为反序列化，即unpickling。 Python提供两个模块来实现序列化：cPickle和pickle。这两个模块功能是一样的，区别在于cPickle是C语言写的，速度快，pickle是纯Python写的，速度慢，跟cStringIO和StringIO一个道理。用的时候，先尝试导入cPickle，如果失败，再导入pickle： 123456789101112131415161718try: import cPickle as pickleexcept ImportError: import pickle&gt;&gt;&gt; d = dict(name=&apos;Bob&apos;, age=20, score=88)&gt;&gt;&gt; pickle.dumps(d) #序列化为str&quot;(dp0\nS&apos;age&apos;\np1\nI20\nsS&apos;score&apos;\np2\nI88\nsS&apos;name&apos;\np3\nS&apos;Bob&apos;\np4\ns.&quot;&gt;&gt;&gt; f = open(&apos;dump.txt&apos;, &apos;wb&apos;)&gt;&gt;&gt; pickle.dump(d, f) #pickle.dump()#写入文件&gt;&gt;&gt; f.close()#当我们要把对象从磁盘读到内存时，可以先把内容读到一个str，然后用pickle.loads()方法反序列化出对象，也可以直接用pickle.load()方法从一个file-like Object中直接反序列化出对象。&gt;&gt;&gt; f = open(&apos;dump.txt&apos;, &apos;rb&apos;)&gt;&gt;&gt; d = pickle.load(f)&gt;&gt;&gt; f.close()&gt;&gt;&gt; d&#123;&apos;age&apos;: 20, &apos;score&apos;: 88, &apos;name&apos;: &apos;Bob&apos;&#125; JSON 标准化对象序列格式Python内置的json模块提供了非常完善的Python对象到JSON格式的转换。我们先看看如何把Python对象变成一个JSON： 1234&gt;&gt;&gt; import json&gt;&gt;&gt; d = dict(name=&apos;Bob&apos;, age=20, score=88)&gt;&gt;&gt; json.dumps(d)&apos;&#123;&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;&#125;&apos; dumps()方法返回一个str，内容就是标准的JSON。类似的，dump()方法可以直接把JSON写入一个file-like Object。 要把JSON反序列化为Python对象，用loads()或者对应的load()方法，前者把JSON的字符串反序列化，后者从file-like Object中读取字符串并反序列化： 123&gt;&gt;&gt; json_str = &apos;&#123;&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;&#125;&apos;&gt;&gt;&gt; json.loads(json_str)&#123;u&apos;age&apos;: 20, u&apos;score&apos;: 88, u&apos;name&apos;: u&apos;Bob&apos;&#125; 有一点需要注意，就是反序列化得到的所有字符串对象默认都是unicode而不是str。 JSON进阶JSON序列化一个类对象，json.dumps(class_a,defaul=func_x) ,func_x函数，负责将对象变成可序列化的json对象。 1print(json.dumps(s, default=lambda obj: obj.__dict__)) JSON反序列化得到类对象：json.loads(json_str,object_hook=dict_2_student) 12345def dict2student(d): return Student(d['name'], d['age'], d['score'])json_str = '&#123;"age": 20, "score": 88, "name": "Bob"&#125;'print(json.loads(json_str, object_hook=dict2student)) 进程和线程windows下没有os.fork（）调用,multiprocessing模块就是跨平台版本的多进程模块。 process 1234567891011121314from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print 'Run child process %s (%s)...' % (name, os.getpid())if __name__=='__main__': print 'Parent process %s.' % os.getpid() p = Process(target=run_proc, args=('test',)) ###创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动，这样创建进程比fork()还要简单 print 'Process will start.' p.start() p.join() #join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。 print 'Process end.' pool 12345678910111213141516171819from multiprocessing import Pool #使用进程池创建大量子进程import os, time, randomdef long_time_task(name): print 'Run task %s (%s)...' % (name, os.getpid()) start = time.time() time.sleep(random.random() * 3) end = time.time() print 'Task %s runs %0.2f seconds.' % (name, (end - start))if __name__=='__main__': print 'Parent process %s.' % os.getpid() p = Pool() for i in range(5): p.apply_async(long_time_task, args=(i,)) print 'Waiting for all subprocesses done...' p.close() p.join() print 'All subprocesses done.' 进程间通信操作系统提供了很多机制来实现进程间的通信。Python的multiprocessing模块包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据。 多线程Python的标准库提供了两个模块：thread和threading，thread是低级模块，threading是高级模块，对thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。启动一个线程就是把一个函数传入并创建Thread实例，然后调用start()开始执行： 123456789101112131415161718import time, threading# 新线程执行的代码:def loop(): print 'thread %s is running...' % threading.current_thread().name n = 0 while n &lt; 5: n = n + 1 print 'thread %s &gt;&gt;&gt; %s' % (threading.current_thread().name, n) time.sleep(1) print 'thread %s ended.' % threading.current_thread().nameprint 'thread %s is running...' % threading.current_thread().namet = threading.Thread(target=loop, name='LoopThread')t.start()t.join()print 'thread %s ended.' % threading.current_thread().name##由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的threading模块有个current_thread()函数，它永远返回当前线程的实例。主线程实例的名字叫MainThread，子线程的名字在创建时指定，我们用LoopThread命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为Thread-1，Thread-2… Lock多线程访问修改同一变量，互斥操作，创建一个锁就是通过threading.Lock()来实现。注意死锁问题。 12345678910111213balance = 0lock = threading.Lock()def run_thread(n): for i in range(100000): # 先要获取锁: lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() Python解释器由于设计时有GIL全局锁，导致了多线程无法利用多核。多线程的并发在Python中就是一个美丽的梦。 Threadlocal每个线程都有自己的局部变量，threading.local()相当于创建了一个全局的dict，每个属性都是线程的局部变量，多线程读写不会相互干扰。ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。 123456789101112131415161718import threading# 创建全局ThreadLocal对象:local_school = threading.local()def process_student(): print 'Hello, %s (in %s)' % (local_school.student, threading.current_thread().name)def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student()t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')t1.start()t2.start()t1.join()t2.join() 进程vs线程 多任务的一般模式master-worker。多进程模式最大的优点就是稳定性高，缺点是创建进程的代价大；多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。异步IO，对于Python语言，单进程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。 分布式进程Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-1]]></title>
    <url>%2F2016%2F07%2F02%2F%E6%8A%80%E6%9C%AF%2Fstudy-of-python%2F</url>
    <content type="text"><![CDATA[基础语法数据结构list len() 求长度 a[i] list a 中第i个元素， a[2:9] 第2到9个元素，a[-1] 倒数第一个元素。 a.append(elem) list尾部追加一个元素 a.insert(i,elem) list第i个位置插入一个元素 a.pop(i) list 删除第i个元素，不指定i则默认删除最后一个 list的元素可以类型不同，也可以是list tupletuple不可变的元素集合，只有一个元素是的定义 a=(1,) 注意,是为了避免数学运算()的歧义 dict dict：key-value对，查询dict[key]可以取出对应的value。添加新键值对或修改值 dict[key]=newvalue。判断key是否存在 key in dict 或 dict.get(key,int-flag) 如果key不存在则返回指定的数值，不指定则返回none，none在交互式命令中不显示。删除一个key-value，用dict.pop(key) dict 插入查找速度快，占用内存多，list查找、插入与元素数量有关，占用内存空间少 set set： 也是一组key的集合，但是没有value set 初始化传入参数由一个list提供，s=set([1,1,2,2,3,4,5]),输出结果s为set([1,2,3,4,5])，其中的[]表示集合，重复元素被剔除，而不是list。 添加元素s.add(key),删除元素 s.remove(key) s1 &amp; s2 求交集；s1 | s2 ,求并集。 判断和循环 if 123456if x : # x非0非None即True xxxelif y: # 可以多个if嵌套使用 oooelse: xxoo for 123# xxxx为可迭代对象，list，dict，set和任何实现__next_item__方法的对象for x in xxxx: print x while 12345678# 当条件为真，执行循环while a&gt;0: a += 1 if a == 100: continue # a为100时跳过本次，进入下一层循环 if a &gt; 1000: # a为1000时跳过本次，跳出循环 break print a 函数 help(func) 查看函数fanc的帮助信息；dir(func)查看函数的内置方法；isinstance（x，（int,float））,类型检查； 定义函数 def myfunc(para1,para2): pass ;pass占位符，不知道写啥时可以先用它占位。 return x 返回x，没有return也会返回none，return可以返回多个值，但其本质是返回一个tuple。 默认参数放到后面，def power(x,n=2): pass;调用函数时默认参数如果不按顺序赋值，则需指定参数名。默认参数的值要设为不可变的变量类型，而不能是list。 可变参数，即输入的可以是0或多个参数不固定。定义可变参数仅在参数前面加了一个*号。在函数内部，参数numbers接收到的是一个tuple，因此调用该函数时，可以传入任意个参数，包括0个参数。如果变量已经保存为list或tuple，则可以用 *list来传递参数，而不用list[0],list[1] … 那么麻烦； 1234def func(*numbers): for num in numbers : pass #numbers是一个tuple pass 关键字参数：允许传入0或多个带参数名的参数，在函数内部作为一个dict, 使用**para 表示关键字参数.调用函数时也可将dict转换为关键字参数传递进去，func（**dict） 12def student(name,age,**p_other): print name,age,p_other 组合参数：参数定义的顺序必须是：必选参数、默认参数、可变参数和关键字参数。在函数调用的时候，Python解释器自动按照参数位置和参数名把对应的参数传进去，因此，对于任意函数，都可以通过类似func(args, *kw)的形式调用它，无论它的参数是如何定义的。 递归函数 注意栈溢出问题 Python高级特性切片切片 L[0:3] 或 L[:3] 取L的前三个元素,不包括3；L[-2:]取倒数后2个元素；L[:10:2] 前10个元素，每2个取一个；tuple或字符串也可以切片操作。 迭代迭代 for … in 实现遍历list或tuple或其他可迭代对象。默认情况下，dict迭代的是key，for key in d : pass。如果要迭代value，可以用for value in d.itervalues()，如果要同时迭代key和value，可以用for k, v in d.iteritems()。字符串也可以迭代，for ch in ‘ABC’: print ch。 for循环中同时迭代索引和元素本身 12for i, value in enumerate(['A', 'B', 'C']): print i, value 列表生成式快速创建list，简单强大。 12345678range(1,11) ;[x * x for x in range(1,11)] =[1,4,9,...,100] ;[x * x for x in range(1,11) if x%2==0] = [4,16,36,64,100] ; # 两层循环 [x + y for x in 'ABC' for y in 'ZXD' ]; # 列出当前目录下的文件和目录：import os[d d in os.listdir('.')] 生成器像不生成完整的list，而采用一边循环一边计算的机制；创建方法1： 将列表生成式的[]改为() g=(x * x for x in range(10)) 如果要一个一个打印出来g的元素，可以通过generator的next()方法，但是通常用for迭代来遍历。 for n in g: print g 方法2：通过函数实现生成器 123456def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b # generator在执行过程中，遇到yield就中断，下次又继续执行。 a, b = b, a + b n = n + 1 要理解generator的工作原理，它是在for循环的过程中不断计算出下一个元素，并在适当的条件结束for循环。对于函数改成的generator来说，遇到return语句或者执行到函数体最后一行语句，就是结束generator的指令，for循环随之结束。 函数式编程高阶函数允许函数作为参数传入: 12def add(x, y, f): return f(x) + f(y) map/reduce map()函数接收两个参数，一个是函数，一个是序列，map将传入的函数依次作用到序列的每个元素，并把结果作为新的list返回。 123456def f(x): return x**2map(f, [1,2,3,4,5,6])# 输出 [1,4,9,16,25,36]map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9])#输出 ['1', '2', '3', '4', '5', '6', '7', '8', '9'] reduce()把一个函数作用在一个序列[x1, x2, x3…]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算,例如，实现‘1 3 5 7 9’序列变为整数13579 1234def fn(x, y): return x*10 + yreduce(fn, [1, 3, 5, 7, 9])# 输出 13579 filter filter()也接收一个函数和一个序列。filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 123def is_odd(n): return n/2==1filter(is_odd,[1,2,3,4,5,6]) #求奇数 sorted Python内置的sorted()函数就可以对list进行排序。sorted()函数也是一个高阶函数，它还可以接收一个比较函数来实现自定义的排序。比如实现倒序排序reversed_cmp函数 1234567891011sorted([36, 5, 12, 9, 21])# 输出 [5, 9, 12, 21, 36]def reversed_cmp(x, y): if x &gt; y: return -1 if x &lt; y: return 1 return 0 sorted([36, 5, 12, 9, 21], reversed_cmp)# 输出 [36, 21, 12, 9, 5] 返回函数匿名函数lambda当高阶函数的参数是传入函数时，可以不显示地定义函数，直接传入匿名函数： 12map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9])# 输出 [1, 4, 9, 16, 25, 36, 49, 64, 81] 关键字lambda表示匿名函数，冒号前面的x表示函数参数。 匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数. 装饰器假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义，这种在代码运行期间动态增加功能的方式，称之为“装饰器”（Decorator）。 123456789def log(func): def wrapper(*args, **kw): print 'call %s():' % func.__name__ return func(*args, **kw) return wrapper #定义如上，使用如下： @logdef now(): print '2013-12-25' 把@log放到now()函数的定义处，相当于执行了语句 now=log(now) 参考文章-python装饰器 偏函数functools模块的一个功能， 1int2 = functools.partial(int, base=2) int2函数就是int（n，base=2）导出的新函数，int()函数的默认base=10，但是int2也可以接收参数base=10；functools.partial的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。 模块package &gt; module &gt; func package:按目录结构来管理模块，每一个包目录下面都会有一个__init__.py的文件，这个文件是必须存在的，否则，Python就把这个目录当成普通目录，而不是一个包。__init__.py可以是空文件，也可以有Python代码，因为__init__.py本身就是一个模块，而它的模块名就是mycompany。 编写模块 1234567891011121314151617181920#!/usr/bin/env python# -*- coding: utf-8 -*-' a test module ' #任何模块代码的第一个字符串都被视为模块的文档注释；__author__ = 'Michael Liao'import sysdef test(): args = sys.argv #用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是该.py文件的名称，运行python hello.py Michael获得的sys.argv就是['hello.py', 'Michael]。 if len(args)==1: print 'Hello, world!' elif len(args)==2: print 'Hello, %s!' % args[1] else: print 'Too many arguments!'if __name__=='__main__': test() 导入模块 123456789try: import cStringIO as StringIO #使用别名except ImportError: # 导入失败会捕获到ImportError import StringIO# 常见用法try: import json # python &gt;= 2.6except ImportError: import simplejson as json # python &lt;= 2.5 模块中__xx__这样的变量或函数是有特殊用途的，比如__doc__,一般自己的不要这样定义。 模块中_XX或__xx是私有函数或变量，不能直接引用。外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public。 安装第三方模块 pip install module-namepip install wheel Scrapy pymongo requests celery -i http://pypi.douban.com/simple --trusted-host pypi.douban.com-i 指定源，—-trusted-host 添加域名信任 导入模块时，默认情况下，Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中 123import syssys.path# 输出 ['', '/Library/Python/2.7/site-packages/pycrypto-2.6.1-py2.7-macosx-10.9-intel.egg', '/Library/Python/2.7/site-packages/PIL-1.1.7-py2.7-macosx-10.9-intel.egg', ...] 如果我们要添加自己的搜索目录，有两种方法：一是运行时修改sys.path，添加要搜索的目录：sys.path.append(‘/user/module/path’) ，运行结束后失效。 第二种在系统中设置环境变量“PYTHONPATH” 面向对象编程类和实例面向对象最重要的概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，比如Student类，而实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但各自的数据可能不同,和静态语言不同，Python允许对实例变量绑定任何数据，也就是说，对于两个实例变量，虽然它们都是同一个类的不同实例，但拥有的变量名称都可能不同。 1234567class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print '%s: %s' % (self.__name, self.__score) (object)，表示该类是从哪个类继承下来的，通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。 类的初始化方法init,传进的第一个参数是self，表示实例自身。创建实例时必须传入与init一致的变量，self除外。 属性方法和访问限制class内的属性前加上__就会变成私有变量，外部无法访问,获取或修改私有变量可以创建方法get set等，在方法中可以对参数进行检查。 类的属性仅对当前类起作用，对继承的子类是不起作用的,除非在子类中也定义__slots__，这样，子类允许定义的属性就是自身的__slots__加上父类的__slots__。 12345678910111213141516171819202122232425class Student(object): passs=Student()s.name='abc' #为一个实例添加属性，对其他实例无效 def set_age(self,age): self.age=agefrom types import MethodTypes.set_age = MethodType(set_age, s, Student) # 给实例绑定一个方法s.set_age(25)print s.age #为对象绑定一个方法def set_score(self,score): self.score=scoreStudent.set_score=MethodType(set_score,None,Student)# 如果我们想要限制class的属性怎么办？比如，只允许对Student实例添加name和age属性。# 为了达到限制的目的，Python允许在定义class的时候，定义一个特殊的__slots__变量，来限制该class能添加的属性：class Student(object): __slots__ = ('name', 'age') # 用tuple定义允许绑定的属性名称#__slots__定义的属性仅对当前类起作用，对继承的子类是不起作用的,除非在子类中也定义__slots__，这样，子类允许定义的属性就是自身的__slots__加上父类的__slots__。 有没有既能检查参数，又可以用类似属性这样简单的方式来访问类的变量呢?Python内置的@property装饰器就是负责把一个方法变成属性调用的. 12345678910111213class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError('score must be an integer!') if value &lt; 0 or value &gt; 100: raise ValueError('score must between 0 ~ 100!') self._score = value 继承和多态继承可以把父类的所有功能都直接拿过来，这样就不必重零做起，子类只需要新增自己特有的方法，也可以把父类不适合的方法覆盖重写； 多态真正的威力：调用方只管调用，不管细节，而当我们新增一种Animal类的子类时，只要确保run()方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则：对扩展开放：允许新增Animal子类；对修改封闭：不需要修改依赖Animal类型的run_twice()等函数。 123def run_twice(animal): #animal是Animal类对象的一个实例 animal.run() animal.run() 获取对象信息获取对象类型用函数type(),模块types里保存了所有的type类型常量。 123import typesprint type('abc') == types.StringType# output: True 使用isinstance(x,y)判断的是x对象是否是y类型本身，或者位于y类型的父继承链上。 dir() 获取一个对象的所有方法和属性，返回一个list。 getattr()、setattr()访问设置对象属性，hasattr() 可以测试对象的属性是否存在。 1234def readImage(fp): if hasattr(fp, 'read'): return readData(fp) return None 多重继承class dog(Mammal, Runnable): pass ,使用多继承可以避免复杂庞大的继承链。 定制类__xxx__的变量或者函数名在Python中是有特殊用途的,__slots__是为了限制类的属性，__len__()方法是为了能让class作用于len()函数。__str__()方法，打印实例；如果一个类想被用于for … in循环，类似list或tuple那样，就必须实现一个__iter__()方法，该方法返回一个迭代对象，然后，Python的for循环就会不断调用该迭代对象的next()方法拿到循环的下一个值，直到遇到StopIteration错误时退出循环。要表现得像list那样按照下标取出元素，需要实现__getitem__()方法；__getattr__()方法，动态返回一个属性，如果找不到引用的属性，则用该方法动态返回 更多可定制属性参考python官方文档 元类参考博客]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python读写XLS和CSV]]></title>
    <url>%2F2016%2F07%2F01%2F%E6%8A%80%E6%9C%AF%2Fpython%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[python读写CSV1. 写入并生成csv文件 代码： 123456789101112131415# coding: utf-8import csvcsvfile = file('csv_test.csv', 'wb')writer = csv.writer(csvfile)writer.writerow(['姓名', '年龄', '电话'])data = [ ('小河', '25', '1234567'), ('小芳', '18', '789456')]writer.writerows(data)csvfile.close() wb中的w表示写入模式，b是文件模式写入一行用writerow多行用writerows 2. 读取csv文件 代码： 123456789# coding: utf-8import csvcsvfile = file('csv_test.csv', 'rb')reader = csv.reader(csvfile)for line in reader: print linecsvfile.close() python读写Excel 1、导入模块 import xlrd 2、打开Excel文件读取数据 data = xlrd.open_workbook(&#39;excelFile.xls&#39;) 3、使用技巧 123456789101112131415161718192021222324252627282930313233343536# 获取一个工作表table = data.sheets()[0] #通过索引顺序获取table = data.sheet_by_index(0) #通过索引顺序获取table = data.sheet_by_name(u'Sheet1')#通过名称获取# 获取整行和整列的值（数组）table.row_values(i)table.col_values(i)# 获取行数和列数nrows = table.nrowsncols = table.ncols# 循环行列表数据for i in range(nrows ): print table.row_values(i)# 单元格cell_A1 = table.cell(0,0).valuecell_C4 = table.cell(2,3).value# 使用行列索引cell_A1 = table.row(0)[0].valuecell_A2 = table.col(1)[0].value#简单的写入row = 0col = 0# 类型 0 empty,1 string, 2 number, 3 date, 4 boolean, 5 errorctype = 1 value = '单元格的值' xf = 0 # 扩展的格式化table.put_cell(row, col, ctype, value, xf) table.cell(0,0) #查看单元格的值table.cell(0,0).value #单元格的值]]></content>
      <categories>
        <category>技术</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github问题汇总]]></title>
    <url>%2F2016%2F05%2F18%2F%E6%8A%80%E6%9C%AF%2Fgithub%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[https 转换为 ssh 查看当前地址版本 git remote -v 123$ git remote -vorigin git@github.com:shuaiyy/shuaiyy.github.io.git (fetch)origin git@github.com:shuaiyy/shuaiyy.github.io.git (push) 设置为ssh地址 git remote set-url origin git@github:USERNAME/OTHERREPOSITROY.git Git冲突：commit your changes or stash them before you can merge. 当本地修改未提交，使用git pull更新时，会报错 解决方法： stash通常遇到这个问题，你可以直接commit你的修改；但我这次不想这样。看看git stash是如何做的。git stashgit pullgit stash pop接下来diff一下此文件看看自动合并的情况，并作出相应修改。git stash: 备份当前的工作区的内容，从最近的一次提交中读取相关内容，让工作区保证和上次提交的内容一致。同时，将当前的工作区内容保存到Git栈中。git stash pop: 从Git栈中读取最近一次保存的内容，恢复工作区的相关内容。由于可能存在多个Stash的内容，所以用栈来管理，pop会从最近的一个stash中读取内容并恢复。git stash list: 显示Git栈内的所有备份，可以利用这个列表来决定从那个地方恢复。git stash clear: 清空Git栈。此时使用gitg等图形化工具会发现，原来stash的哪些节点都消失了。 放弃本地修改，直接覆盖之 git reset –hardgit pull​]]></content>
      <categories>
        <category>技术</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从二次型最优化问题中理解矩阵特征值的意义]]></title>
    <url>%2F2015%2F09%2F12%2F%E7%A7%91%E7%A0%94%2F2015-09-12-Intuition-of-Eigen-Value%2F</url>
    <content type="text"><![CDATA[惯例开场故事在某次从实验室去往食堂的路上，曾发生这样一段对话： 『大师兄，为什么你对算法的理解总是那么透彻呢？为什么我很难看出它背后的思想？』 『因为你去理解一个算法的时候，不能只是看懂它的形，还要去思考它的神啊~』 这就是我天分不够当不了科学家的佐证吧T。T 从二次型最优化来理解最小化二次型目标函数，其中A为已知的实对称二阶矩阵，，.这个问题的求解很简单，这里以此为例来说明该问题与矩阵特征值的关系。 首先，可以得到目标函数的网格图与等高线图如下。 对矩阵A进行特征分解可以得到其特征向量为[-0.7071, 0.7071], [0.7071, 0.7071]，对应的特征值分别是0.5, 1.5. 观察函数的等高线图可以知道，等高线最密集的地方，函数值变化最快，而这个函数值变化最快的方向归一化后就是[0.7071, 0.7071]，这恰好是矩阵的一个特征向量。同样地，可以观察，等高线最稀疏的地方，函数值变化最慢，变化方向则是矩阵的另一个特征向量。可以看出，矩阵特征值的大小与函数值的变化快慢有关，较大特征值所对应的特征向量方向上函数值的变化较快，较小特征值所对应的特征向量方向上函数值的变化较慢。 进一步，对于实对称矩阵，我们总是可以对其进行相似变化，得到一个以该矩阵特征值为对角线元素的对角阵。，其中，P为正交矩阵，有性质P的逆等于P的转置。把目标函数改写为，其中. 相似变换的作用可以理解为将等高线图进行旋转，于是得到下面经过旋转后的等高线图。 在这张图上说明矩阵特征值的意义。当函数值取1时所对应的椭圆等高线的长轴长度为， 即由矩阵特征值0.5决定。同理，该椭圆短轴长度为，由矩阵特征值1.5决定。 二阶矩阵的理解较为直观。高阶矩阵的道理是一样的。 资料【1】如何理解矩阵特征值]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛顿法与拟牛顿法（DFP BFGS LBFGS VLBFGS）]]></title>
    <url>%2F2015%2F03%2F23%2F%E7%A7%91%E7%A0%94%2F2015-03-23-Newton-QuasiNewton-Method%2F</url>
    <content type="text"><![CDATA[最近做LBFGS的并行，顺便把牛顿法、拟牛顿法顺理一下。 拟牛顿法是求解非线性优化问题最有效的方法之一。考虑无约束的极小化问题，假设为凸函数，且二阶连续可导。 原始牛顿法基本思想：在现有极小点估计值的附近对f(x)进行二阶泰勒展开，进而找到下一个极小点的估计值 牛顿法具有二次收敛性，但当目标函数非二次型时，牛顿法不能保证函数稳定地下降（缺点）。 阻尼牛顿法每次迭代前需要沿迭代方向做线搜索，寻求最优的步长因子，即 拟牛顿法基本思想：不使用二阶偏导数而构造出可以近似Hession或Hession的逆的正定对称阵，在“拟牛顿”的条件下优化目标函数。 先推导拟牛顿条件：在附近对做泰勒展开，取二阶近似项 推出 取，推出 引入记号 ， 推出 (拟牛顿条件) 它迭代过程中的hession矩阵做约束，因此，对hession对近似的，以及对hession的逆做近似的，可以将 或 作为指导。 DFP算法（Davidon–Fletcher–Powell formula）核心：通过迭代的方法，对hession的逆做近似。迭代格式为 （通常） 猜想待定为（具有对称性） 括号中是数值，将其分别简单赋值为1，-1，即 其中向量u,v仍有待确定，由上面 （要此式成立，不妨直接取） 至此，校正矩阵就已经构造出来了 BFGS算法（Broyden–Fletcher–Goldfarb–Shanno algorithm）核心公式的推导过程与DFP完全类似，只是互换了其中s{k}和y{k}的位置。BFGS直接逼近Hession矩阵B_k。(公式敲起来太累了，请自行推导) LBFGS算法(limited-memory BFGS)不再存储完整的D_k，而是存储计算过程中的向量序列{s}，{y}。当需要矩阵D_k时，利用向量序列的计算来代替。并且，向量序列也不是全部存储，而是固定存最新的m个。 若要实现并行，需要同时在x与梯度（影响y的计算）那儿求一致平均。 资料【1】DFP算法 【2】BFGS算法 【3】LBFGS算法 【4】Large-scale L-BFGS using MapReduce]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据机器学习初探---南大李武军]]></title>
    <url>%2F2015%2F02%2F04%2F%E7%A7%91%E7%A0%94%2F2015-02-04-Group-Meeting%2F</url>
    <content type="text"><![CDATA[每周的组会大概会持续2小时。如果是主讲，就需要花更多的时间去准备报告内容。之前，组会开完我就不管了，缺乏总结思考。而这样子的话实质上意义就不大了，没有内化为自己的知识，也没有什么critical thinking。从现在开始，记录每一次组会的思考。 常言道：亡羊补牢，为时未晚。T.T Outline Learning to Hash Distributed Learning Stochastic Learning 有一个形象的比喻是这样说的，大数据是金矿，云计算是采矿技术，大数据机器学习是冶金技术。 大数据机器学习面临的挑战，一是存储，而是计算速度，三是网络。 哈希学习，在内存、硬盘、cpu、通信上有优势；分布式学习在内存、硬盘、cpu上有优势，但会增加通信成本；随机学习在内存、硬盘、cpu方面有优势。 Learning to Hash主讲人：大师兄 最近邻搜索在大数据背景下，会出现维数灾难，存储成本也高，查询速度也慢。解决方法之一是保相似性哈希，可以降低维数并减少存储成本。通常用海明距离（hamming distance）来表征哈希值之间的差异。哈希方案也具有较快的查询速度，通常具有常数或者次线性的搜索时间复杂度；即使是穷举搜索也可以被接受，因为海明距离计算起来是很快的。 哈希函数学习的两个阶段： Projection Stage（dimension reduction） Quantization Stage 贡献： Isotropic Hash 思想：学习一个正交阵（幻灯片21页），其目的是让大于某一阈值的feature的重要程度是一样的。 Supervised Hashing with Latent Factor Models Supervised Multimodal Hashing with SCM Multiple-Bit Quantization Distributed Learning主讲人：我 主要内容： 文章：Coupled Group Lasso for Web-Scale CTR Prediction 文章：Distributed Power-Law Graph Computing 文章1为了解决在线广告的CTR（click through rate）预测，即当某广告展示给某用户时，它被该用户点击的概率，通常的方法是LR（logistic regression），即逻辑回归。但LR的一个短板是，因其是线性的，所以无法将用户与广告之间某些微妙的非线性关系纳入。注意LR中，正则项若为2范数平方，称为标准逻辑回归；正则项若为1范数，问题通常被叫做Lasso。所以需要一种新的方法。 这里的贡献是： CGL的似然定义中，可以纳入用户与广告之间的某些非线性关系的考量。 正则项改为参数的2-1范数，目的是是用户特征向量参数W、广告特征向量参数V中更多的行为0，行为0说明该行对应的feature没作用，即达到删除冗余feature的作用。 分布式实现。这个算法具有较好的扩展性，一个master，若干slave，类似于并行计算，从而实现分布式。 文章2GP（graph partitioning）图分割的方法有两种：边分割；点分割。点分割在分布式计算中的通信成本会比图分割小，原因在于在不同的machine上，点分割只需保留点的copy，而边分割需要同时保留点与边的copy。 切割degree大的点，即邻居多的点可以降低通信成本。 Stochastic Learning主讲人：浩锋 思想：在需要用到所有节点上的信息时，通信代价往往很大，这时可以随机的选取某一个节点上的信息（比如梯度）作为替代品。 资料【1】幻灯片 【2】Coupled Group Lasso forWeb-Scale CTR Prediction in Display Advertising 【3】Distributed Power-law Graph Computing:Theoretical and Empirical Analysis]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Math</tag>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git常用命令速查表]]></title>
    <url>%2F2015%2F01%2F30%2F%E6%8A%80%E6%9C%AF%2FGit-Resources%2F</url>
    <content type="text"><![CDATA[master: 默认开发分支 origin: 默认远程版本库 Head: 默认开发分支 Head^: Head的父提交 创建版本库12$ git clone &lt;url&gt; #克隆远程版本库$ git init #初始化本地版本库 修改和提交123456789$ git status #查看状态$ git diff #查看变更内容$ git add . #跟踪所有改动过的文件$ git add &lt;file&gt; #跟踪指定的文件$ git mv &lt;old&gt;&lt;new&gt; #文件改名$ git rm&lt;file&gt; #删除文件$ git rm --cached&lt;file&gt; #停止跟踪文件但不删除$ git commit -m "commit messages" #提交所有更新过的文件$ git commit --amend #修改最后一次改动 查看提交历史123$ git log #查看提交历史$ git log -p &lt;file&gt; #查看指定文件的提交历史$ git blame &lt;file&gt; #以列表方式查看指定文件的提交历史 撤销1234$ git reset --hard HEAD #撤销工作目录中所有未提交文件的修改内容$ git checkout HEAD &lt;file&gt; #撤销指定的未提交文件的修改内容$ git revert &lt;commit&gt; #撤销指定的提交$ git log --before="1 days" #退回到之前1天的版本 分支与标签1234567$ git branch #显示所有本地分支$ git checkout &lt;branch/tag&gt; #切换到指定分支和标签$ git branch &lt;new-branch&gt; #创建新分支$ git branch -d &lt;branch&gt; #删除本地分支$ git tag #列出所有本地标签$ git tag &lt;tagname&gt; #基于最新提交创建标签$ git tag -d &lt;tagname&gt; #删除标签 合并与衍合12$ git merge &lt;branch&gt; #合并指定分支到当前分支$ git rebase &lt;branch&gt; #衍合指定分支到当前分支 远程操作12345678$ git remote -v #查看远程版本库信息$ git remote show &lt;remote&gt; #查看指定远程版本库信息$ git remote add &lt;remote&gt; &lt;url&gt; #添加远程版本库$ git fetch &lt;remote&gt; #从远程库获取代码$ git pull &lt;remote&gt; &lt;branch&gt; #下载代码及快速合并$ git push &lt;remote&gt; &lt;branch&gt; #上传代码及快速合并$ git push &lt;remote&gt; :&lt;branch/tag-name&gt; #删除远程分支或标签$ git push --tags #上传所有标签 资料链接 Try Git]]></content>
      <categories>
        <category>技术</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Mac上用LaTeX写漂亮的简历]]></title>
    <url>%2F2014%2F12%2F06%2F%E6%8A%80%E6%9C%AF%2FMake-resume-by-LaTeX%2F</url>
    <content type="text"><![CDATA[你会搜索查看到这篇文章，相信就不需要我解释为什么要用LaTeX写Resume了：） 今晚报名Facebook China Tech Talk，最后一步需要上传简历。看着已经2年没有更新过的简历，好捉急。那时真是年轻，不舍得做减法，恨不能一张A4纸写尽一生。于是索性重新制作一份简历。 需要准备 安装好的LaTeX，如果没有安装请参考在Mac上通过Sublime、Skim编辑LaTeX 互联网 资料 Using the LaTeX Resume Templates LaTeX Templates 步骤 在上述资料中寻找自己喜欢的模板 下载模板对应的tex文件 用LaTeX打开对应文件，编辑，编译 这个时候，如果你使用的是Mac系统，非常不幸，大多数情况下都将编译失败。因为网上多数模板需要使用windows环境下的Tex应用程序，而Mac环境下MacTex应用程序会缺少部分文件。没关系，我们有办法解决。 解决方案一：moderncv 进入http://www.ctan.org/pkg/moderncv 下载moderncv package 解压，找到模板文件template.tex 用已经安装好的LaTeX打开模板文件，编辑，编译，成功 但是呢，我个人觉得moderncv模板并不够好，虽然其结构清新简洁，但布局过于稀疏。没关系，我们仍然有办法。感谢一个我无意中发现的网站：ShareLaTeX.com 解决方案二：ShareLaTeX.com也许你在上面的资料中找到了你最喜欢的模板，却苦于在Mac OS X系统下无法编译成功。这时可以求助于ShareLaTeX，这是一个在线LaTeX编辑网站，并且提供Resume,Cover Letter,Journal Article,Presentation,Thesis,Bibliographies等不同分类的多种模板。最重要的一点事，只需要确定Latex语法无误，再也不需担心什么编译环境、文件缺失等乱七八糟的问题。 进入ShareLaTeX，注册账号 点击New Project，选择CV or Resume，挑选你喜欢的简历模板 根据自己的情况编辑，自动或手动编译，保存PDF 后记既然写到这里了，还想讲讲自己对于简历的体会。但我真的是困得不行了。。。。北京第一次不归夜。。。改天再来补全。。。。]]></content>
      <categories>
        <category>技术</category>
        <category>latex</category>
      </categories>
      <tags>
        <tag>LaTeX</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的报告 Decentralized Privacy-Preserving Low-Rank Matrix Completion]]></title>
    <url>%2F2014%2F11%2F26%2F%E7%A7%91%E7%A0%94%2F2014-11-26-Presentation-at-Chinese-Academy-of-Science%2F</url>
    <content type="text"><![CDATA[转载 我的报告Section 0: Introduce MyselfPage 1 Good afternoon everyone! My name is Anya Lin. I’m a second-year master candidate from University of Science and Technology of China. It’s my great pleasure to introduce to you the Decentralized Privacy-Preserving Low-Rank Matrix Completion. It’s a joint work with my supervisor, Prof. Ling from USTC. Before I start, I want to express my thanks to Prof. Ling for his patient instructions and help over the last a few months. Page 2 Here’s the outline of my presentation. First is the introduction. And then the centralized matrix completion problem. We develop a decentralized algorithm in section 3, and our algorithm is derived from a centralized algorithm as I will talk about in section 2. Next, I will introduce the main result of the topology-dependent privacy preservation. At last, it’s the conclusion. Section 1: IntroductionPage 3 OK， let’s go into the introduction. Page 4 I’d like to begin with the concept of matrix completion. So what is matrix completion? As we can see in this picture, we have an incomplete matrix, whose entries are known only for a subset of the whole matrix. And the rank of the matrix is very small compared with the size of the matrix. The goal of the matrix completion is to recover all these unknown entries of of the matrix, as the right-side picture shows. Here, Z is the recovery of W. Page 5 There’s many applications of such a problem. Like image processing, recommendation system and so on. Here are 2 examples. The first one is a problem of image processing. The left picture has a lot of noises, or say, only a part of the original picture is known. By using the fact that the original picture is usually low-rank, we can matrix completion to denoise the picture and get a clear version of high quality as the right picture shows. The second example is more close to our lives. It’s related to a recommendation system. As you can see, it’s a webpage of Douban Movie. A user sees a movie, such as Interstella, and then scores it on the website. Here we can imagine a huge matrix with rows representing users and columns the movies. This matrix is incomplete and it’s low-rank. Once this matrix is completed, the website can recommend new movies to users. Page 6 Now, here comes a privacy concern. First what is privacy? Privacy is the values one considers private. In the example we mentioned just now, the users’ scores of the movies are privacy, because one may not want others to know what movies he has seen or likes. Also, the entries of the matrix could be medical records of patients, or selling data of merchants. These data are considered as privacy. Obviously, no one wants the leakage of his privacy. However, in reality there may exist a malicious agent, a bad guy. For some reasons you have to give your private data to it, but you don’t know wether you can trust it or not. In this situation, we need privacy-preservation. Privacy-preservation is the ability to prevent a malicious agent from obtaining or reconstructing the private data. Section 2: Centralized Matrix Completion ProblemPage 7 Now let’s go into the centralized matrix completion problem. Page 8 When we are faced with a low-rank matrix completion problem, the intuitive thought would be to minimize the rank of the matrix, but this is a nonconvex problem. Therefore, we insteadly minimize the nuclear-norm the the matrix, since nuclear-norm is the approximation of the rank and it’s convex. Another approach is if the rank of matrix is known to be r as a prior theoreticallyor empirically, we can get the matrix factorization formulation. This approach is advantageous over the nuclear-norm approach since the latter one needs sigular value decomposition, which is computationally expensive and even intractable in decentralized computing. A centralized algorithm called LMaFit to solve this is as the following steps shows. We have to keep in mind that our algorithm is derived from LMaFit. Section 3: Decentralized Matrix Completion ProblemPage 9 After the centralized problem, let’s go into the decentralized one. Page 10 In decentralized computing, we have a network composed of L agents. There is an undirected edge between two agents if they can communicate with each other through one hop. The goal of all the agents in such a network is to collaboratively complete a low-rank matrix in a decentralized fashion. Page 11 To be specific, we segment the whole data matrix W into groups of columns. And do the same to Y and Z. Each agent i in the network holds the corresponding Zi, Yi and entries of Wi. However, X cannot be segmented and distributed to agents because the update of X contains the summation of ZiYi’ over all agents. So we let each agent i holds a local copy X(i) of X. After doing this, we get a naive decentralized implementation of LMaFit. At iteration k, each agent i does the following steps respectively. Notice that the update of X requires information aggregation of all agents. So here is the challenge: informationaggregationofallagentsisimpossible in real decentralized network unless every agent is connected to all the other agents. How to deal with this challenge? Page 12 The answer is dynamic average consensus. Recall that each agent i holds a local copy X(i) of X. If we can make sure that X(i) equals to X for all i, the challenge is solved. To do this, we choose c to be 1/L and the update of X becomes the average consensus problem, as we can see in equation (8), X(i) is the average of all the ZiYi’. At iteration k,we formulate the average consensus problem as equation (9). The constraint means that instead of letting all the X(i) to be identical we choose to let each X(i) equals to it neighboring X(j). A key observation is that exact average consensus at every iteration is not necessary. We use EXTRA to do inexact dynamic average consensus, which saves the computational cost. Page 13 Our decentralized algorithm called D-LMaFit is developed as below. Step 1 is the initialization. Step 2, use EXTRA to do the inexact average consensus problem. Step 3, update Y and X respectively. Page 14 The performance of D-LMaFit is shown in these two pictures. (Explain what these pictures indicate to the audience) Section 4: Topology-Dependent Privacy PreservationPage 15 Now let’s go into the section of the topology-dependent privacy preservation. Page 16 First compare decentralized algorithm with centralized one. Centralized algorithm needs a fusion center to collect global data. What if the fusion center is a malicious agent? Oops TT, you’ll lose all your privacy. How about the decentralzied algorithm? One important advantage of decentralized algortihm over a centralized is there’s no global data collection, each agent observes part of the raw data and communicates with its neighboring agent(s). It seems safer. But things aren’t so lucky in reality. Because the communication of X(i) among the network may lead to information leakage. Page 17 How does this happen? Suppose in a network as this picture shows, we have a malicious agent M, and M attempts to recover the local data matrices of some other agents through information exchange. M is interested in recovering the local data Wi, or equivalently Yi or Zi of a set of agents i∈I. When the iteration k is large enough, X(i) will be identical. So if a malicious agent M somehow knows other agents’ Yi, it can recover the data Zi of agent i by doing X(M)Yi. So our concern is, is there any possibility that the malicious agent M can somehow obtain Yi of agent i, and thus get Zi, which is private. Page 18 Before going to details, consider two simple topologies. (Explain the two topologies) Under what conditions can not a malicious agent M reconstruct the sensitive information of P and Q ? Page 19 Recall the update of X. If you could just take a look at the equation, you can find that if the topology is as in the left picture, M can reconstruct ZiYi’ and it may be able to solve Yi so that gets the privacy of P and Q. But if the topology is as shown in the right picture. M cannot get the private data of P and Q. Why? (Explain with the equation) M can solve a series of linear inverse problems and calculate the values of ZiYi’, as what we have said just now. Page 20 Now we get a naive conclusion. Page 21 So the privacy-preserving problem boils down to the linear inverse problem. First we define some variables as this. And further we define A and B. Using these definition,the update of X can be represented by (14). Page 22 Rewrite this as a linear time-invariant systems we get (15). In this system, QI selects those row blocks in Q belonging to the agents in I, and BI selects the corresponding columns in B. QIC and BIC selects the other corresponding row blocks and columns which do not belong to the agents in I. Our analysis uses the concept of z-transfer matrix of (15). The concept is from modern control theory. Obviously, rank(T)=rank(TI TIC), since the latter matrix is just a column rearrangement of the former one. Now we are ready to develop our theorem. Page 23 Check the proof of the suffienciency of our theorem, it’s rather straightforward. If this condition is satisfired, then M has full knowledge of all the X(i). So M can solve a series of linear inverse problems. Page 24 The proof of necessity is a little bit complicated. Here’s the only the simplified version of the proof. First we show that to determine a unique sequence of Q􏰇 from V􏰇 , we must have (18). Suppose (18) doesn’t hold, then there exists at least one column of TI that is linearly dependent on the other columns of T. Then there exists a Q with that column nonzero, and satisfies TQ=0. This corresponds to a nonzero input in I, but the output V is zero for all time. Thus this nonzero input cannot be recovered. This contradicts with the hypothesis. So (18) must hold. (Explain these items) Page 25 (Explain these items) Section 5: ConclusionPage 26 I’d like to quickly go over the main point of today’s topic. Page 27 First, we propose a decentralized privacy-preserving algorithm, D-LMaFit, to solve the matrix completion problem. We solve dynamic average consensus subproblem inexactly. We prove the topology-dependent privacy-preserving theorem. It provides a guideline of designing a privacy-preserving network. Page 28 Still we’ve got work to do in the future. (Read items) Page 29 I guess that’s it. Thank you all very much for listening. Now if you have any question, please feel free to ask me. 故事这学期我在中科院数学与系统科学研究院(AMSS)访问。第一次参与这边的讨论班时，我就被惊到了：学生做报告也全程英文，不愧是袁亚湘老师的学生。于是，11月25日，我也在这儿完成了自己第一次的英文学术报告。 报告前3天，我问盛镇醴师兄他们报告前会不会排练，师兄说：“肯定要啊！上次去葡萄牙开会，马士谦师兄已经讲得那么好了，都还又自己私下练习了5、6遍。师兄真的可以做到每句话精确到几秒钟！”太荔枝了有木有TT。 于是我也练习了。果然只有努力了内心才会踏实。在当天的报告中，我不仅不紧张，还在瞅到台下一堆人的专注神情时，心里突然弹幕全开：“哇，这感觉好爽。” 在记录报告之前，插播一段回忆：大三暑假，我参加中国大学生物联网创新创业大赛，正式比赛前一天系里组织答辩练习，我们组讲得一塌糊涂。那一晚，我和向国菲师兄在实验室通宵改幻灯片，准备发言稿，然后一句一句地练习。中途师兄压力太大又累得不行溜出去躲着抽了根烟，回来被我发现了教育了一顿，嗅觉就是这么灵敏没办法。直到凌晨4点，终于觉得还算满意了，两人躺椅子上睡了会儿，当然我被蚊子咬安逸了。早晨7点，寝室开门，两人各自回去洗澡调整状态。9点，开始比赛。不知道为什么突然想起这个，太，美好了。尽管当时觉得真苦逼。 资料【1】有哪些高级的英语表达技巧，让人一听就很地道？]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[凸优化的一些基础算法]]></title>
    <url>%2F2014%2F11%2F10%2F%E7%A7%91%E7%A0%94%2F2014-11-10-Basic-Algorithms-of-Convex-Opt%2F</url>
    <content type="text"><![CDATA[本文假设读者对凸优化有基本了解，主要归纳一些基础算法，以便查阅。 其中，f，g，h都是凸函数，g是光滑项，h是非光滑项。 Gradient Descent ###Proximal Gradient Conjugate Gradient是介于最速下降法和牛顿法之间的一个方法，它仅需要利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hession并求逆的缺点。它是解决大型线性方程组最有用的方法之一，也是解决大型非线性最优化最有效的算法之一。 Newton见牛顿法与拟牛顿法（DFP BFGS LBFGS VLBFGS） Quasi Newton见牛顿法与拟牛顿法（DFP BFGS LBFGS VLBFGS）]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是 P, NP, NP-complete, NP-hard]]></title>
    <url>%2F2014%2F11%2F09%2F%E7%A7%91%E7%A0%94%2F2014-11-09-What-is-NP-Hard%2F</url>
    <content type="text"><![CDATA[相关概念NP-hard（non-deterministic polynomial-time hard） P：能在多项式时间内解决 NP：不能在多项式时间内解决或不确定能不能在多项式时间内解决，但一旦你找到一个解，只需要多项式时间去验证这个解是正确的 NP-hard：如果一个问题是NP-hard，意味着可以将任意NP问题化约到这个问题。如果可以解这个问题，那么可以轻松地解任意NP问题。 NPC：NP完全问题，所有NP问题在多项式时间内都能化约（Reducibility）到某一NP问题，这一NP问题就是NPC问题，即解决了此NPC问题，所有NP问题也都解决了。 资料原文These refer to how long it takes a program to run. Problems in class P can be solved with algorithms that run in polynomial time. Say you have an algorithm that finds the smallest integer in an array. One way to do this is by iterating over all the integers of the array and keeping track of the smallest number you’ve seen up to that point. Every time you look at an element, you compare it to the current minimum, and if it’s smaller, you update the minimum. How long does this take? Let’s say there are n elements in the array. For every element the algorithm has to perform a constant number of operations. Therefore we can say that the algorithm runs in O(n) time, or that the runtime is a linear function of how many elements are in the array. So this algorithm runs in linear time. You can also have algorithms that run in quadratic time (O(n^2)), exponential time (O(2^n)), or even logarithmic time (O(log n)). Binary search (on a balanced tree) runs in logarithmic time because the height of the binary search tree is a logarithmic function of the number of elements in the tree. If the running time is some polynomial function of the size of the input, for instance if the algorithm runs in linear time or quadratic time or cubic time, then we say the algorithm runs in polynomial time and the problem it solves is in class P. NPNow there are a lot of programs that don’t (necessarily) run in polynomial time on a regular computer, but do run in polynomial time on a nondeterministic Turing machine. These programs solve problems in NP, which stands for nondeterministic polynomial time. A nondeterministic Turing machine can do everything a regular computer can and more. This means all problems in P are also in NP. An equivalent way to define NP is by pointing to the problems that can be verified in polynomial time. This means there is not necessarily a polynomial-time way to find a solution, but once you have a solution it only takes polynomial time to verify that it is correct. Some people think P = NP, which means any problem that can be verified in polynomial time can also be solved in polynomial time and vice versa. If they could prove this, it would revolutionize computer science because people would be able to construct faster algorithms for a lot of important problems. NP-hardWhat does NP-hard mean? A lot of times you can solve a problem by reducing it to a different problem. I can reduce Problem B to Problem A if, given a solution to Problem A, I can easily construct a solution to Problem B. (In this case, “easily” means “in polynomial time.”) If a problem is NP-hard, this means I can reduce any problem in NP to that problem. This means if I can solve that problem, I can easily solve any problem in NP. If we could solve an NP-hard problem in polynomial time, this would prove P = NP. NP-completeA problem is NP-complete if the problem is both NP-hard, and in NP. 参考资料【1】What are P, NP, NP-complete, and NP-hard?]]></content>
      <categories>
        <category>学习</category>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown输入LaTeX数学公式]]></title>
    <url>%2F2014%2F09%2F16%2F%E6%8A%80%E6%9C%AF%2FMarkdown-Math%2F</url>
    <content type="text"><![CDATA[Markdown是读写性都非常好的轻量文本编辑语言，这个博客以及世界上许多博客的文章都是用其书写的。但是，在写“科研”博客时，难免会需要频繁地输入数学公式，而Markdown本身并不支持数学公式的输入。我曾经想偷懒直接用Markdown的语法去代替LaTeX数学公式，最后页面显示的结果有点儿丑。却一直也没有去修改。直到前天收到了印卧涛老师的一封邮件，邮件里所有的数学公式都是用LaTeX代码写的，正规而美观。由此觉得自己做事还是水了点。做事要认真啊亲。 本文默认我们是会使用LaTeX编辑数学公式的。 解决办法： 将数学公式以图片形式保存，再在Markdown中将其插入。 或者，使用LaTeX在线编辑器，输入数学公式，获得html代码，将其插入Markdown。 步骤： 进入CodeCogs 在盒子里书写公式 在页面下方复制html代码 将复制的html代码拷贝到Markdown里 缺点：Markdown文件的易读性却因此下降了很多。]]></content>
      <categories>
        <category>技术</category>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>LaTeX</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 0：实际问题]]></title>
    <url>%2F2014%2F09%2F12%2F%E6%8A%80%E6%9C%AF%2FLinux-Problems%2F</url>
    <content type="text"><![CDATA[《鸟哥的Linux私房菜——基础学习篇》 《鸟哥的Linux私房菜——服务器架设篇》 本系列文章分为两部分： 系统学习上面两本书的笔记。 实际中遇到的问题及解决方案，即本文内容。 实际问题1. 建立网络映射 Mac：Finder-&gt;前往-&gt;连接服务器-&gt;输入smb://IPaddress/samba-&gt;连接 Linux：位置-&gt;连接服务器-&gt;“服务类型”选择自定义位置-&gt;输入smb://IPaddress/samba-&gt;连接 2. ssh登陆失败以root身份远程登陆服务器，密码正确，却显示如下警告： 12345678910111213@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that a host key has just been changed.The fingerprint for the RSA key sent by the remote host is3a:17:4b:6e:62:e6:94:df:09:78:99:90:51:68:18:62.Please contact your system administrator.Add correct host key in /Users/AnyaLin/.ssh/known_hosts to get rid of this message.Offending RSA key in /Users/AnyaLin/.ssh/known_hosts:4RSA host key for 222.195.93.129 has changed and you have requested strict checking.Host key verification failed. 解决方法： 12345vi ~/.ssh/known_hosts #选中最后一条登陆记录，双击`d`删除，按“：”进入末行编辑模式，输入“x”，回车ssh root@222.195.93.129 #再次登陆The authenticity of host '222.195.93.129 (222.195.93.129)' can't be established.RSA key fingerprint is 3a:17:4b:6e:62:e6:94:df:09:78:99:90:51:68:18:62.Are you sure you want to continue connecting (yes/no)? #输入yes 3. tar.gz 文件解压 打开终端 进入需要解压的xxxx.tar.gz文件所在目录 $ tar xvfz xxxx.tar.gz -C /指定的目录 压缩并打包目录 123tar -czf small.tar.gz small(目录名)tar zcvf backup.tar.gz site/* --exclude=site/attach --exclude=site/images注意 --exclude后面的排除目录后不能带/ ，否则不起作用 ​ 4. 新建文件命令1touch a.txt =======]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matlab科研小贴士]]></title>
    <url>%2F2014%2F09%2F11%2F%E7%A7%91%E7%A0%94%2F2014-09-11-Matlab-tips%2F</url>
    <content type="text"><![CDATA[使用MATLAB运行算法程序时，可能遇到各种各样的报错。比如，为了保护隐私数据，我在分布式矩阵补全算法中加入随机矩阵之后，某项变量在运行几百步之后会出现NaN报错。我只根据算法顺序去分析问题出现的可能原因，并修改程序。感觉并没有很好地利用MATLAB的强大功能去锁定症结所在。幸运的是，施伟大师兄当时和我在一起，他非常热心地帮我分析问题，教我以后遇到类似状况应该怎么去分析与思考。和大师兄讨论了半小时，感觉自己收获不少。 这篇文章会陆续记录下自己使用MATLAB的体会，以及解决问题的一些技巧。 Clear运行一段代码前通常需要将工作空间里的已有数据清除掉。只需要在编辑有实际意义的代码之前写下如下代码： 1clc; clear; close all; Random Seed为了保证程序在相同环境下运行以便测试某一个或几个改变对于算法的影响，在使用各种random命令时，需要设定固定种子。这样就不会因为每次随机产生的序列不同而影响程序运行结果。设置随机种子的代码如下： 12345678910%% random seed%seed=round(5000*rand); % use this line if you set a random seedseed=3302; % use this line if you set a fixed seed. 3302 can be replaced by other numbers.fprintf('Seed = %d\n',seed); % print the current seed if exist('RandStream','file') RandStream.setGlobalStream(RandStream('mt19937ar','seed',seed));else rand('state',seed); randn('state',seed^2);end NaNNaN是Not a Number的缩写。当某变量显示NaN时，表示该变量是不明确的数值结果。比如0/0、inf/inf等运算会出现NaN报错。遇到这种情况，首先判断NaN出现在哪一步： 123if isnan(norres) %括号里是变量名。判断norres是否为NaN，若是，则在该步暂停程序。 keyboard;end 再在命令窗里单独查看与该变量有关的其他变量，从而排除正常变量，获知究竟是哪个或哪几个变量出了问题，变为无穷大或无穷小。再检查与这些变量有关的算法。 SaveAs若需要比较各参数对算法性能的影响，通常是在程序中修改参数运行，得到算法收敛精度与迭代次数的曲线图。再根据曲线图反向思考修改哪些参数有效。这个过程需要保存产生的大量图片。可以使用hold on命令将所有虚线画在同一张图上，也可以使用saveas将所有图片自动保存。 123456789%% plotfigure(1)semilogy(1:iter,y_axis(1:iter),'b-'); %b：蓝色。－：线段形状set(gca,'fontsize',12);grid on;xlabel('\fontsize&#123;12&#125;\it Iteration'); ylabel('\fontsize&#123;12&#125;\it Normalized residual');legend('\fontsize&#123;12&#125;\it text'); %text：这条蓝色代表什么hold onsaveas(gcf,'filename','fig') %filename：将图片保存为这个名字。fig：保存为fig格式 保存变量数据的命令： 12save(&apos;filename&apos;)save(&apos;filename&apos;,&apos;variables&apos;) 注意，在使用hold on命令时，应该保留上次程序运行后产生的各种数据。即不能在程序中写类似与clear all之类的清除语句，否则上次曲线图也将被删除。 矩阵规范化已知满秩矩阵A，进行下面操作使其所有奇异值均为1。 12[u s v]=svd(A);A=u*v'; 安装CVX 将cvx压缩包解压 将cvx文件夹拷贝至如D:\MATLAB Programs\Compressed Sensing目录下 在Current Folder窗口中打开cvx文件夹 在Command Window中输入cvx_setup 在MATLAB的File菜单下的set path把此路径加上。 把路径加进去后在file→Preferences→General的Toolbox Path Caching里点击update Toolbox Path Cache更新一下 完成 %%分段运行程序 选中%%分段 右键选择evaluate current section]]></content>
      <categories>
        <category>学习</category>
        <category>Matlab</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态一致平均问题算法-EXTRA和DAC]]></title>
    <url>%2F2014%2F08%2F31%2F%E7%A7%91%E7%A0%94%2F2014-08-31-Papers-about-average-consensus%2F</url>
    <content type="text"><![CDATA[EXTRA DAC 优缺点比较其中，DAC最大的缺点在于第一次迭代时对于r(-1)时刻的依赖，在实际仿真中，如果需要对动态输入求一致平均，往往并不能获取输入在-1时刻的值。导致在矩阵补全问题中，DAC做不精确的动态一致平均的子问题效果并不好。 而EXTRA却有很好的效果。]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对于一致平均问题的理解]]></title>
    <url>%2F2014%2F08%2F22%2F%E7%A7%91%E7%A0%94%2F2014-08-22-Average-Consensus%2F</url>
    <content type="text"><![CDATA[最近几个月在研究分布式低秩矩阵补全的问题，参考文章《低秩矩阵补全》。 低秩矩阵补全问题的各类算法中，很关键的一个子问题叫做一致平均问题(Average Consensus)，而上一篇文章并没有对这个问题进行说明。那么，什么叫做“一致平均”呢？ 问题阐释：考虑一个有个节点的网络，每个节点上存储一个关于自己的数值信息，叫做初值。一致平均问题就是使所有节点在算法停止的时候收敛到个初值的平均。 在分布式算法中，通常会存在这样一个变量，它作为公有信息在网络中传递，每个节点储存自己的。在总算法的每一次迭代中，每个节点接收自己邻居节点传递过来的公有信息，然后与自己的私有信息共同计算出新的公有信息，并传给自己的邻居。我们需要保证每个节点上的公有信息，使分布的算法以某种方式交流合作，以便获得最优解。这就是分布式算法中的一致平均问题。 问题分类：根据节点上的数值信息是否随时间变化，又把一致平均问题分为： 静态一致平均 (Static average consensus) 动态一致平均 (Dynamic average consensus) 顾名思义，静态一致平均指节点上的初值不会发生变化，只需要保证最后每个节点都收敛到个初值的平均值即可；而动态一致平均则是，节点上的数值不断发生变化，即在时刻的值并不一定与0时刻的值（初值）相同，我们使用不断变化的公有信息（因为公有信息不断在被更新），仍需要保证最后每个节点都收敛到个初值的平均。相比于静态一致平均，动态一致平均问题更为棘手。 求解方法分类：在文章《低秩矩阵补全》的最后，提供一种求解方法的分类概念，将方法分为： 精确一致平均 (Exact average consensus) 不精确一致平均 (Inexact average consensus) 这又是什么意思呢？ 在解决分布式低秩矩阵补全问题的时候，我们将算法分解两个子问题不断求解，一是交替极小化(Alternating minimization)得到每个节点的新的,；二是在网络中对个节点的求一致平均。对于第二个子问题，在每一次算法总的迭代中，都去求解精确的一致平均显然能够解决问题，但是因为一致平均也需要一定次数的迭代才能被解除，如果在总算法的每一次迭代中都去求精确的一致平均，则相当耗费计算资源，增加了算法的时间复杂度。 很自然地，我们会想到，既然一致平均只是矩阵补全的一个子问题，我们是不是可以通过某种松弛，来降低算法的时间复杂度并节省计算资源，同时仍旧保证总算法的收敛呢。这样，就提出了不精确的一致平均。 不精确一致平均，就是在每一次求解一致平均子问题时，只迭代一次或者若干次，而不是迭代所有次（可以很大），使每个节点近似的达到它们初值的均值。 精确一致平均与不精确一致平均优缺点比较: 精确(Exact)求解：理论上容易证明，但计算代价通常比inexact方法高 不精确(Inexact)求解：理论分析上不好证明，除此之外具有exact不具有的所有优点，比如算法时间复杂度低，节省计算资源等等。]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Mac上通过Sublime、Skim编辑LaTeX]]></title>
    <url>%2F2014%2F08%2F10%2F%E6%8A%80%E6%9C%AF%2FUsing-LaTeX-with-Sublime-and-Skim-for-Mac%2F</url>
    <content type="text"><![CDATA[Sublime Text是一款非常优秀的编辑器，速度快，界面简洁，插件众多。并且能跨平台使用，在Mac和Windows上都能完美使用。虽然是一款付费软件，但作者很厚道地给了无限期的试用期限。这一切正如其官网广告词说的那样：The text editor you’ll fall in love with. Skim是一款免费轻量的PDF阅读、标注工具，布局贴心友好，与OS X自带的Previewer相比，Skim能更好的注释PDF文件。 LaTeX是一款权威的科技论文排版软件，不仅可以写论文，也可以处理日常的各种文档工作，甚至是做幻灯片。相比于Word，LaTeX最大的优势是对于复杂公式的编辑与排版非常漂亮。并且用简单的命令就可以生成脚注、索引、目录和参考文献等复杂的结构。这一切优点都使得世界上众多的“科学家”们不再需要身兼作者与排版工两职，从而将更多的精力集中于文章内容本身。 本文的目的是将上述三种软件综合部署在Mac上。完成之后，你将可以在Sublime Text里面进行LaTeX代码编辑，用Skim预览生成的PDF文件。更重要的是，让你觉得，写论文也可以是一件很优美的事。 准备工作： Mac上至少4GB的空余空间 高速的互联网连接 第一步：安装MacTeX 进入MacTeX官网下载MacTeX.pkg文件。文件大约2GB，需要一段时间才能完成下载，趁现在去喝杯咖啡吧。 下载完成之后，双击MacTeX.pkg进行安装。 安装完成之后，会看到许多与TeX有关的程序图标，暂时忽略它们。 第二步：安装Sublime Text 进入Sublime Text官网下载最新版本的Sublime Text。这里我下载的是Sublime Text 3. 下载完成之后，将文件拖入应用程序文件夹安装。 第三步：在Sublime Text中安装Package Control我们需要在Sublime Text中下载插件以便能够很好地操作与LaTeX有关的文件。而插件是通过Package Control下载的。 进入Package Control官网复制灰色区块的代码。 打开Sublime Text。 使用快捷键“control+~”（~就在Esc键的下方）打开控制面板Console。你会在Sublime Text的底部看到弹出一个白色窗口。 将刚才复制的代码粘贴到控制面板。 按下“Enter”回车键。然后退出并重启Sublime Text。 第四步：安装LaTeX Tools Sublime Text重启后，按下“Command+Shift+P”打开命令托盘Command pallet，这一步也可以通过Tools下拉菜单完成。 在命令托盘里输入“Install Package”，按下Enter回车建。 完成之后，输入“LaTeX Tools”，找到这一项并回车安装。 退出并重启Sublime Text。 第五步：安装Skim 进Skim下载Skim并安装 打开Skim，在菜单栏中Skim &gt; Preference(选项) &gt; Sync(同步) 在预设菜单中选择Sublime Text 关闭上面这个窗口。 全部完成，✌️现在，我们已经做完了所有的步骤，可以打开Sublime Text，Command+N新建文件并在里面编写LaTeX代码了，完成编辑之后，Command+S保存文件，Command+B编译并运行，这时就可以在Skim里面看到PDF预览了。]]></content>
      <categories>
        <category>技术</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>LaTeX</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各类范数]]></title>
    <url>%2F2014%2F07%2F27%2F%E7%A7%91%E7%A0%94%2F2014-07-27-norms-of-vector-and-matrix%2F</url>
    <content type="text"><![CDATA[转载 向量范数 矩阵范数 矩阵乘积的迹 特殊范数 矩阵W的L2-1范数： TV范数||L(x)||_1。 其中L是差分算子，x是某种数字信号，在一维情况下，如下所示： ||L(x)||_1 = |x2-x1| + |x3-x2| + |x4-x3| + …… 加TV范数的目的是为了使求得的去噪信号仍然具有分段连续的性质。因为差值的1范数说明差值稀疏，从而说明求得的信号分段连续。]]></content>
      <categories>
        <category>学习</category>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[低秩矩阵补全]]></title>
    <url>%2F2014%2F07%2F25%2F%E7%A7%91%E7%A0%94%2F2014-07-25-matrix-completion%2F</url>
    <content type="text"><![CDATA[转载 问题描述:如果有这样一个矩阵 矩阵中有部分元素缺失 矩阵的秩相较于矩阵维数来说很小，并作为先验已知 我们希望恢复那些缺失的元素，这个问题就是低秩矩阵补全问题。 思考过程: 需要恢复一个低秩矩阵 直接想法是极小化矩阵的秩但是的优化问题非凸，不好求解核范数是秩的凸近似，所以想到 极小化核范数的集中式算法:求解的集中式算法有很多，比如： singular value thresholding algorithm fixed-point shrinkage algorithm proximal gradient algorithm ADMM 但如果矩阵规模和秩增大，以上算法的计算代价也增大，因为它们都需要求解奇异值分解(SVD)。SVD中求伪逆的步骤运算量大，很耗费资源。因此需要想更好的方法，避免极小化核范数。 极小化分解矩阵之积的集中式方法:将问题写为，其中是对的估计，在元素没有缺失的位置上和的元素相同，,是对的乘法分解。介绍两种求解该问题的方法： nonlinear Gauss-Seidel method nonlinear SOR(Successive Over-Relaxation)-like scheme:LMaFit 其中SOR方法是GS方法的拓展，区别仅在于SOR方法中对于X的更新加了权重，并对权值进行更新。 去中心式算法:当矩阵规模大到一定程度时，集中式算法在计算能力上要求过高，普通计算机也许无法计算。这时，我们需要在由许多普通计算机作为节点组成的网络中运算，这需要实现去中心式计算。去中心式计算式很容易实现的，将,,分别切块放在每个节点上，将作为公共信息在网络各个邻居节点间交换，优化问题形式不变，但需要加上的约束。而这样一个约束就引出了另一个子问题：一致平均(average consensus)问题。 关于一致平均问题的介绍请看： 《对于一致平均问题的理解》 《动态一致平均问题的4篇论文》]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
      </tags>
  </entry>
</search>
